
R 프로그램 설치
R 스튜디오 설치

* R 수업 목차

1장. R 기본문법과 시각화
2장. 데이터 관리와 이해
3장. knn 알고리즘
4장. 나이브베이즈 이론
5장. 의사결정트리
6장. 회귀분석
7장. 신경망과 서포트벡터 머신
8장. 연관규칙
9장. k-means
10장. 모델성능평가
11장. 모델성능개선

1장. R 기본문법과 시각화

* R 이 무엇인가?

 뉴질랜드의 aukland 대학의 robert gentleman 과 Ross ihaka 가 
 1995년에 개발한 소프트웨어 입니다. 데이터 분석을 위한 통계
 및 시각화를 지원하는 무료 소프트웨어입니다.

 파이썬과 비교해서 장점?  1. 데이터 시각화가 R 이 더 예쁘다

                                  2. R 이 역사가 더 오래되어서인지 단순한 코드로
                                     데이터 분석을 빠르게 할 수 있다. 
■ R 의 장점 ?

 1. 무료 입니다.
 2. data 분석을 위한 통계 패키지들이 유용한게 아주 많고 실시간 
    업데이트 되고 있습니다.
 3. 누구든지 유용한 패키지를 생성해서 공유할 수 있고 새로운 기능에
    대한 전파가 빠르다.
 4. 어떠한 os 에도 설치가 가능합니다. 아이폰에도 설치할 수 있습니다.
 5. 홈페이지도 너무 쉽게 만들 수 있습니다.
 6. 딥러닝 구현도 너무 쉽게 할 수 있습니다. 

 구글 코랩에서 R 코드를 사용할 수 있습니다. 

■ R 의 자료구조 

 1. vector  :  같은 데이터 타입을 갖는 1차원 배열구조
 2. matrix :   같은 데이터 타입을 갖는 2차원 배열구조
 3. array  :    같은 데이터 타입을 갖는 다차원 배열구조
 4. data.frame :  오라클의 테이블, 판다스의 데이터 프레임과 같음
 5. list  :  서로 다른 데이터 구조인 데이터 타입이 중첩된 구조 

 R 자료구조 그림 

점심시간 문제 검사 받겠습니다.  B 반은 라인검사
                                           A 반은 카페에 올려주세요 ~

■ R 을 사용하여 데이터를 검색하는 방법 배우기 

문제1.  데이터를 저장할 워킹 디렉토리를 지정합니다.

getwd()   # working directory 확인

setwd("d:\\data")  

getwd()

문제2. emp3.csv 를 d 드라이브 밑에 data 폴더 밑에 가져다 두시오 


문제3.  emp3.csv 를 로드해서 emp 라는 데이터 프레임을 생성하시오!

emp <- read.csv("emp3.csv")

emp <- read.csv("d:\\data\\emp3.csv")

emp

문제4. emp 데이터 프레임에서 이름과 월급을 조회하시오 !

문법: emp[ 행, 열 ]

emp[       ,  c("ename","sal") ] 

문제5.  월급이 3000 인 사원들의 이름과 월급을 출력하시오 !

emp[ emp$sal==3000, c("ename","sal") ] 

문제6. 월급이 2000 이상인 사원들의 이름과 월급을 출력하시오 !

emp[ emp$sal >= 2000, c("ename","sal") ] 

문제7. 직업이 SALESMAN 인 사원들의 이름과 월급과 직업을 출력하시오!

emp[ emp$job=='SALESMAN', c("ename", "sal", "job") ]

문제8. 1981년 12월 11일에 입사한 사원들의 이름과 입사일을 출력하시오!

emp[ emp$hiredate=="1981-12-11 0:00", c("ename","hiredate") ]

문제9. emp 데이터 프레임의 구조를 확인하시오 !

R> str(emp)

pandas>  print( emp.info() ) 

※ 날짜형 컬럼인 hiredate 가 chr(문자형) 이므로 검색할 때 문자 그대로
   검색하면 됩니다. 

■ 연산자 총정리

 1. 산술 연산자 :  *  /  +  -
 2. 비교 연산자 :  >, <, >=, <=, ==, !=
 3. 논리 연산자 :   &  : and (백터화된 연산)
                        && :  and ( 백터화 되지 않은 연산)
                         |    :  or  ( 백터화된 연산)
                         ||   :  or  ( 백터화 되지 않은 연산)
                         !   :  not
예제:  x <- c(1,2,3)   # 파이썬 [ 1, 2, 3 ]
        x
        str(x) 
        ( x > c(1,1,1) ) & ( x < c(3,3,3) )   # 백터화된 연산 

        [1] FALSE  TRUE FALSE

예제:   x <- 1
         x
         (x > -2 ) && ( x < 2 )  # 백터화 되지 않은 연산 

문제10.  직업이 SALESMAN 이고 월급이 1200 이상인 사원들의  이름과
            월급과 직업을 출력하시오 !

문법: emp[ 행, 열 ] 

emp[ (emp$job=='SALESMAN')  &  (emp$sal >=1200), c("ename","sal", "job") ]

■ 연결연산자 

            오라클  -----------  파이썬 -------------- R 
                 ||                      +                     paste

문제11. 아래의 SQL을 R 로 구현하시오 !

SQL>  select ename || '의 직업은 ' || job
           from  emp;

답:  paste( emp$ename , ' 의 직업은 ' , emp$job )

1. data.table  패키지를 설치한다.
install.packages("data.table")

2. data.table 을 사용하겠다고 지정한다
library(data.table)

3. data.table 을 사용한다.
data.table( emp$ename, ' 의 직업은 ', emp$job)

문제12.  아래의 SQL을 R 로 구현하시오 !

SQL> select  ename || ' 의 월급은 ' || sal || '입니다'
           from  emp; 

library(data.table)

data.table(emp$ename,' 의 월급은 ' , emp$sal, '입니다')

문제13. 아래의 SQL을 R 로 구현하시오 !

SQL> select  ename || ' 의 월급은 ' || sal || '입니다'
          from  emp
          where  job='SALESMAN' ;

result <-  emp[ emp$job=='SALESMAN',  c("ename","sal") ]
result$ename
result$sal
library(data.table)
data.table( result$ename, ' 의 월급은 ' , result$sal, '입니다.')

■ 기타 비교연산자 

     오라클               vs                   R 
1.     in                                       %in%
2.    like                                      grep
3.    is null                                   is.na
4.    between  .. and           (emp$sal >= 1000) & (emp$sal <=3000)

문제14.  직업이 SALESMAN, ANALYST 인 사원들의 이름과 월급과 직업을
            출력하시오 !

문법:  emp[ 행, 열 ]

답: emp[ emp$job %in%  c("SALESMAN","ANALYST"), c("ename","sal","job") ]

문제15. 직업이 SALESMAN , ANALYST 가 아닌 사원들의 이름과 월급과 
           직업을 출력하시오 !

답: emp[ ! emp$job %in%  c("SALESMAN","ANALYST"), c("ename","sal","job") ]

문제16.  부서번호가 10번, 20번인 사원들의 이름과 월급과 부서번호를 출력하시오!

답: emp[ emp$deptno  %in% c(10, 20), c("ename","sal","deptno")] 

문제17.  백화점 데이터를 R 의 데이터 프레임으로 만드시오
            (데이터프레임명: x_train,  데이터: X_train.csv )

x_train <- read.csv("X_train.csv")

head(x_train,10)

문제18.  주구매지점이 강남점, 잠실점이 고객들의 고객번호와 주구매지점을
            출력하시오 !

x_train[ x_train$주구매지점 %in% c("잠실점","강남점"), c("cust_id","주구매지점")]

문제19.  위의 출력되는 건수가 몇건인지 출력하시오 !

result <- x_train[ x_train$주구매지점 %in% c("잠실점","강남점"), c("cust_id","주구매지점")]

nrow(result) 

문제20.  커미션이 null 인 사원들의 이름과 월급과 커미션을 출력하시오 !

emp[ is.na(emp$comm) ,  c("ename","sal","comm") ] 

설명: ?is.na 하시면 is.na 함수에 대한 정의와 예제를 볼 수 있습니다.

문제21.  백화점 데이터에서 환불금액의 결측치가 몇건인지 출력하시오!

result <- x_train[ is.na(x_train$환불금액), c("cust_id","환불금액") ]
nrow(result)

※ 결측치를 검색하는 함수 3가지 ?

  1. NA(결손값) ---->  is.na(컬럼)
  2. NaN(비수치)  ---> is.nan(컬럼)
      ↓
     Not a Number
  3. NULL(아무것도 없다) ---> is.null(컬럼)

문제22. 월급이 1000 에서 3000 사이인 사원들의 이름과 월급을 출력하시오

emp[ 행, 열 ]

emp[ (emp$sal >=1000) & (emp$sal<=3000) , c("ename","sal") ]

문제23. 총구매액이 2,136,000 ~  10,056,000 사이로 구매한 고객들이
          몇 명인지 출력하시오 !

a <- x_train[ (x_train$총구매액 >= 2136000) & (x_train$총구매액<= 10056000),
                    c("cust_id","총구매액") ] 

nrow(a) 

설명:   데이터 프레임의  전체 건수를 출력하고 싶다면?  nrow(emp)
         데이터 프레임의 컬럼의 건수를 출력하고 싶다면? ncol(emp)

문제24. 문제23번의 고객번호와 총구매액을 csv 파일로 생성하시오!

a <- x_train[ (x_train$총구매액 >= 2136000) & (x_train$총구매액<= 10056000),
                    c("cust_id","총구매액") ] 

write.csv( a,  "a2.csv", row.names=FALSE )

문제25. 이름의 첫글자가 A 로 시작하는 사원들의 이름과 월급을 출력하시오

답: emp[ grep("^A", emp$ename), c("ename","sal") ] 

설명:  ^ 는 시작을 나타냅니다.  예: ^A   
        $ 는 끝을 나타냅니다.     예: A$

문제26. 백화점 데이터에서 주구매상품이 '남성'으로 시작하는 상품을 구매한
          고객번호와 주구매상품을 출력하시오!

x_train[ grep('^남성', x_train$주구매상품), c("cust_id", "주구매상품") ]

문제27. 위의 결과에서 주구매상품만 출력하는데 중복을 제거해서 
           출력하시오 ! ( unique 함수 사용)

a <- x_train[ grep('^남성', x_train$주구매상품), c("주구매상품") ]
a2 <- data.table(unique(a))   # 중복제거하면서 테이블 형태로 출력
names(a2) <- '상품'            # a2 테이블의 컬럼명을 변경합니다. 
a2

문제28.  이름의 끝글자가 T 로 끝나는 사원들의 이름과 월급을 출력하시오!

emp[ grep('T$', emp$ename), c("ename","sal") ] 

문제29. 주구매상품이 '캐주얼'로  끝나는 주구매상품을 구입한 고객들의
           건수가 어떻게 되는지 출력하시오 !

a <- x_train[ grep('캐주얼$', x_train$주구매상품),  c("cust_id")) 
length(a)

※ 건수를 카운트 할때 :  nrow(데이터 프레임명)
                               length(벡터)

문제30.  이름의 두번째 철자가 M 인 사원들의 이름과 월급을 출력하시오!
             ( 한자리 : .(점하나) )

답: emp[ grep('^.M), emp$ename), c("ename","sal") ) 

문제31.  이름의 세번째 철자가 L 인 사원들의 이름과 월급을 출력하시오! 

답: emp[ grep('^..L', emp$ename), c("ename","sal") ] 

■ 중복제거 

            오라클              vs                     R 

            distinct                                  unique 

문제32.  부서번호를 출력하는데 중복을 제거해서 출력하시오 !

data.table( "부서번호" = unique(emp$deptno) )

문제33.  주구매지점을 출력하는데 중복을 제거해서 출력하시오 !

data.table("주구매지점"= unique(x_train$주구매지점) ) 

문제34. 위에서 출력된 건수를 출력하세요 !

a<- data.table("주구매지점"= unique(x_train$주구매지점) ) 
nrow(a)

■ 정렬작업 

            오라클            vs             R

           order  by                       1. data frame 에서 order 옵션
                                              2. doBy 패키지 설치하고 orderBy 함수를 
                                                 사용

문제35. 이름과 월급을 출력하는데 월급이 높은 사원부터 출력하시오 !

emp[ 행,  열 ]

emp[ order(emp$sal,  decreasing=T), c("ename","sal") ]

※ dataframe 에 내장되어 있는 order 옵션을 사용하면 정렬할 수 있습니다.

문제36. 이름과 입사일을 출력하는데 먼저 입사한 사원부터 출력하시오! 

emp[ order( emp$hiredate, decreasing=F), c("ename","hiredate") ]

문제37. 직업이 SALESMAN 인 사원들의 이름과 월급과 직업을 출력하는데
           월급이 높은 사원부터 출력하시오 !


답:  1.  직업이 SALESMAN 인 사원들의 이름과 월급을 출력해서 result 변수에 담는다

      result <-  emp[ emp$job=='SALESMAN', c("ename", "sal", "job") ]

      2.  result 변수의 월급을 높은 것부터 출력한다. 

      result[ order(result$sal, decreasing=T), c("ename", "sal", "job") ]


문제38. (오늘의 마지막 문제) 부서번호가 30번인 사원들의 이름과 월급과
          입사일을 출력하는데 먼저 입사한 사원부터 출력하시오!

R>


Pandas> 

문제39. doBy 패키지를 이용해서 이름과 월급을 출력하는데 월급이 높은
          사원부터 출력하시오 !

install.packages("doBy")
library(doBy)

setwd("d:\\data")
emp <- read.csv("emp3.csv")

orderBy( ~ -sal, emp[   , c("ename","sal") ] )

설명:  emp[  , c("ename","sal"] 이름과 월급을 출력하는 결과를
        orderBy 함수에 넣고 정렬하고자하는 sal 앞에 ~ 물결을 사용해서
        정렬하면 됩니다. 마이너스(-) 를 붙여주면 높은것부터 정렬됩니다.

문제40.  직업이 ANALYST 가 아닌 사원들의 이름과 월급과 직업을
           출력하는데 월급이 높은 사원부터 출력되게하시오 !

library(doBy)

orderBy( ~-sal,  emp[ emp$job !='ANALYST'  , c("ename","sal","job")] )

attach(emp)  

orderBy( ~-sal,  emp[  job !='ANALYST'  , c("ename","sal","job")] )

설명:  attach(emp) 를 실행하면 뒤에 이어지는 코드에서 emp$컬럼명
        이라고 하지 않고 그냥 컬럼명만 사용할 수 있습니다.

문제41.  그럼 위의 결과를 판다스로 수행하시오 !

import  pandas  as  pd

emp = pd.read_csv("d:\\data\\emp3.csv")
emp[["ename","sal","job"]][emp['job'] !='ANALYST'].sort_values('sal',ascending=False)

설명:  sort_values 가 기억이 안날때 아래와 같이 help 를 수행합니다. 

      help(pd.DataFrame.sort_values)

문제42. 범죄 발생요일(crime_day.csv) 를 R 로 로드해서 토요일에 발생하는
          범죄유형, 범죄건수를 출력하는데 범죄건수가 높은것부터 
          출력하시오!  ( R 로 해보세요)

crime_day <- read.csv("crime_day.csv")
head(crime_day)
result <- crime_day[crime_day$DAY=='SAT',c("C_T","CNT")]
library(doBy)
orderBy(~-CNT, result    )

문제43. 위의 결과를 판다스로 수행하세요 ~

import  pandas  as  pd
crime_day = pd.read_csv("d:\\data\\crime_day.csv", encoding="euckr")
crime_day[["C_T","CNT"]][crime_day["DAY"]=="SAT"].sort_values("CNT",ascending=False)

문제44.  그럼 위의 결과에서 위의 5건만 출력하시오 !

R>
crime_day <- read.csv("crime_day.csv")
head(crime_day)
result <- crime_day[crime_day$DAY=='SAT',c("C_T","CNT")]
library(doBy)
result2 <- orderBy(~-CNT, result    )
head(result2, 5)

Pandas>   
import  pandas  as  pd
crime_day = pd.read_csv("d:\\data\\crime_day.csv", encoding="euckr")
result=crime_day[["C_T","CNT"]][crime_day["DAY"]=="SAT"].sort_values("CNT",ascending=False)
result.head(5)

문제45. 지금 현재 R 에서 사용되고 있는 result 와 같은 변수 목록을 확인
         하는 방법은 무엇인가요?

ls()

문제46. 살인이 일어나는 장소와 건수를 출력하는데 건수가 높은것부터
          출력하시오 !( R로 먼저하세요) 

데이터: crime_loc.csv

crime_loc <- read.csv("crime_loc.csv")
head(crime_loc)
result <- crime_loc[crime_loc$범죄=='살인', c("장소","건수")]
library(doBy)
orderBy(~-건수, result) 

문제47. 위의 결과를 판다스로 수행하세요 ~

import pandas as pd
crime_loc = pd.read_csv("d:\\data\\crime_loc.csv", encoding="euckr")
crime_loc[["장소","건수"]][crime_loc["범죄"]=="살인"].sort_values('건수',ascending=False)

■ 문자함수

               오라클 함수             vs                 R  

                 upper                                     toupper
                 lower                                      tolower
                 substr                                     substr
                 replace                                    gsub

문제48. 이름과 직업을 출력하는데 소문자로 출력하시오 !(R로 수행하세요)

library(data.table)
data.table( 이름=tolower(emp$ename), 직업=tolower(emp$job)

문제49.  월급이 1200 이상인 사원들의 이름과 월급과 직업을
            출력하는데 이름과 직업을 소문자로 출력하세요 ! 

a <- emp[ emp$sal >= 1200, c("ename","sal","job") ]
library(data.table)
data.table(이름=tolower(a$ename), 월급=a$sal, 직업=tolower(a$job) )

문제50.  위의 결과를 판다스에서 수행하세요 ~

emp = pd.read_csv("d:\\data\\emp3.csv")
a = emp[["ename","sal","job"]][ emp["sal"] >=1200 ]
a['ename'] = a['ename'].apply(lambda  x : x.lower() )
a['job'] = a['job'].apply(lambda  x : x.lower() )
a

민석이 코드:
emp= pd.read_csv('d:\\data\\emp3.csv')
r1= emp[['ename','sal','job']] [emp['sal']>=1200].apply(lambda x: x.astype(str).str.lower())
r1.rename(columns={'ename':"이름", 'sal':"월급", 'job':"직업"})

인훈이 코드:
emp[['ename', 'sal','job']][emp['sal']>=1200].apply(lambda x:x.astype(str).str.lower())

문제51.  아래의 SQL을 R 로 구현하시오 ! 

SQL>  select  ename, substr( ename, 1, 3 )
            from  emp ;


R>   library(data.table)
       data.table( 이름=emp$ename,  철자=substr(emp$ename,1,3) )

설명:  substr( 변수, 시작, 끝 )           S M I T H     ---->  SMI  

문제52. 이름의 첫철자가 S 로 시작하는 사원들의 이름을 출력하시오 

library(data.table)
a <- emp[ substr(emp$ename,1,1)=="S" , "ename" ]
data.table(이름=a)

문제53. 위의 결과를 판다스로 수행하세요 ~~

emp[["ename"]][emp['ename'].apply(lambda x:x[0])=="S"]

12시 신호 보냈습니다. ~~

문제54.  이름, 월급을 출력하는데 월급을 출력할 때에 숫자 0 을 * 로
            출력하시오!

SQL> select  ename, replace( sal, 0, '*')
           from  emp;

R>  data.table( 이름=emp$ename, 월급=gsub(0,'*', emp$sal) )

설명: gsub(변경전, 변경후, 변수)

문제55. 위의 결과를 판다스로 수행하시오 ~

emp['sal_re'] = emp['sal'].apply(lambda x: str(x).replace('0','*'))
emp[['ename','sal_re']]

문제56. 이름과 월급을 출력하는데 월급을 출력할 때에 숫자 0, 1, 2 를
            * 로 출력하시오 ! (R 로 하세요)

SQL>  select  ename, regexp_replace( sal, '[0-2]', '*')
            from  emp;

R>  data.table( 이름=emp$ename, 월급=gsub('[0-2]','*', emp$sal) )

설명:  '[0-2]'  :  숫자 0~2까지 

문제57. 위의 결과를 판다스로 수행하시오 !

import  re   # 데이터 전처리 모듈을 임폴트 합니다. 

emp['sal_re'] = emp['sal'].apply(lambda x: re.sub('[0-2]','*', str(x)) )
emp[['ename','sal_re']]

설명:  re.sub(변경전, 변경후, 변수)

■ 숫자 함수 

     오라클           vs                 R               vs           파이썬
1.   round                             round                         round
2.   trunc                              trunc                          trunc
3.    mod                               %%                             %
4.   power                              2^3                           2**3

문제58.  이름과 월급을 출력하는데 월급을 출력할때에 십의 자리에서 
            반올림되게하시오 !

            1250 --------> 1300

data.table( 이름=emp$ename,  월급= round(emp$sal, -2) )

emp$sal2 = round(emp$sal, -2)  # emp 데이터프레임에 sal2 라는 변수를
                                          # 월급을 10의 자리에서 반올림한 값으로
                                          # 추가하겠다.

emp[     ,  c("ename","sal","sal2") ]

8   JAMES  950  1000
9    WARD 1250 1200                    1200       1250       1300  

※ R은 짝수를 좋아합니다.     
round(122.5) ------>  122                 122       122.5        123
round(123.5) -------> 124                 123       123.5        124

문제59.  이름과 월급을 출력하는데 월급을 출력할때에 십의 자리부터
            끝까지를 다 버리고 출력되게하시오 !  (trunc) 

 예:  1250  ---------->  1200

emp$sal3 = trunc( emp$sal, -2)
emp[  , c("ename","sal","sal3") ] 

설명:  R 의 trunc 함수는 소수점 이하면 버릴 수 있습니다. 
        소수점이전은 못버리므로 수행이 안됩니다. 

■ 날짜함수 

             오라클              vs             R  
            sysdate                            Sys.Date()
           add_months                       difftime
           months_between                내장함수 없음
           last_day                            내장함수 없음
           next_day                           내장함수 없음

문제60. R 에서 오늘날짜를 출력하시오!

 Sys.Date()

문제61.  이름, 입사한 날짜부터 오늘까지 총 몇일 근무했는지 출력하시오 !

SQL>  select  ename,  sysdate - hiredate
             from  emp;

R>  emp$hiredate_date = as.Date(emp$hiredate) 
      emp$hire_day =  Sys.Date() - emp$hiredate_date
      emp[   , c("ename","hire_day") ]
 
R>  emp$hiredate2 = difftime(Sys.Date(),emp$hiredate)  
      emp[  , c("ename","hiredate2") ]                          

설명: difftime(날짜2, 날짜1)  ?  날짜1 ~ 날짜2 까지의 일수 출력

문제62. 위의 결과를 판다스로 수행하시오 !

- 태진이 코드
import datetime
hire = emp.hiredate.apply(lambda x:datetime.datetime.strptime(str(x), '%Y-%m-%d %H:%M'))
emp['hiredays'] = (datetime.datetime.now()-hire).apply(lambda x:x.days)
emp[['ename','hiredays']]

- 승혁이 코드
diff = pd.datetime.now() - pd.to_datetime(emp['hiredate']) 
emp['days'] = list(map(lambda x: x.days, diff))
pd.concat([emp['ename'], emp['days']], axis=1)

- 위의 코드를 조합해서 ...
import  datetime 
emp['hiredate3']= pd.to_datetime(emp['hiredate']) # hiredate 를 날짜로 변환
                                                                # 한 hiredate3 를 생성
emp['day'] = datetime.datetime.now() - emp['hiredate3']
emp[["ename","day"]]

문제63.  오늘날짜의 달의 마지막 날짜를 출력하시오 !

SQL>  select  last_day( sysdate )
           from  dual ;

힌트코드:  install.packages("lubridate")
              library(lubridate)
              Sys.Date()   #  오늘날짜 출력    2021-06-11
              floor_date( Sys.Date(), "months")  # 2021-06-01
              ceiling_date( Sys.Date(), "months") # 2021-07-01

   ceiling_date( Sys.Date(), "months") # 2021-07-01
           ↑
     2021-06-11
           ↓
  floor_date( Sys.Date(), "months")  # 2021-06-01

   ceiling_date( Sys.Date(), "months") - days(1)   #  "2021-06-30"

문제64.  아래의 SQL을 R 로 구현하시오 !  

SQL>select  ename,  last_day(hiredate) 
          from  emp;

emp$last_day <- ceiling_date( as.Date(emp$hiredate), "months") -  days(1)

emp[    ,  c("ename","last_day") ] 

■ 변환함수 

     오라클               vs               R

    to_char                             as.character
    to_number                         as.integer
    to_date                             as.Date

문제65.  이름, 입사한 요일을 출력하시오 !

SQL>  select  ename, to_char(hiredate, 'day')
             from  emp;

R>  data.table( emp$ename, format( as.Date(emp$hiredate), '%A') ) 

설명:  format ( 특정날짜, 옵션 )

 옵션의 종류 :   %A :  요일
                     %Y :  년도 4자리
                     %y :  년도 2자리
                     %m :  달
                     %d  : 일

문제66.  위의 결과를 판다스로 출력하시오 !

emp = pd.read_csv("d:\\data\\emp3.csv")
emp['hiredate2'] = pd.to_datetime(emp['hiredate'])  # 날짜로 변환합니다. 
emp['day'] = emp['hiredate2'].dt.day_name()    #  날짜를 요일로 변환합니다. 
emp[['ename','day']]

-- 선우 코드
from datetime import datetime
emp = pd.read_csv("C://data//emp3.csv",encoding = 'euckr')
daylist = ['월', '화', '수', '목', '금', '토', '일']
emp['hiredate']= emp.hiredate.apply(lambda x:datetime.strptime(x,'%Y-%m-%d %H:%M'))

emp['days'] = emp.hiredate.apply(lambda x:daylist[datetime.date(x).weekday()])
emp

-- 동민이 코드 
a = pd.to_datetime(emp.hiredate)
for i in a:
    print(i.strftime('%a'))

문제67.  11월에 입사한 사원들의 이름과 입사일을 출력하시오(R로 수행)

설명:  format ( 특정날짜, 옵션 )

 옵션의 종류 :   %A :  요일
                     %Y :  년도 4자리
                     %y :  년도 2자리
                     %m :  달
                     %d  : 일

R>  emp[ format(as.Date(emp$hiredate), '%m')== '11' , c("ename", "hiredate") ]

문제68.  위의 결과를 판다스로 수행하세요 !

emp[ ['ename', 'hiredate']] [ pd.to_datetime(emp['hiredate']).dt.month==11]

설명: 
import  pandas as pd
help(pd.Series.dt.month)

문제69.  오늘부터 100달 뒤에 돌아오는 날짜를 출력하시오 !

SQL> select  add_months( sysdate, 100)
            from  dual;

R>  Sys.Date() + months(100) 
                  
문제70. 위의 결과를 판다스로 수행하시오 ! 

import  datetime

print ( datetime.datetime.now() )

print ( datetime.datetime.now() + pd.DateOffset(months=100))

문제71.  아래의 SQL 을 R 로 구현하시오 !

SQL> select  to_char( add_months( sysdate, 100) , 'day') 
             from  dual;


R>   format( Sys.Date() + months(100), '%A' ) 

문제72. 위의 결과를 판다스로 구현하시오 !

import  datetime

a = datetime.datetime.now() + pd.DateOffset(months=100)
a2 = pd.Series(a)
a2.dt.day_name()

문제73. 내가 무슨요일에 태어났는지 출력하시오 !

SQL>  select  to_char( to_date('1999/11/17','RRRR/MM/DD'), 'day') 
              from  dual;

R>  format( as.Date('1999/11/17'), '%A')

문제74. 위의 결과를 판다스로 수행하시오 !

a = pd.to_datetime('1999/11/17')
a2 = pd.Series(a)
a2.dt.day_name()

■ 일반함수 
     
    Oracle                  vs               R

1. nvl 함수                                is.na
2. decode                                 ifelse
3. case                                     ifelse 

문제75.  이름, 월급, 등급을 출력하는데 월급이 1500 이상이면 등급을 A로
           출력하고 아니면 B 로 출력하시오!

data.table( emp$ename, emp$sal, ifelse( emp$sal >=1500, 'A', 'B') ) 

문제76.  위의 결과를 판다스로 수행하시오 !

def func1(num):
    if num >= 1500:
        return 'A'
    else:
        return 'B'
    
emp['grade']=emp['sal'].apply(func1)
emp[['ename','grade']]

문제77.  이름, 월급, 등급을 출력하는데 월급이 3000 이상이면 A 등급,
                               2000 이상이고 3000보다 작으면 B등급,
                               나머지는 C등급으로 출력하시오 !

R> emp$grade <- ifelse( emp$sal >= 3000, 'A', 
                        ifelse( emp$sal >= 2000, 'B', 'C') ) 

     emp[   , c('ename','grade') ]
    
문제78. (오늘의 마지막 문제)  백화점 데이터에 고객번호, 총구매액, 등급을
           출력하는데 총 구매액이 1억이상이면 A 등급을 출력하고 
           5천만원 이상이면 B등급을 출력하고 
           2천만원 이상이면 C등급을 출력하고 나머지 고객들은 D등급을 
           출력하시오 !

R>



Pandas>        


■ R 함수

* 단일행 함수 
 1. 문자함수 : upper, lower, substr, replace
 2. 숫자함수 : round, truc, %%, 2^3
 3. 날짜함수 : Sys.Date(), difftime
 4. 변환함수 :  as.Character, as.integer, as.Date, format
 5. 일반함수 :  is.na,  ifelse

* 그룹함수 
    
      Oracle                 vs                    R

1.    max                                          max
2.    min                                           min
3.    sum                                          sum
4.    avg                                           mean
5.    count                                        length(세로)
                                                     table(가로)

문제79. 최대월급을 출력하시오 !

emp <-  read.csv(d:\\data\\emp3.csv")

R>    max(emp$sal)

emp = pd.read_csv("d:\\data\\emp3.csv")

Pandas>  emp['sal'].max()

문제80. 직업이 SALESMAN 인 사원들의 최대월급을 출력하시오 !

R> result <- emp[emp$job=="SALESMAN", "sal"]
    max(result)


Pandas>  result = emp.loc[ emp['job']=='SALESMAN', 'sal' ].max()
             또는
             result = emp.loc[ emp['job']=='SALESMAN', 'sal' ]
             print(max(result.values))

문제81. 부서번호가 20번인 사원들중에서의 최소월급을 출력하시오 !

R> result <-  emp[ emp$deptno==20,  'sal' ]
    min(result)


Pandas> result = emp.loc[ emp['deptno']==20, 'sal'].min()
            print(result) 

문제82.  부서번호, 부서번호별 최대월급을 출력하시오 !

R>  x <- aggregate( sal~job, emp, max )  
     names(x) <- c("job","sumsal")
     x

※설명:  aggregate(값 컬럼명~그룹핑할 컬럼명, 테이블명, 그룹함수)

Pandas> x = emp.groupby('job')['sal'].max().reset_index()
            x.columns = ['job','sumsal']
            x

※설명: df.groupby('그룹핑할 컬럼명')['값컬럼명'].그룹함수().reset_index()
                                                                               ↑
                                                       데이터 프레임으로 변환하는 함수

문제83.  부서번호, 부서번호별 토탈월급을 출력하시오 !

SQL>  select  deptno, sum(sal)
            from  emp
            group  by  deptno;

R> x <- aggregate(sal~deptno, emp, sum )
    names(x) <- c("deptno","sumsal")
    x

Pandas> x = emp.groupby('deptno')['sal'].sum().reset_index()
            x.columns = ['deptno','sumsal']
            x

문제84. 위의 결과를 다시 출력하는데 토탈월급이 높은것부터 출력하시오 !

R>  library(doBy)
     orderBy( ~-sumsal, x )

※ 설명:  orderBy( ~-정렬할 컬럼명, 데이터프레임명)

Pandas>  x.sort_values('sumsal', ascending=False)

문제85. 직업, 직업별 인원수를 출력하시오 !

R> x <- aggregate(empno~job, emp, length)
    names(x) <- c("직업","인원수")
    x

Pandas> x = emp.groupby('job')['sal'].count().reset_index()
            x.columns= ["직업","인원수"]
            x

문제86. 위의 결과를 다시 출력하는데 인원수가 높은것부터 출력하시오!

R>  library(doBy)
     orderBy( ~-인원수, x)

Pandas>   x.sort_values('인원수', ascending= False) 

문제87. 직업과 직업별 토탈월급을 출력하는데 직업별 토탈월급이 4000 
          이상인것만 출력하시오 ~

R> x <- aggregate( sal~job,  emp,  sum )
    names(x) <- c("job","sumsal")
    x[ x$sumsal >= 6000, c("job","sumsal") ]

Pandas>  x = emp.groupby('job')['sal'].sum().reset_index()
             x.columns = ["job","sumsal"]
             x.loc[ x['sumsal'] >= 6000,  ["job","sumsal"] ]

문제88. 위의 결과를 다시 출력하는데 토탈월급이 높은것부터 출력하시오 !

R> x <- aggregate( sal~job,  emp,  sum )
    names(x) <- c("job","sumsal")
    x[ x$sumsal >= 6000, c("job","sumsal") ]
    x2<-  x[ x$sumsal >= 6000, c("job","sumsal") ]
    library(doBy)
    orderBy( ~-sumsal, x2)

Pandas>  x = emp.groupby('job')['sal'].sum().reset_index()
             x.columns = ["job","sumsal"]
             x2 = x.loc[ x['sumsal'] >= 6000,  ["job","sumsal"] ]
             x2.sort_values('sumsal', ascending=False)

문제89. 아래의 SQL을 R 과 판다스로 구현하시오 !

SQL>  select  deptno, job, sum(sal)
            from  emp
            group  by  deptno, job
            order by deptno, job; 

R>  x <- aggregate(sal~deptno+job, emp, sum)
     library(doBy)
     orderBy( ~deptno, x) 

Pandas> emp.groupby(['deptno','job'])['sal'].sum().reset_index()

문제90.  사원 테이블에서 입사한 년도를 4자리로 출력하시오 !

R> library(data.table)

     data.table( format( as.Date(emp$hiredate), "%Y"))

Pandas>  emp['hiredate'] = pd.to_datetime(emp['hiredate'])
             emp.info()
             emp['hiredate'].dt.year

문제91. 아래의 SQL을 R 과 판다스로 구현하시오 !

SQL> select  to_char(hiredate,'RRRR'), deptno, sum(sal)
           from  emp
           group  by  to_char(hiredate,'RRRR'), deptno;

R> hire_year <- format( as.Date(emp$hiredate),'%Y') 
    aggregate(sal~hire_year+deptno, emp, sum)

Pandas> emp['hire_year'] = pd.to_datetime(emp['hiredate']).dt.year
            emp.groupby(['hire_year','deptno'])['sal'].sum().reset_index()

문제92. 아래의 SQL을 R 와 판다스로 구현하시오 !

SQL>  select  *
              from  ( select  deptno, sal from  emp )
              pivot ( sum(sal) for  deptno in (10, 20, 30) ) ;

R>  attach(emp)
      tapply( sal, deptno, sum )
         
Pandas>  x=emp.pivot_table( columns='deptno', aggfunc='sum')
             x

문제93.  직업, 직업별 인원수를 가로로 출력하시오 

SQL>  select  *
          from  ( select  job, empno  from  emp )
          pivot ( count(empno) for job  in ('ANALYST','CLERK','MANAGER',
                                                    'SALESMAN', 'PRESIDENT') )  

R>  emp <- read.csv("emp3.csv")
      tapply ( emp$empno, emp$job,  length)

pandas> emp.pivot_table( columns='job', values='empno', aggfunc='count')

문제94. 위의 결과를 막대 그래프로 시각화 하시오 !

R>
x  <- tapply ( emp$empno, emp$job,  length)

barplot( x ,  ylim=c(0,5), main="직업별 인원수", col=rainbow(5), density=50)

Pandas>
x = emp.pivot_table( columns='job', values='empno', aggfunc='count')
print(type(x) )

x = emp.pivot_table( columns='job', values='empno', aggfunc='count').unstack()
x2 = pd.Series(x)
x2.plot(kind='bar')

※ 설명: unstack() 함수는 가로를 세로로 변경 또는 세로를 가로로 변경 합니다. 

R 은 데이터의 결과를 가로로 출력을 해야 그래프 그리기가 편하고
Pandas 는 데이터의 결과를 세로로 출력해야 그래프 그리기가 편하다. 
Pandas 는 Series 로 변환해줘야 plot 메소드를 사용할 수 있습니다. 
                ↓
          오라클의 테이블의 컬럼이라고 생각하면됨 

문제95.  부서번호, 부서번호별 토탈월급을 막대그래프로 시각화 하시오 !

R>
x  <- tapply ( emp$sal, emp$deptno,  sum)

barplot( x , main="부서번호별 토탈월급", col=rainbow(5), density=50)

Pandas>
x = emp.pivot_table( columns='deptno', values='sal', aggfunc='sum').unstack()
x2 = pd.Series(x)
x2.plot(kind='bar')


문제96. 아래의 SQL 을 R과 판다스로 구현하시오 !

SQL> select  job, 
                 sum( decode(deptno, 10, sal, null) )  as "10",
                 sum( decode(deptno, 20, sal, null) )  as "20",
                 sum( decode(deptno, 30, sal, null) )  as "30"
          from  emp
          group  by  job;

R>  attach(emp)
      tapply(  sal,  list(job, deptno), sum )


Pandas> 

emp.pivot_table(columns = 'deptno',index = 'job',values = 'sal',aggfunc = 'sum')

문제97.  입사한 년도,  부서번호, 입사한 년도별 부서번호별 토탈월급을 가로로 출력
           하시오 !

SQL>  select  to_char( hiredate,'RRRR'), 
                  sum( decode( deptno, 10, sal, null) )  as  "10",
                  sum( decode( deptno, 20, sal, null) )  as  "20",
                  sum( decode( deptno, 30, sal, null) )  as  "30"
            from  emp
           group  by  to_char(hiredate,'RRRR');

R> hire_year <- format( as.Date(emp$hiredate), '%Y')  
     tapply( emp$sal, list( hire_year, emp$deptno), sum)  

pandas> 

emp['hire_year'] = pd.to_datetime( emp['hiredate']).dt.year
emp.pivot_table(columns = 'deptno',index = 'hire_year',values = 'sal',aggfunc = 'sum')


문제98.  위의 결과에서 NA 값이 0 이 되게하시오 !

           10   20   30
1980   NA  800   NA
1981 7450 5975 9400
1982 1300 3000   NA
1983   NA 1100   NA

R> hire_year <- format( as.Date(emp$hiredate), '%Y')  
    x <- tapply( emp$sal, list( hire_year, emp$deptno), sum) 
    is.na(x) 
    x[ is.na(x) ] <- 0   # na 를 0 으로 치환해라 ~
    x

Pandas> 
x = emp.pivot_table(columns = 'deptno',index = 'hire_year',values = 'sal',aggfunc = 'sum')
x.fillna(0, inplace=True) # x 데이터 프레임의 nan 을 0 으로 변경해라 ~
x

문제99.  위의 결과를 막대 그래프로 시각화 하시오 !

R>  colnames(x)   # 컬럼이름을 출력
      rownames(x)  #  로우이름을 출력 

barplot( x, col=rainbow(4), legend=rownames(x), beside=T )

설명:  legend 는 그래프의 설명 박스 입니다.
        beside=T 를 해야 직업별로 각각 막대그래프가 그려집니다. 

Pandas> 
x = emp.pivot_table(columns = 'deptno',index = 'hire_year',values = 'sal',aggfunc = 'sum')
x.fillna(0, inplace=True) # x 데이터 프레임의 nan 을 0 으로 변경해라 ~
x.plot(kind='bar')

x.plot(kind='bar', figsize = (4,6) , rot=False)

설명:  가로 4, 세로 6으로 그래프 사이즈를 키우고 rot=False 를 써서
         x 축의 이름을 회전시킴

문제100. 직업, 직업별 토탈월급을 가로로 출력하시오 !

R> tapply( emp$sal, emp$job, sum )

pandas> emp.pivot_table(columns = 'job',values = 'sal',aggfunc = 'sum')

문제101.  위의 결과를 원형(pie) 그래프로 그리시오 !

R> x <- tapply( emp$sal, emp$job, sum )
    pie(x, col=rainbow(5) , density=80)
    x2 <- aggregate( sal~job, emp, sum) 
    pct <- round(x2$sal/sum(emp$sal) * 100,1)
    job_label <- paste( x2$job, ':', pct, '%')
    job_label 
    pie(  x, col=rainbow(5) , density=80, labels=job_label) 

Pandas> 
import numpy as np
x =emp.pivot_table(columns = 'job',values = 'sal',aggfunc = 'sum')
x.iloc[0,:].plot(kind='pie', autopct='%0.0f%%')

명규코드:
x = emp.pivot_table(columns='job',values='sal',aggfunc='sum')
x.iloc[0].plot(kind='pie',autopct='%0.0f%%', figsize=(5,5), fontsize=10)

문제102.  부서번호, 부서번호별 토탈월급을 원형 그래프로 시각화 하시오!

R> x <- tapply( emp$sal, emp$deptno, sum )
    pie(x, col=rainbow(5) , density=80)
    x2 <- aggregate( sal~deptno, emp, sum) 
    pct <- round(x2$sal/sum(emp$sal) * 100,1)
    deptno_label <- paste( x2$deptno, ':', pct, '%')
 
    pie(  x, col=rainbow(5) , density=80, labels=deptno_label) 

Pandas> 
x = emp.pivot_table(columns='deptno',values='sal',aggfunc='sum')
x.iloc[0].plot(kind='pie',autopct='%0.0f%%', colors=['red','blue','lime'] )


색깔 변경하기 

https://matplotlib.org/stable/gallery/color/named_colors.html

x = emp.pivot_table(columns='deptno',values='sal',aggfunc='sum')
x.iloc[0].plot(kind='pie',autopct='%0.0f%%', colors=['red','dodgerblue','lime'] )

■  SQL 과 R  조인문법 비교 

   Oracle          vs               R 

   equi join
 non equi join                   merge 
  outer join
  self  join 

예제1.  dept.csv 를 R 로 로드해서 dept 데이터 프레임을 만드세요~

dept  <- read.csv("dept.csv")

예제2. 이름과 부서위치를 출력하시오 !

SQL>  select  e.ename,  d.loc
            from  emp  e,  dept  d
            where  e.deptno = d.deptno ;

R> x <- merge( emp, dept, by="deptno")
    x[    ,  c("ename","loc") ]

Pandas> x = pd.merge( emp,dept, on="deptno")
            x.loc[ :, ['ename', 'loc'] ]

문제103.  DALLAS 에서 근무하는 사원들의 이름과 부서위치를 출력하시오

R> x <- merge( emp, dept, by="deptno")
    x[ x$loc=='DALLAS'  ,  c("ename","loc") ]

Pandas> x = pd.merge( emp,dept, on="deptno")
            x.loc[ x['loc']=='DALLAS' , ['ename', 'loc'] ]

설명:  df.loc[행, 열]   ,   예: emp.loc[emp['sal']==3000, ["ename","sal"] ]
        df.iloc[행번호, 열번호], 예: emp.iloc[ 2, 0:3 ] 
        df[[열]][행]

문제104. 직업이 SALESMAN 이고 월급이 1000 이상인 사원들의 이름과
            월급과 직업과 부서위치를 출력하시오 !


R> 
x <- merge( emp, dept, by="deptno")
x[ (x$job=="SALESMAN") & (x$sal >=1000)  , c("ename", "sal", "job", "loc") ]

Pandas> 
x = pd.merge( emp,dept, on="deptno")
x.loc[ (x['job']=="SALESMAN") & (x['sal'] >=1000) , ['ename','sal','job', 'loc'] ]

문제105.  커미션이 NA 인 사원들의 이름과 부서위치와 커미션을 출력하시오

R>
x <- merge( emp, dept, by="deptno")

x[ is.na(x$comm),  c("ename", "loc", "comm") ]

Pandas> 
x = pd.merge( emp, dept, on="deptno")

x.loc[ x['comm'].isna(), ['ename','loc','comm'] ]

문제106. (오늘의 마지막 문제) 부서위치, 부서위치별 토탈월급을
           원형 그래프로 시각화 하시오 !


Pandas>

import matplotlib.pyplot as plt
import pandas as pd
emp = pd.read_csv("C://data//emp3.csv",encoding = 'utf8')
dept = pd.read_csv("C://data//dept.csv",encoding = 'utf8')
ALL = pd.merge(emp,dept, on = 'deptno')
label = X.columns
explode = [0.03]*3
colors = ['#ff9999', '#ffc000', '#8fd9b6']
X = ALL.pivot_table(columns = 'loc',values = 'sal',aggfunc = 'sum')
plt.pie(X,labels = label,autopct= '%0.0f%%',explode=explode,shadow = True,colors = colors)
plt.show()

 
문제107.  월급이 1000 에서 3000 사이인 사원들의 이름과 월급과 부서위치를 
             출력하시오 !

SQL> select  e.ename,  e.sal, d.loc
           from  emp  e,  dept  d
           where  e.deptno = d.deptno  and  e.sal  between  1000  and 3000;


R>
emp_dept = merge(emp, dept, by="deptno")
emp_dept[ (emp_dept$sal) >= 1000 & (emp_dept$sal <=3000) ,c("ename","sal","loc")]

Pandas>
emp_dept = pd.merge( emp, dept, on="deptno")
a = emp_dept.loc[ emp_dept['sal'].between(1000,3000), ['ename','sal','loc'] ]
print ( len( a.index) )


문제108. 위의 결과를 다시 출력하는데 월급이 높은 사원부터 출력하시오!

R>
emp_dept = merge(emp, dept, by="deptno")
x <- emp_dept[ (emp_dept$sal) >= 1000 & (emp_dept$sal <=3000) ,c("ename","sal","loc")]
library(doBy)
orderBy( ~-sal, x) 


Pandas>
emp_dept = pd.merge( emp, dept, on="deptno")
a = emp_dept.loc[ emp_dept['sal'].between(1000,3000), ['ename','sal','loc'] ]
a.sort_values('sal', ascending=False)

■ 오라클 조인문법 4가지  ?

  1.  equi join
  2. non equi  join
  3. outer  join
  4. self  join 

문제109.  아래의 SQL을 R 과 Pandas 로 구현하시오 !

SQL> select e.ename,  d.loc
           from  emp  e,  dept  d
           where  e.deptno (+) = d.deptno;

R> x <- merge( emp, dept,  by="deptno", all.y=T)
     x[     ,  c("ename", "loc") ] 

※설명: all.y=T 는 dept 테이블 쪽의 데이터가 모두 나오게해라 ~~

Pandas> x = pd.merge( emp, dept, on="deptno",  how="right")
            x.loc[ :,  ['ename', 'loc'] ]

※설명: how="right" 를 써주면 오른쪽 즉 y 쪽을 다 나오게해라 ~
          how= "left" 를 써주면 왼쪽 즉 x 쪽을 다 나오게해라 ~
          how="outer" : 오라클의 full outer join 과 똑같다.
          how="inner" :  오라클의 equi join 과 똑같다. (pd.merge 의 기본값)

문제110.  아래의 SQL을 R 과 판다스로 구현하시오 !

SQL>  select  e.ename, d.loc
           from  emp  e,  dept   d
           where  e.deptno = d.deptno (+);

R> x <- merge( emp, dept,  by="deptno", all.x=T)
     x[     ,  c("ename", "loc") ] 

※설명: all.x=T 는 emp 테이블 쪽의 데이터가 모두 나오게해라 ~~

Pandas> x = pd.merge( emp, dept, on="deptno",  how="left")
            x.loc[ :,  ['ename', 'loc'] ]

문제111.  아래의 SQL을 R과 판다스로 구현하시오 !

SQL>  select  e.ename,  d.loc
            from  emp  e   full  outer  join  dept   d
            on  ( e.deptno = d.deptno ); 

R> x <- merge( emp, dept,  by="deptno", all=T)
     x[     ,  c("ename", "loc") ] 

※설명: all=T 는 emp,dept 테이블 둘다 데이터가 모두 나오게해라 ~~

Pandas> x = pd.merge( emp, dept, on="deptno",  how="outer")
            x.loc[ :,  ['ename', 'loc'] ]

■ 오라클 조인문법 4가지  ?

  1.  equi join
  2. non equi  join
  3. outer  join
  4. self  join  : 자기 자신의 테이블과 조인하는 조인문법

문제112. 아래의 SQL을 R 과 판다스로 구현하시오 !

SQL>  select  사원.ename, 관리자.ename
             from  emp  사원, emp 관리자
             where  사원.mgr = 관리자.empno;

R> x <- merge( emp,  emp, by.x = "mgr", by.y="empno") 
                      ↑      ↑
                      x        y

     x[      ,  c("ename.x", "ename.y") ]

Pandas>  
             x = pd.merge( emp, emp, right_on="mgr", left_on="empno")
             x.loc[ : , ["ename_y","ename_x"] ]

문제113.  위의 결과를 다시 출력하는데 자기의 월급이 자기의 관리자
             즉 직속상사보다 더 많은 월급을 받는 사원들의 사원 이름,
             사원월급, 관리자 이름, 관리자 월급을 출력하시오 ~

SQL>  select  e.ename, e.sal, m.ename, m.sal
           from  emp  e,  emp  m
           where  e.mgr = m.empno    and  e.sal > m.sal;  

R> x <- merge( emp,  emp, by.x = "mgr", by.y="empno") 
                      ↑      ↑
                      x        y

     x[  x$sal.x > x$sal.y   ,  c("ename.x", "sal.x", "ename.y", "sal.y") ]

Pandas>  
x = pd.merge( emp, emp, right_on="mgr", left_on="empno")

x.loc[  x["sal_y"] > x["sal_x"]  , ["ename_y","sal_y", "ename_x", "sal_x"] ]

문제114.  위의 self join 한 결과 데이터로 사원이름과 직속상사의 이름을 
             가지고 사원 테이블의 조직도를 그리시오 !

x <- merge( emp, emp, by.x="mgr" , by.y="empno")
a <- x[           , c("ename.x", "ename.y") ] 
a

install.packages("igraph")
library(igraph)
b <-  graph.data.frame(a, directed=T)
plot(b)

문제115.  위의 그래프를 구글의 googleVis 를 이용해서 시각화 하시오!

install.packages("googleVis")
library(googleVis)

a <- merge(emp,emp, by.x="empno",by.y="mgr", all.y=T)

org <- gvisOrgChart(a, idvar="ename.y",parentvar="ename.x",
        options=list(width=600, height=250, size='middle', allowCollapse=T))

plot(org)


■ 오라클 조인문법 4가지  ?

  1.  equi join
  2. non equi  join :  두 테이블 사이에 서로 공통된 컬럼이 없었을때의 
                           조인방법
  3. outer  join
  4. self  join 

문제116.  아래의 non equi join 을 R과 판다스로 구현하시오 

SQL> select  e.ename, e.sal, s.grade
          from  emp  e,  salgrade   s
          where  e.sal between  s.losal  and s.hisal;

salgrade <- read.csv("salgrade.csv")

R> 조인할 수 없습니다. 
pandas> 조인할 수 없습니다.

문제117.  부서위치, 부서위치별 토탈월급을 출력하시오 ! (세로)

R>  x <- merge( emp, dept,  by="deptno", all=T)
     aggregate( x$sal ~ x$loc,  x,  sum , na.action= na.pass) 

※ na.action= na.pass 를 써야 boston 도 출력됩니다. 

pandas> 
x = pd.merge( emp, dept, on="deptno", how="outer")
x.fillna(0,inplace=True)
x.groupby('loc')['sal'].sum().reset_index()

또는 
x = pd.merge( emp, dept, on="deptno", how="outer")
x.groupby('loc', dropna=False)['sal'].sum().reset_index()

코로나 데이터를 내려받고 날씨와 확진자 발생수는 상관이 있을까?

문제118.  코로나 데이터를 내려받고 weather.csv 와 time.csv 를
             서로 조인하세요 !

통계기법: 상관관계
데이터 :  weather.csv,  time.csv 

1. 포트폴리오는 쉬운 주제로 한다. 대신 분석 목표를 명확한다.
   기술을 현란하게 하는것 보다는 질문에 대한 답을 명확하게 
   설명하는게 중요하다.
2. 질문에 대한 답을 데이터를 통해 찾을수 있게 해야한다.
3. 분석한 코드와 시각화 내용을 어디서든지 다운 받을 수 있게 준비

판다스>
tm = pd.read_csv("d:\\data\\time.csv")
wt = pd.read_csv("d:\\data\\weather.csv")
cov = pd.merge( tm, wt, on="date")
cov.head()

문제119.  날짜(date), 평균기온(avg_temp), 확진자수(confirmed) 를 출력하시오

판다스>

cov.loc[  :  ,  ['date', 'avg_temp', 'confirmed'] ]

문제120.  confirmed 를 출력하고 그 전행의 confirmed 를 출력하시오 !

cov['confirmed2'] = cov['confirmed'].shift(1)

cov.loc[ :  , ['confirmed', 'confirmed2'] ]

문제121.  confirmed 와 confirmed2 의 차이를 구해서  day_raise 라는
             파생 변수로 생성하고 province 가 Seoul 인 곳의 날짜와 
              일일 발생 확진자수를 출력하시오 

cov['day_raise'] = cov.loc[ : , 'confirmed'] - cov.loc[ : , 'confirmed2']
cov.loc[ cov['province']=='Seoul' , ['date','day_raise'] ]

문제122.  위의 데이터를 라인 그래프로 시각화 하시오 !

cov['day_raise'] = cov.loc[ : , 'confirmed'] - cov.loc[ : , 'confirmed2']
cov.loc[ cov['province']=='Seoul' , ['date','day_raise'] ].plot(kind='line')

#데이터프레임 복사

df = cov
#date 컬럼 type 확인
type(df.date[0])

#date컬럼 date_time으로 변환
df.date = pd.to_datetime(df.date)

#그래프 그리기
import matplotlib.pyplot as plt

plt.plot(df.date,df.day_raise)



문제123. 서울 데이터를 가지고 평균기온과 일일 확진자수가 상관관계가
            있는지 상관계수를 출력하시오 !

cov_seoul = cov.loc[ cov['province']=='Seoul' , ['date','avg_temp','day_raise'] ]
cov_seoul.corr(method='pearson')
상관계수가 0~1 사이의 실수  

               avg_temp	day_raise
avg_temp  1.000000	-0.226361
day_raise	 -0.226361	1.000000

평균 온도가 올라갈수록 일일 확진자수가 줄어드는 낮은 상관관계를 보이고 있다

문제124. numpy 를 이용해서 평균기온과 일일 확진자수에 대한 상관계수를
            구하시오 !

import  numpy  as  np
import  numpy.ma  as  ma

cov_seoul = cov.loc[ cov['province']=='Seoul' , ['date','avg_temp','day_raise'] ]

A = cov_seoul['avg_temp']
B = cov_seoul['day_raise']

print(ma.corrcoef(ma.masked_invalid(A), ma.masked_invalid(B)))

설명: Nan 값이 있을 경우에는 numpy 는 numpy.ma 를 이용해야 합니다. 

판다스는 이렇게 간단하게 됨 :  데이터프레임.corr(method='pearson')

문제125.  아래의 SQL을 R 과 판다스로 구현하시오 !

SQL> select  d.loc, sum( decode(e.deptno, 10, e.sal, null) )  as "10",
                          sum( decode(e.deptno, 20, e.sal, null) )  as "20",
                          sum( decode(e.deptno, 30, e.sal, null) )  as "30"
        from   emp  e, dept  d
        where  e.deptno = d.deptno
        group  by  d.loc;

R>  x <-  merge( emp, dept, by="deptno", all=T)
     tapply( x$sal, list(x$loc,x$deptno) , sum) 

설명: tapply( 값컬럼, 그룹핑할 컬럼명, 그룹함수)  
       tapply 함수는 그룹핑할 결과를 가로로 출력해주는 함수

Pandas> 
x = pd.merge( emp, dept,  on="deptno", how="outer")
x.pivot_table( index='loc',columns='deptno', values='sal', aggfunc = 'sum')

문제126.  아래의 SQL을  R 과 판다스로 구현하시오 !

SQL> select  d.loc,  sum( decode( e.job, 'ANALYST', e.sal, null) ) as "ANALYST",
                          sum( decode( e.job, 'CLERK', e.sal, null) ) as "CLERK",
                          sum( decode( e.job, 'MANAGER', e.sal, null) ) as "MANAGER",
                          sum( decode( e.job, 'PRESIDENT', e.sal, null) ) as "PRESIDENT",
                          sum( decode( e.job, 'SALESMAN', e.sal, null) ) as "SALESMAN"
        from  emp  e, dept  d
       where  e.deptno = d.deptno
       group  by   d.loc;


R>  x <-  merge( emp, dept, by="deptno", all=T)
     tapply( x$sal, list(x$loc,x$job) , sum) 

Pandas> 
x = pd.merge( emp, dept,  on="deptno", how="outer")
x.pivot_table( index='loc',columns='job', values='sal', aggfunc = 'sum')

문제127.  입사한 년도(index), 부서위치(column), 입사한 년도별 부서위치별
             토탈월급을 출력하시오 ! (R 로 수행하세요)

         BOSTON   CHICAGO   DALLAS   NEW YORK 
1980
1981
1982
1983

x <- merge( emp, dept, by="deptno", all=T)
tapply( x$sal,  list( format(as.Date(x$hiredate),'%Y'), x$loc), sum )

문제128. 위의 결과를 판다스로 수행하세요 ~

x = pd.merge(emp, dept, on="deptno", how='outer')
x['hiredate'] = pd.to_datetime(x['hiredate'])
x['hire_year'] = x['hiredate'].dt.year

x.pivot_table( index='hire_year', columns='loc', values='sal', aggfunc='sum')

문제129.  코로나 데이터에서 인덱스는 도시명, 컬럼명은 월별로 하고 
             값은 일일 확진자수로 하고 월별 토탈값을 출력하시오 !
              (판다스로 수행하세요)

cov = pd.merge( weather, time, on='date') 
cov['confirmed2'] = cov['confirmed'].shift(1)
cov['day_raise'] = cov['confirmed'] - cov['confirmed2']
cov['date'] = pd.to_datetime(cov['date'])
cov['date_month']  = cov['date'].dt.month
cov['date_month']
cov.pivot_table(index='province', columns='date_month', values='day_raise',aggfunc=sum)

■ 집합 연산자  

            SQL                  vs                 R  

1.  union all                                          rbind
2.  union                                              rbind + unique
3.  intersect                                           intersect
4.  minus                                              setdiff 

※ setdiff 의 경우 R 에 내장되어있는 setdiff 를 사용하면 안되고
   dplyr 패키지의 setdiff 를 이용해야 됩니다. 

문제130. 아래의 SQL 의 결과를 R 로 구현하시오 !

SQL> select ename, sal, deptno
            from emp
            where deptno in (10, 20)
         union  all
         select  ename, sal, deptno
             from  emp
             where deptno = 20;

R>  rbind( emp[ emp$deptno  %in%  c(10,20), c("ename","sal","deptno")] ,
              emp[ emp$deptno ==  20, c("ename","sal","deptno") ] )

설명: rbind( 결과1, 결과2)  
 
  결과1과 결과2가 위 아래로 출력됩니다. 

문제131.  아래의 SQL을 R 로 구현하시오 !

SQL> select  deptno, sum(sal)
            from  emp
            group   by  rollup(deptno);

R> 
attach(emp)
rbind( aggregate( sal~deptno, emp, sum), c("  ", sum(emp$sal) ) )

※ 설명:  rbind 는 두개의 결과를 위아래로 출력하고 싶을때 사용하는 함수
           cbind 는 두개의 결과를 양옆으로 출력하고 싶을때 사용하는 함수 

문제132.  아래의 R 문법을 SQL 로 구현하시오 !

a <- aggregate( sal~deptno, emp, sum)
b <- aggregate( sal~deptno, emp, mean)
c <- cbind( a, b$sal)
names(c) <- c("부서번호","토탈월급","평균월급")
c

SQL> select  deptno, sum(sal) as 토탈월급, avg(sal) as 평균월급
           from  emp
           group by deptno;

문제133.  아래의 SQL을 R 로 구현하시오 !

SQL>  select  ename, sal, deptno
            from  emp
            where  deptno in ( 10, 20)
         union
         select  ename, sal, deptno
           from  emp
           where deptno = 10; 

※ SQL 의 union 과 union all 의 차이점 ?  1. 중복된 데이터를 제거
                                                      2. 데이터를 정렬합니다. 


x <- unique( rbind( emp[ emp$deptno  %in%  c(10,20), c("ename","sal","deptno")] ,
                 emp[ emp$deptno ==  20, c("ename","sal","deptno") ] )  )

x

문제134.  아래의 SQL을 R 로 구현하시오 !

SQL>  select  ename, sal, deptno
            from  emp
            where  deptno in ( 10, 20)
         minus
         select  ename, sal, deptno
           from  emp
           where deptno = 10; 

R>  install.packages("dplyr")   
      library(dplyr)

      setdiff( emp[ emp$deptno  %in%  c(10,20), c("ename","sal","deptno")] , 
               emp[ emp$deptno ==  20, c("ename","sal","deptno") ] ) 

※ 설명: R 내장함수에도 setdiff 가 있고 dplyr 패키지에도 setdiff 가 있는데
          위의 경우는 dplyr 을 라이브러리 했기 때문에 dplyr 의 setdiff 
          함수를 이용하는 것입니다. 

문제135.  아래의 SQL을 R 로 구현하시오 !

SQL>  select  ename, sal, deptno
            from  emp
            where  deptno in ( 10, 20)
         intersect 
         select  ename, sal, deptno
           from  emp
           where deptno = 10; 

R>  library(dplyr)

      intersect( emp[ emp$deptno  %in%  c(10,20), c("ename","sal","deptno")] , 
                  emp[ emp$deptno ==  20, c("ename","sal","deptno") ] ) 

아래의  4가지를 다 구현해봤습니다.

            SQL                  vs                 R  

1.  union all                                          rbind
2.  union                                              rbind + unique
3.  intersect                                           intersect
4.  minus                                              setdiff 

■ SQL 의 서브쿼리를 R 로 구현하기 

문제136.  아래의 서브쿼리문을 R 로 구현하시오 !

SQL>  select  ename, sal
           from  emp
           where sal > ( select  sal
                               from emp
                               where  ename='JONES');

R>  jones_sal <-  emp[ emp$ename=='JONES', c("sal") ]
     jones_sal
     emp[ emp$sal > jones_sal , c("ename","sal") ]

문제137. 아래의 SQL을 R 로 구현하시오 !

SQL> select  ename, sal
          from  emp
          where  sal = ( select  max(sal)
                               from emp );

R>  max_sal = max(emp$sal) 
      emp[ emp$sal ==max_sal, c("ename","sal") ]

문제138. 전국에서 등록금이 가장 비싼 학교이름과 등록금을 출력하시오 !

univ <- read.csv("전국_대학별등록금통계_현황.csv")
colnames(univ)

문제139. (오늘의 마지막 문제) 위의 결과를 판다스로 수행하세요 ~

나머지 시간은 자유롭게 자습 또는 스터디 하시면 됩니다. ~

요번주 토요일에 빅분기 실기시험이 있습니다.

자기 자리에 있는 책이라든가 귀중품은 정리해주세요 ~

1장. R 기본문법 

       1. R 로 데이터 검색
       2. R 함수
       3. R 로 조인하기
       4. R 집합 연산자
       5. R 로 서브쿼리 구현하기

문제140.  SMITH 와 같은 부서번호에서 근무하는 사원들의
             이름과 월급과 부서번호를 출력하시오 !

SQL> select  ename, sal, deptno
           from  emp
           where  deptno = ( select  deptno
                                     from  emp
                                     where  ename='SMITH' );

R> a <- emp[ emp$ename=='SMITH', 'deptno' ]
     emp[ emp$deptno == a ,  c("ename", "sal", "deptno") ]

문제141.  위의 결과를 판다스로 수행하시오 ~

a = emp.loc[ emp['ename']=='SMITH',  'deptno' ].values[0]

emp.loc[ emp['deptno'] ==a, ['ename', 'sal', 'deptno'] ]

■  오라클의 서브쿼리 3가지

1. 단일행 서브쿼리

SQL> select  ename, sal, deptno
           from  emp
           where  deptno = ( select  deptno
                                     from  emp
                                     where  ename='SMITH' );

2. 다중행 서브쿼리

SQL> select  ename, sal, deptno
           from  emp
           where  deptno in ( select  deptno
                                     from  emp
                                     where  job='SALESMAN' );

문제142.  아래의 SQL을 R 로 구현하시오 

SQL> select  ename, sal, deptno
           from  emp
           where  deptno in ( select  deptno
                                     from  emp
                                     where  job='SALESMAN' );

R> a <- emp[ emp$job=='SALESMAN', 'deptno' ]
     emp[ emp$deptno %in% a , c("ename","sal","deptno") ]

문제143.  위의 결과를 판다스로 수행하세요 !

a = emp.loc[ emp['job']=='SALESMAN', 'deptno'].values
emp.loc[ emp['deptno'].isin(a), ['ename', 'sal', 'deptno'] ]

문제144. 관리자인 사원들의 이름을 출력하시오 !

SQL> select  ename 
            from  emp
            where  empno  in  ( select  mgr
                                        from  emp );


R> a <- emp[ emp$empno %in% emp$mgr  , 'ename'] 
     library(data.table)
     data.table(이름=a)

문제145. 위의 결과를 판다스로 구현하시오 !

emp.loc[ emp['empno'].isin(emp['mgr']), 'ename']

문제146.  관리자가 아닌 사원들의 이름을 출력하시오 !

SQL> select  ename
            from  emp
            where  empno  not  in  ( select  mgr
                                               from  emp
                                               where  mgr  is  not  null );

R> a <- emp[ ! emp$empno %in% emp$mgr  , 'ename'] 
     library(data.table)
     data.table(이름=a)

문제147.  위의 결과를 판다스로 수행하세요 ~

emp.loc[ ~ emp['empno'].isin(emp['mgr']), 'ename']

문제148.  지하철에서 가장 많이 발생하는 범죄유형이 무엇인지 출력하시오

crime_loc <- read.csv("crime_loc.csv")

x <- crime_loc[ crime_loc$장소 =='지하철', c('범죄', '건수') ]
x [ x$건수 == max(x$건수),   ]

문제149.  위의 결과를 판다스로 수행하시오 

crime_loc = pd.read_csv("d:\\data\\crime_loc.csv", encoding='cp949')

x =crime_loc.loc[crime_loc['장소']=='지하철', ['범죄','건수']]
x.loc[x['건수']==x['건수'].max(), : ]

문제150. 강력범죄가 가장 많이 발생하는 요일은 언제인가 ?
            (crime_day.csv 를 이용하세요)

crime_day <- read.csv("crime_day.csv")
crime_day

x <- crime_day[ trimws(crime_day$C_C) =='강력범죄',     ]
x[ x$CNT == max(x$CNT) , 'DAY']

설명:  trimws 함수 --->  양쪽 공백을 제거하는 함수 

문제151.  위의 결과를 판다스로 수행하시오 ~

crime_day= pd.read_csv("d:\\data\\crime_day.csv", encoding='cp949')
x = crime_day.loc[crime_day['C_C'].str.strip()=='강력범죄', :]
x.loc[x['CNT'] == x['CNT'].max(), 'DAY']

문제152.  살인기수가 많이 발생하는 요일을 1위부터 3위까지 출력하시오 !
             (R로 수행하세요) 

x <- crime_day[ trimws(crime_day$C_T)=='살인기수',    ]
library(doBy)
x2 <- orderBy( ~ -CNT, x )
head(x2,3)

문제153.  위의 결과를 판다스로 수행하시오 !

crime_day= pd.read_csv("d:\\data\\crime_day.csv", encoding='cp949')
x = crime_day.loc[crime_day['C_T'].str.strip()=='살인기수', :]
x2 = x.sort_values('CNT', ascending=False)
x2.head(3)

■ R 에서 순위를 출력하는 방법

문법 : rank 함수 

예제:  이름, 월급, 월급에 대한 순위를 출력하시오 !

x <-  data.table( 이름=emp$ename, 월급=emp$sal, 
                       순위=rank(-emp$sal, ties.method="min") )

library(doBy)
orderBy( ~순위,  x )

설명: rank 에 마이너스(-) 를 사용하면 월급이 높은것부터 출력됩니다.

ties.method  옵션 :   1. min :  오라클의 rank 와 같다
                           2. first :  오라클의 rank 와 같은데 순위가 같은 데이터가
                                       있으면 인덱스 순서가 먼저 나온 데이터를 
                                       높은 순위로 부여합니다.
                           3. max :  2등이 두명이면 둘다 3등으로 출력합니다.

※ 오라클의 dense_rank 와 같은 함수는 무엇인가 ?

SQL> select  ename, sal, dense_rank()  over ( order  by sal desc) 순위
          from  emp;

R>  library(dplyr)
R>  x <-  data.table( 이름=emp$ename, 월급=emp$sal,
                           순위=dense_rank(-emp$sal) )

R>  library(doBy)
R>  orderBy(~순위, x)

문제154. 월요일에 많이 발생하는 범죄, 건수, 순위를 출력하시오!
            ( crime_day.csv 를 이용하세요) 

crime_day <- read.csv("crime_day.csv")
c_mon <- crime_day[ crime_day$DAY=='MON',   ]
x <- data.table( 범죄=c_mon$C_T, 건수=c_mon$CNT,
                     순위= dense_rank(-c_mon$CNT) )
orderBy(~순위, x)

문제155. 위의 결과를 판다스로 수행하세요

월요일에 많이 발생하는 범죄, 건수, 순위를 출력하시오!

※ 판다스에서 순위 출력하는 방법
emp['sal_rank1'] = emp['sal'].rank(ascending=False)  # 월급이 높은것부터 출력

emp['sal_rank2'] = emp['sal'].rank(method='dense', ascending=False) 
emp['sal_rank3'] = emp['sal'].rank(method='min', ascending=False) 
emp['sal_rank4'] = emp['sal'].rank(method='max', ascending=False) 
emp.sort_values('sal', ascending=False)

답:
c_mon = crime_day.loc[ crime_day['DAY']=='MON', :]
c_mon['cnt_rank'] = c_mon['CNT'].rank(method='dense', ascending=False)
c_mon.sort_values('CNT',ascending=False)

문제156.  여자들이 많이 걸리는 암과 그 환자수와 순위를 출력하시오 !
             ( 암종이 모든암은 제외하는 조건을 주세요 ~)
             ( cancer2.csv 를 이용하세요)

library(dplyr)
c <-  read.csv("cancer2.csv")
head(c)

w_c  <- c[  c$성별=='남자' & trimws(c$암종) != '모든암',   ]
x <- data.table( 암종=w_c$암종, 환자수=w_c$환자수, 순위=dense_rank(-w_c$환자수))

x2 <- orderBy(~순위, x)
x3 <- unique(x2)
x3

문제157. 위의 결과를 판다스로 수행하시오 ~

c = pd.read_csv("d:\\data\\cancer2.csv", encoding='cp949')

w_c = c.loc[ (c['성별']=='여자') & (c['암종'] !='모든암'), :]
w_c['rnk'] = w_c['환자수'].rank(method='dense', ascending=False)

w_c2 = w_c.loc[:,['암종','환자수','rnk']].sort_values('환자수',ascending=False)

w_c2.loc[w_c2.duplicated(), : ]

설명: df.dumplicated() 라고하면 데이터 프레임에서 중복된 행을 제거해줍니다.
       df.Series.unique() 라고 하면 시리즈에서 중복된 행을 제거해줍니다. 


1장. R 기본문법 ( 데이터 검색 + 시각화 )
                     
■ R 로 데이터 시각화 하기

1. 막대그래프
2. 원형그래프
3. 라인그래프
4. 히스토그램 그래프
5. 사분위수 그래프 

■ 막대그래프 그리기 

  여러 데이터 간의 크기를 비교할 때 유용한 그래프 입니다. 

문제158.  emp 테이블의 월급으로 기본적인 막대 그래프를 그리시오 ! 

barplot(emp$sal)

문제159. 위의 그래프의 제목을 Salary Bar Chart 라고 이름을 붙이시오

barplot(emp$sal, main='Salary Bar Chart' )

문제160. 위의 막대 그래프의 x 축에 사원이름을 붙이시오 !

barplot(emp$sal, main='Salary Bar Chart' , names.arg=emp$ename) 

문제161.  막대 그래프의 x 축과 y 축의 이름을 각각 이름, 월급이라고 하시오

barplot(emp$sal, main='Salary Bar Chart' , names.arg=emp$ename ,
           xlab='이름', ylab='월급' ) 

문제162. 막대 그래프의 색깔을 그린 옐로우로 출력하시오 !

barplot(emp$sal, main='Salary Bar Chart' , names.arg=emp$ename ,
           xlab='이름', ylab='월급', col='Green Yellow' ) 

문제163. 식품매장에 대한  "창업건수.csv" 를 R 로 로드하고 그중에서
            치킨집의 창업건수를 막대그래프로 시각화 하시오 !

c_cnt <- read.csv("창업건수.csv")
c_cnt
View(c_cnt)

barplot(c_cnt$치킨집, main='년도별 치킨집 창업건수' ,
           names.arg=c_cnt$년도 , col='Green Yellow', ylim=c(0,1600)  )

문제164.  치킨집의 폐업건수를 막대그래프로 시각화 하시오 !

p_cnt <-  read.csv("폐업건수.csv")

barplot(p_cnt$치킨집, main='년도별 치킨집 폐업건수' ,
           names.arg=p_cnt$년도 , col='Green Yellow', ylim=c(0,1600)  )

문제165. 치킨집의 창업건수와 폐업건수를 같이 막대그래프로 시각화 하시오

c_cnt <- read.csv("창업건수.csv")
p_cnt <-  read.csv("폐업건수.csv")

x <- rbind( c_cnt$치킨집, p_cnt$치킨집)
x

barplot(  x, main='년도별 치킨집 창업/폐업건수' ,
           names.arg=p_cnt$년도 , col= c('Green Yellow', 'Hot pink'),
           ylim=c(0,4000), beside=T  )

문제166. 위의 시각화된 결과에 legend 를 다시오 ~ 

barplot(  x, main='년도별 치킨집 창업/폐업건수' ,
           names.arg=p_cnt$년도 , col= c('Green Yellow', 'Hot pink'),
           ylim=c(0,4000), beside=T  )

legend("topright", c('창업','폐업'), 
          fill=c('Green Yellow', 'Hot pink') )

문제167. 판다스로 치킨집의 창업건수를 막대그래프로 그리시오 !

c = pd.read_csv("d:\\data\\창업건수.csv", encoding='cp949')

c.loc[ :, ['치킨집','년도']].plot(kind='bar',x='년도', y='치킨집', 
      color='greenyellow')

레전드에서 한글이 안깨지게 하려면?
from matplotlib import font_manager, rc
font = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
rc('font', family=font)

문제168. 판다스로 치킨집의 폐업건수를 막대그래프로 그리시오 !

p = pd.read_csv("d:\\data\\폐업건수.csv", encoding='cp949')

p.loc[ :, ['치킨집','년도']].plot(kind='bar',x='년도', y='치킨집',  color='hotpink')

문제169. 창업건수.csv 의 데이터 프레임과 폐업건수.csv 의 데이터 프레임을
            조인해서 년도, 치킨집의 창업건수, 치킨집의 폐업건수를 출력하시오

c = pd.read_csv("d:\\data\\창업건수.csv", encoding='cp949')
p = pd.read_csv("d:\\data\\폐업건수.csv", encoding='cp949')

c_p = pd.merge(c, p, on='년도', how='outer')
c_p.loc[ :, ['년도','치킨집_x', '치킨집_y'] ]

문제170.  위의 데이터를 이용해서 치킨집의 창업건수와 폐업건수를 같이
             막대그래프로 시각화 하시오 !

c_p.plot(kind='bar',x='년도', y=['치킨집_x','치킨집_y'], color=['greenyellow','hotpink'])

45분까지 쉬세요 ~~

import matplotlib.pyplot as plt

fig, ax2 = plt.subplots()  # fig로는 주로 그래프를 시각화, ax2 는 레전드 또는 사이즈와 같은
                                # 부가적인 기능들을 구현할 때 사용

c_p.plot(kind='bar',x='년도', y=['치킨집_x','치킨집_y'], color=['greenyellow','hotpink'],ax=ax2)
ax2.legend(["창업", "폐업"])

■ 원형 그래프 그리기 

  원형 그래프는 주로 비율의 크기를 나타낼때 유용한 그래프 입니다. 

문제171. 사원 테이블의 월급을 원형 그래프로 그리시오 !

pie(emp$sal)

문제172. 위의 그래프를 다시 그리는데 누구의 월급인지가 명시되게하시오

pie(emp$sal, labels=emp$ename, col=rainbow(15) )

문제173.  위의 그래프에 월급에 비율을 붙여서 출력하시오 !

sal_label <-  round( emp$sal / sum(emp$sal) *100, 1)

sal_label2 <- paste( emp$ename, sal_label, '%')

pie(emp$sal, col=rainbow(15), labels= sal_label2)

문제174.  위의 결과를 판다스로 수행하시오 !

emp['sal'].plot(kind='pie', labels=emp['ename'], autopct='%0.0f%%' )

■ 라인 그래프 그리기 

 시간 순서에 따른 데이터의 변화를 볼 때 유용한 그래프 

문제175.  아래의 데이터로 plot(점) 그래프를 그리시오 !

cars <- c( 1, 3, 6, 4, 9 )
cars
plot(cars)

문제176. 위의 그래프에 파란색 선을 그리시오 !

cars <- c( 1, 3, 6, 4, 9 )
cars
plot(cars, type='o', col='blue' )

설명: type='o' : 선을 그어라 ~

문제177. 차와 트럭의 판매된 뎃수를 라인 그래프로 시각화 하시오 !

car <- c(1, 3, 6, 4, 9)
trucks <- c(2, 5, 4, 5, 12)

plot( cars, type='o', col='blue', ylim=c(0,12), axes=FALSE )
lines( trucks, type='o', pch=22, lty=2, col='red')

설명:  pch = 21 : 동그라미 ,  lty=1 : 직선
        pch = 22 : 네모      ,   lty=2 : 점선 

axes=FALSE 는 x 축과 y축 둘다 지웁니다. 

문제178. 가로축을 월,화,수,목,금 으로 변경하시오 !

axis( 1, at=1:5,  lab=c("mon","tue","wed","thu","fri") )
axix(2)  # y 축을 그려라 !

문제179.  아래의 리스트를 판다스 데이터 프레임으로 생성하시오 !

 리스트 --->  numpy  array ----> pandas  series --->  pandas  dataframe

day = ["mon","tue","wed","thu","fri"]
car  =  [ 1, 3, 6, 4, 9 ]
trucks = [ 2, 5, 4, 5, 12 ]
d = np.array(day)  
a = np.array(car)                       45분까지 쉬세요 ~~~
b = np.array(trucks)
d2 = pd.Series(d)
a2=pd.Series(a)
b2=pd.Series(b)
c_t = pd.concat([d2,a2,b2], axis=1)
c_t.columns = ['day','cars', 'trucks']
c_t

또는 

df = pd.DataFrame({'cars': [1,3,6,4,9],
                          'trucks': [2,5,4,5,12],
                          'days':["mon","tue","wed","thu","fri"]})

df

c_t.plot(kind='line', x='day')

문제180. (오늘의 마지막 문제) 서울지하철_5-8호선_이용현황_시간대별
            데이터를 이용해서 5호선과 8호선의 데이터를 라인 그래프로
             시각화 하시오 ( 판다스로 수행 )

5시 신호 보냈습니다. ~~
6시 신호 보냈습니다. ~~

마지막 문제 올려주시고 자유롭게 자습 또는 스터디를 하세요

요번주 토요일에 실기시험이 있으므로 자기 자기의 책이나 귀중품은 사물함에
넣어주세요 ~~~

■ R 을 활용한 머신러닝

1장. R 기본 문법
         - 데이터 검색 ( R  vs  Pandas )
         - 함수 사용 ( R  vs  Pandas )
         - 데이터 조인 ( R   vs  Pandas )
         - 데이터를 위아래로 조인( R )
         - 데이터 서브쿼리  ( R  vs  Pandas )
         - 순위 함수 ( R  vs  Pandas )
         - 데이터 시각화  ( R  vs  Pandas )
                1. 막대그래프
                2. 원형그래프
                3. 라인그래프 
                4. 산점도 그래프
                5. 히스토그램 그래프
                6. 박스 그래프 
                7. 기타 그래프

2장. 이 책을 보기 위해 필요한 R 기능들 소개
3장. knn 머신러닝

■ 판다스로 시각화 하는 문법 

1. 막대그래프:  df.plot(kind='bar')   또는 df.plot.bar()
2. 원형그래프:  df.plot(kind='pie')   또는 df.plot.pie()
3. 라인그래프 : df.plot(kind='line)   또는 df.plot.line()
4. 산점도 그래프 : df.plot(kind='scatter')  또는  df.plot.scatter()
5. 히스토그램 그래프 :  df.plot(kind='his')  또는 df.plot.his()
6. 박스 그래프  : df.plot(kind='box')  또는 df.plot.box()
7. 기타 그래프 

■ 산점도(산포도) 그래프 

 두 변수간의 관계를 파악할 때 사용하는 그래프 
 특히 두 변수간의 관계가 양의 관계인지 음의 관계인지를 파악할 때 유용
 합니다. 

 두 데이터(변량)간의 상관관계 유무를 x, y 평면상에 시각적으로 나타내는
 그래프

 예제:  사원 테이블의 커미션과 월급과의 관계를 산포도 그래프로 그리시오 !

 emp <- read.csv("emp2.csv")

 plot( emp$comm,  emp$sal , pch=21, col='red', bg='red')

그래프 해석:  커미션을 받는 사원들의 월급이 대체적으로 작은것을 확인
                 할 수 있습니다. 이 사원들의 직업이 SALESMAN 이라서
                 커미션으로 보상을 받기 때문이라고 볼 수 있습니다.

문제181. 중고차의 주행거리가 높으면 중고차의 가격이 낮아지는지 
            산포도 그래프로 확인하시오 ! (데이터 : usedcar.csv)

car <- read.csv("usedcars.csv")
car

plot ( car$mileage,  car$price ,  pch=21, col='blue', bg='blue') 

문제182. 위의 그래프를 판다스로 구현하시오 !

import pandas as pd
car = pd.read_csv("d:\\data\\usedcars.csv")

car.plot(style=".", x='mileage', y='price')
또는
car.plot(kind = 'scatter',x='mileage', y='price')

문제183. 중고차 주행거리와 가격간의 상관계수를 출력하시오

cor( car$mileage, car$price )

-0.8061494

설명: 주행거리가 높아질수록 가격이 낮아지는 음의 상관관계를 보이고 
       있습니다. 

문제184. 위의 결과를 판다스로 수행하세요 ~

car.corr(method='pearson')

문제185. 미국 의료비 데이터로 산포도 그래프를 그리는데 bmi 가 
            높을수록(비만일 수록) 의료비가 많이 드는지 확인하시오 ~
            ( insurance.csv)

R>ins2 <- read.csv("insurance.csv")
    head(ins2)
    plot ( ins2$bmi,  ins2$expenses ,  pch=21, col='blue', bg='blue') 
    cor( ins2$bmi,  ins2$expenses)

Pandas> ins2 = pd.read_csv("d:\\data\\insurance.csv")
            ins2.plot(style=".", x='bmi', y='expenses')
            ins2.corr(method='pearson')

■ 히스토그램 그래프 

  히스토그램은 하나의 속성에 대한 데이터의 분포를 시각적으로 
  표현하는 그래프 입니다. 

예제1.  중고차의 가격(price) 데이터를 히스토그램 그래프로 그리기

hist(car$price)

책99페이지:

히스토그램은 값의 개수와 빈도를 나타내는 높이를 갖는 일련의 막대로
구성되며, 값이 소속되어있는 균등한 폭의 빈(bin) 이 전체 값을 분할합니다.

위의 그래프에서는 bin의 갯수가 10개이며 가운데에 있는 가장 높은 막대는
12,000 달러~ 14,000 달러 까지의 범위로 빈도는 50입니다.  
총 데이터의 갯수가 150개이므로 50이면 전체 1/3 의 차량이 
12,000 ~ 14,000 달러 사이에 있음을 확인할 수 있습니다. 

문제186. 위의 그래프를 판다스로 그려보시오 ~

car.price.plot(kind='hist')

문제187. 미국 국민의 의료비 데이터를 판다스로 히스토그램 그래프로
            그리시오 !

ins2.expenses.plot(kind='hist')

※ seaborn 모듈을 이용해서 히스토그램 그래프 그리기 (+ 확률밀도함수)

머신러닝 데이터 분석을 할 때는 종속변수(정답컬럼) 의 데이터의
데이터 분포를 확인할 필요가 있습니다. 

예:  미국민 의료비 데이터의 price

머신러닝 학습이 잘 되려면 종속변수 컬럼의 데이터가 정규성을 보이면
학습이 더 잘됩니다. 

예제1. 타이타닉 데이터의 운임(fare) 이 정규성을 띠는지 히스토그램
         그래프로 확인하시오 !

import  seaborn  as  sns 

tat = sns.load_dataset('titanic')  # 시본에 내장되어 있는 타이타닉 데이터를
tat                                     # 불러옵니다.

tat.fare.plot(kind='hist')        # 판다스로 그렸을때

sns.set_style('darkgrid')   #  스타일 설정하기 
sns.distplot(tat.fare) # 히스토그램 그래프 + 확률밀도 함수 그래프
sns.distplot(tat.fare, hist=False)  # 확률 밀도함수 그래프만 표시
sns.distplot(tat.fare, kde=False)  # 히스토그램 그래프만 표시 

그래프 해석:  타이타닉 운임의 분포는 대체적으로 100달러 미만에
                  집중되어져있다.

※ 확률밀도함수 ?  확률변수의 분포를 나타내는 함수입니다. 


 1. 이산확률변수 :  정수와 같이 명확한 값을 변수 값으로 함
                        예: 2개의 동전을 던져서 나오는  앞면의 수 

 2. 연속확률변수 : 변수값이 정수처럼 명확하지 못함
                       확률변수가 연속량으로 표기되어 가능한 변수값의
                        갯수를 셀수 없는 변수
                       예: 서울지역 초등학교 학생들의 평균키

문제188. 초등학생 10만명의 키를 랜덤으로 생성해서 다음의 문제를 
            해결하세요 ~
           10만명의 초등학생 중 무작위로 한명을 추출했을때 이 어린이의
           키가 145 ~ 150 사이에 있을 확률은 ?  

           ( 평균키가 140, 표준편차 5로 해서 데이터를 10만개 생성 )

데이터 만드는 코드:  height = np.random.randn(100000) * 5 + 140

import numpy as np

height = np.random.randn(100000)*5 + 140
cnt = 0

for i in height:
    if (i >= 145) & (i <= 150):
        cnt += 1

print(cnt/ 100000 * 100 , '%')

문제189. 초등학생 키 10만명의 데이터를 가지고 히스토그램 그래프와
           확률밀도함수 그래프를 같이 그리시오 !

h = np.random.randn(100000)*5 + 140
h2 = pd.DataFrame(h)
h2.columns=['height']

sns.set_style('darkgrid')   #  스타일 설정하기 
sns.distplot(h2.height)

문제190.  미국민의 의료비 데이터로 히스토그램 그래프와 확률밀도함수
             그래프를 같이 그리시오 !

ins2
sns.set_style('darkgrid')   #  스타일 설정하기 
sns.distplot(ins2.expenses)

■ 판다스로 시각화 하는 문법 

1. 막대그래프:  df.plot(kind='bar')   또는 df.plot.bar()
2. 원형그래프:  df.plot(kind='pie')   또는 df.plot.pie()
3. 라인그래프 : df.plot(kind='line)   또는 df.plot.line()
4. 산점도 그래프 : df.plot(kind='scatter')  또는  df.plot.scatter()
5. 히스토그램 그래프 :  df.plot(kind='his')  또는 df.plot.his()
6. 박스 그래프  : df.plot(kind='box')  또는 df.plot.box()

■ 사분위수 그래프 (박스 플롯 그래프 )

  사분위수 그래프는 데이터를 그림을 이용하여 집합의 범위와 중앙값을
  빠르게 확인할 수 있으며 또한 이상치 값이 있는지 빠르게 확인이 가능한
  시각화 기법입니다. 

  평균값, 중앙값, 최빈값만으로는 데이터 분석을 하기가 부족한 경우가
  있다. 특히 평균 데이터는 데이터의 중심이 어디즘인지 알려주지만
  특정 데이터가 평균을 중심으로 어떻게 분포가 되어있는지 알려주지는
  않습니다.  그래서 필요한게 사분위수 그래프 입니다 

▩ 농구 선수 3명이 각각의 게임당 득점한 점수

x1 <- c(7,8,9,9,10,10,11,11,12,13)
x2 <- c(7,9,9,10,10,10,10,11,11,13 )
x3 <- c(1,1,7,7,10,10,10,11,13,30 )

ball2 <- cbind(x1,x2,x3)

ball <- as.data.frame( ball2 )
ball

문제191. 위의 데이터를 판다스의 데이터 프레임으로도 생성하시오 !

ball2= { 'x1' : [7,8,9,9,10,10,11,11,12,13],
           'x2' : [7,9,9,10,10,10,10,11,11,13 ], 
           'x3' : [1,1,7,7,10,10,10,11,13,30 ] }

ball = pd.DataFrame(ball2)
ball

문제192.  위의 농구선수 3명의 득점점수의 평균값, 중앙값, 최빈값을 
             각각 구하시오 ! (R 로 수행하세요)

summary(ball)

  Min.   : 7   Min.   : 7.00   Min.   : 1.00  
 1st Qu.: 9   1st Qu.: 9.25   1st Qu.: 7.00  
 Median :10   Median :10.00   Median :10.00  
 Mean   :10   Mean   :10.00   Mean   :10.00  
 3rd Qu.:11   3rd Qu.:10.75   3rd Qu.:10.75  
 Max.   :13   Max.   :13.00   Max.   :30.00  

names( table(ball$x1)  ) [ table(ball$x1) == max(table(ball$x1)) ]  # 9, 10, 11
names( table(ball$x2)  ) [ table(ball$x2) == max(table(ball$x2)) ]  # 10
names( table(ball$x3)  ) [ table(ball$x3) == max(table(ball$x3)) ]  # 10

문제193. 위의 결과를 판다스로 수행하세요 ! 

ball2= { 'x1' : [7,8,9,9,10,10,11,11,12,13],
           'x2' : [7,9,9,10,10,10,10,11,11,13 ], 
           'x3' : [1,1,7,7,10,10,10,11,13,30 ] }

ball = pd.DataFrame(ball2)
ball.describe().round()    # R의 summary 와 같은 기능입니다. 

ball.mode()  # 최빈값 확인 

데이터 설명:  농구 감독이 위의 농구 선수 3명중에 1명을 뽑아아한다고 했을때 
                  중앙값, 최빈값, 평균값이 모두 10이므로 어느 한명을 
                  위의 통계치로는 선택하기가 어렵습니다. 
                  그래서 다른 데이터 분석 방법이 필요한데 바로 데이터 분포를
                  확인하는 것입니다. 

* 데이터의 분포를  확인하는 방법 3가지

1. 범위
2. 사분위수 범위
3. 히스토그램 그래프 

문제194.  농구선수 3명의 득점점수의 범위값을 확인하시오 !

range(ball$x1)  # 7 13
range(ball$x2)  # 7 13
range(ball$x3)  # 1 30

데이터 설명:  범위는 그 자체로는 데이터의 폭만 설명할 뿐 그 안에서
                  데이터가 분포되는 방식은 설명해 주지 않습니다.
                  특히 이상치에 민감합니다. 

3번째 선수의 경우 어쩌다 한번 잘한 게임(30점) 인 이상치 때문에
범위가 넓어져 버렸다. 그러므로 이상치로 부터 멀어질 필요가 있습니다. 
이상치로 부터 멀어지고 가운데 있는 데이터에만 집중하게 해주는게
바로 사분위수 범위입니다. 

문제195. 판다스에서 농구선수 3명에 대한 득점점수의 범위를 구하시오 !

ball.quantile([0,1])

문제196. 이상치 데이터(30점)을 가지고 있는 3번째 선수의 점수 데이터로
             사분위수 그래프를 그리시오 ~

a <- boxplot(ball$x3, horizontal=T )
a

$stats
     [,1]
[1,]    1    <----  하한값  (min)
[2,]    7    <----  하한 사분위수 (0.25), Q1
[3,]   10   <----  중앙값
[4,]   11   <----  상한 사분위수(0.75) , Q3
[5,]   13   <----  상한값  (max)

$out
[1] 30   <-----  이상치 

문제197.  판다스로 3번 농구선수의 사분위수 그래프를 그리시오 !

ball['x3'].plot(kind='box')

ball['x3'].describe()

Q1 = ball['x3'].quantile(0.25)
median = ball['x3'].median()
Q3 = ball['x3'].quantile(0.75)

iqr  =  Q3  -  Q1  # 사분위수 범위 
val1 = Q3 + iqr * 1.5
print(val1)
ball.loc[ ball['x3'] > val1, 'x3' ]   

문제198.  위와 같이 사분위수 그래프를 그리면서 위의 통계치를
             확인하는 이유가 무엇인가 ?

답: 이상치 때문입니다.  이상치를 제거하고 가운데 50% 의 데이터에만
     집중함으로써 더 좋은 데이터 분석을 할 수 있습니다.

문제199.  코로나 환자 데이터의 접촉자수의 이상치를 출력하시오 
            ( 판다스로 수행하는데  아까 만들었던 변수중에 pat3 로 하세요)

pat3 는 이미 결측치를 제거했고 데이터 정재를 마친 데이터프레임입니다.

Q1 = pat3['contact_number'].quantile(0.25)
median = pat3['contact_number'].median()
Q3 = pat3['contact_number'].quantile(0.75)

iqr  =  Q3  -  Q1  # 사분위수 범위 
val1 = Q3 + iqr * 1.5
print(val1)
ball.loc[ pat3['contact_number'] > val1, 'contact_number' ]   

위의 방법으로는 이상치를 찾을 수 없습니다.

pat3.describe().round() 를 해보면 이상치가 너무 커서 하한값, 중앙값, 상한값
전부 다 0 으로 출력되고 있습니다. 

pat3.contact_number.unique() 를  데이터를 보고 이상치를 따로 제거해야
합니다. 

문제200. 중고차 가격(price) 데이터의 이상치를 확인하시오 ! ( R 로 확인)

car <- read.csv("usedcars.csv")

a <- boxplot( car$price ) 
a

$out
[1] 21992 20995  4899  3800   <---  이상치 4개가 발견되었습니다.

문제201. 중고차 가격 데이터의 가운데 50% 에 해당하는 데이터에 집중하기
            위해 사분위 범위의 Q1 과 Q3 값을 확인하시오 !

quantile(car$price) 

   0%     25%     50%     75%    100% 
 3800.0 10995.0 13591.5 14904.5 21992.0 
             ↑        ↑        ↑
            Q1        Q2       Q3
     하한 사분위수  중앙값  상한 사분위수

IQR(car$price)   # InterQuartile Range( 사분위수 범위) 
3909.5
                        46분까지 쉬세요 ~~

문제202.  그러면 농구감독은 농구 선수 3명 중에 누구를 선택하는게 좋은가?

boxplot(ball)

설명:  그래프를 보면 1, 2 번 선수가 3번 선수보다 상대적으로 사분위수
        범위가 좁은범위를 가지고 있습니다.  
         3번 선수는 1, 2번 선수에 비해 휠씬 높은 점수(30점)을 득점했지만
        다른 경우에는 휠씬 낮은 점수에 대한 기록도 보입니다. 
        그래서 1, 2번 선수가 훨씬 더 점수가 일관성이 있고 대부분의 경우
        3번 선수보다 더 높은 점수를 기록했습니다. 
        그래서 3명중에 한사람을 고른다면 1,2번 선수중에서 고를것입니다. 

■ 지금까지 그린 그래프

1. 막대
2. 원형
3. 라인
4. 산포도
5. 히스토그램
6. 사분위수 
7. 기타 그래프 

■ 지도 그래프 

map 패키지를 설치하고 중국지도만 확대해서 출력하시오 !

install.packages("maps")
install.packages("mapproj")
library(maps)
library(mapproj)

map("world")
map("world", "china")

문제203. 우리나라 지도를 그리시오 !

map("world", "south korea")

■ 워드 클라우드 그리기 

 감정분석 또는 텍스트 마이닝 분석을 위해 시각화하는 그래프

※ R 로 워드 클라우드를 그리려면 R java 를 설치해야합니다. 

1. 아래의 사이트에서 java 64 비트를 다운로드 받습니다.

http://www.java.com/en/download/manual.jsp

 3번째 것인 offline 으로 받아서 설치하세요 !

2. 자바 설치시 대상 폴더 변경으로 설치

3. 아래와 같이 R 에서 환경설정을 합니다

Sys.setenv( JAVA_HOME="C:\\Program Files\\Java\\jre1.8.0_291") 

4. R java 를 설치 합니다.

install.packages("rJava")
library(rJava)

> useSejongDic()   # 세종 사전에 있는 한글을 R 로 로드하는 명령어 
Backup was just finished!
370957 words dictionary was built.

한글을 워드 클라우드로 시각화하는것은 다음주에 알려주겠습니다. 

문제204. (오늘의 마지막 문제) 백화점 데이터의 총구매액의 이상치를 
            출력하시오 !  ( 판다스로 하세요 )

  이상치의 기준 :  Q3 + IQR * 20 보다 큰 데이터를 출력하세요 ~ 

■ R 을 활용한 머신러닝 2장

  3장부터 시작할 머신러닝 데이터 분석을 위해 데이터를 보는 방법과
  머신러닝을 위해 데이터를 볼 때 필요한 함수들을 소개하고 있음

▩ 소목차

 1. R 의 자료구조의 종류 
 2. R 에서 데이터를 로드하는 4가지 방법
 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)
 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포)
 5. 범주형 데이터 살펴보기(산포도 그래프)
 6. CrossTable (이원교차표)
 
▩ 1. R 의 자료구조의 종류 

 1. 벡터(vector)
 2. 행렬(matrix)
 3. 어레이(array)
 4. 데이터 프레임(Data Frame)
 5. 리스트 (list)

▩ 1. 벡터(vector)

  "같은 데이터 타입을 갖는 1차원 배열 구조"

  " c() 을 이용해서 구조를 생성할 수 있다 "

예:  a <- c(1,2,3,4,5)
     a
     b <- c('a', 'b', 'c', 'd', 'e')
     b

문제205. 아래의 숫자들을 출력하는 vector 를 생성하시오 !
  
   1  2  3   4   5   6   7   8   9  10  11  12  13  14   15  16  17  18  19  20 

답:  b <- c(1 : 20 )
     b

문제206. 아래의 숫자들을 출력하는 vector 를 생성하시오 !

  1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4 4 

답:  d <- c( rep(1, 5), rep(2, 6), rep(3, 8), rep(4, 6) )
     d 

문제207. 아래와 같이 출력하시오 !

 jones  jones  jones   jones  king  king  king  scott

답:  x <- c(  rep('jones', 3), rep('king', 3), rep('scott', 1 ) )
     x 
    table(x)

   jones  king scott 
       3     3     1

   str(x)   #  x 변수의 데이터 유형을 확인하는 방법

  chr [1:7] "jones" "jones" "jones" "king" ..    문자형 벡터라는 뜻입니다.

문제208.  직업, 직업별 인원수를 출력하시오 ! 위와 같이 가로로 출력하세요

emp <- read.csv("emp3.csv")
str(emp$job)

chr [1:14] "PRESIDENT" "MANAGER" "MANAGER" ... 문자형 벡터 

table(emp$job)

ANALYST     CLERK   MANAGER PRESIDENT  SALESMAN 
        2         4         3         1         4

str(emp$job)

책 67페이지에 4번째 줄에 위와 같은 기능을 갖는 명령어를 소개하고 있음

typeof(emp$job)

[1] "character"  <-- 문자형

설명:  integer (소수 자리가 없는 숫자)
        double (소수 자리가 있는 숫자)
        logical ( True 또는 False)
        NULL ( 값이 존재하지 않음)  ----> 진짜 값이 비어있고 길이가 0 이다.
        NA  (결측치)    --->  어떤값을 가질 수 있는 저장소이므로 길이가 1이다.

문제209.  아래의 문제208번을 다시 수행하는데 직업이 PRESIDENT 는 
             제외하고 출력하시오 !

table(emp$job)

ANALYST     CLERK   MANAGER PRESIDENT  SALESMAN 
        2         4           3             1              4

답:  x <-  table(emp$job)
     x  
     x[-4]

▩ 팩터(책 69페이지)
 
 팩터(factor) 는 범주 변수나 순위변수를 나타내기 위해 사용하는
 특별한 종류의 벡터(vector) 이다

 머신러닝(기계학습) 학습 데이터를 줄 때 factor 로 제공해줘야합니다. 
 기계가 학습할때 순서 벡터를 기대하기 때문에 팩터형태로 제공해야합니다.

 팩터 = 일반 벡터 + level(순서)

기계를 학습시킬때 데이터와  정답을 같이 주면서 공부 시켜야합니다. 
     
기계에게 암 환자 데이터를 주고 암판정을 하게 하고 싶다면?

우리의 관심범주는 암(1), 다른 대조군은 정상환자(0) 

1과 0 이 들어있는 컬럼(종속변수) 를 팩터 형태로 제공해줘야합니다. 

 1 > 0  <---  순서를 알려줘야합니다. 

 팩터(factor) = 벡터 + 순서 

예제:  f1 <- c("middle", "low", "high")
        f1
        typeof(f1)
        str(f1)

        f2 <- factor(f1)
        f2
        typeof(f2)
        str(f2)

        Factor w/ 3 levels "high","low","middle": 3 2 1

※ 설명:  f2 팩터의 데이터의 순서는 기본값인 알파벳 순서데로 부여되었습니다.

 f3 <-  factor( f1,  order=TRUE, level=c("low", "middle", "high") )
 f3

[1] middle low    high  
Levels: low < middle < high

max(f3)
min(f3) 

문제210.  위에서 만든 f3 팩터의 요소를 출력하는데 아래와 같이 정렬해서
             출력하시오 

결과:  high  middle  low

답:  order( f3, decreasing=T)  # 3 2  1

    f3[  order( f3, decreasing=T) ] 


문제211.  아래의 리스트를 factor 로 구성하시오 ! 순서는 아래와 같이 되게
             하시오 !

 f7 
 large  medium  small  

 large > medium > small  

f6 <- c('large', 'medium', 'small') 

f7 <- factor( f6,  order=TRUE,  level=c("small", "medium", "large") )
f7

* 팩터(factor)

 1. 범주(값의 목록)을 갖는 vector
 2. factor() 함수를 통해서 생성
 3. factor 는 nominal, ordinal 형식 2가지가 존재한다.
 4. nominal 은 level 의 순서의 값이 무의미하며 알파벳 순서로 정의
 5. ordinal 은 level 의 순서의 값을 직접 정의해서 원하는 순서를 정할 수 있다

문제212. 범주형의 대표적인 데이터인 혈액형을 factor 로 구성하시오 ! 
            (책70페이지 아래쪽에 나온데로 구현합니다.)

blood <- factor( c("O", "AB", "A"), levels=c("A","B","AB",O")  )

blood
[1] O  AB A
 
Levels: A B AB O

※ 머신러닝 학습때 사용할 데이터의 환자중에 혈액형 B 형이 없어서 위와 같이
   구성을 했지만 실제로 B 형이라는 혈액형도 존재한다는 것을 알 수 있습니다.

table(blood)

■ 판다스의 데이터 범주화(factor)

예제:

import  pandas  as  pd
age =[ 0, 10, 15, 13, 21, 23, 37, 43, 80, 61, 20, 41, 32, 100]
bins = [ 0,     15,      25,     35,    60,     100]
labels=['어린이', '청년', '장년', '중년', '노년']

cuts = pd.cut( age,  bins, right=False, labels=labels) 

 0  <= 나이 < 15     어린이
 15 <= 나이 < 25   청년 

cuts

['어린이' < '청년' < '장년' < '중년' < '노년'] 

문제213.  emp 테이블의 월급에 대한 파생변수를 아래와 같이
             생성하시오 ( 파생변수명: sal_grade) 

0 < sal <= 1800      low
1801 < sal <= 2500  middle
2501 < sal  <= 9999  high

import  pandas  as  pd
bins = [ 0,     1800,     2500,    9999 ]
labels=['low', 'middle', 'high']

emp = pd.read_csv("d:\\data\emp3.csv")
emp['sal_grade'] = pd.cut( emp['sal'],  bins, right=True, labels=labels) 
emp

▦ 리스트 (list)  책 79 페이지

 - 서로 다은 데이터 구조(vector, data frame, array, list) 의  중첩된 구조

 - list() 함수를 이용해서 데이터 구조를 중첩할 수 있다.

예: a <- list( ename="scott", sal = 3000)
    a 

벡터는 모든 항목이 같은 타입어야 하는 반면, 리스트는 수집될 항목이
다른 타입이어도 됩니다. 이런 유연함 때문에 다양한 타입의 입출력
데이터와 머신러닝 모델의 설정 파라미터 집합을 지정하는데 자주 사용됩니다.


   하이퍼 파라미터 -> 인공신경망: 뉴런의 갯수(숫자), 신경망 층(숫자)
                                             활성화 함수( relu, sigmoid 등은 문자)              

   정확도가 가장 좋은 뉴런의 갯수 와 활성화 함수를 알아내야한다면
   하나는 숫자고 다른 하나는 문자이므로 하이퍼 파라미터를 리스트로 
   제공해줘야합니다. 

▦ 데이터 프레임(data frame)

 - 각기 다른 데이터 타입을 갖는 컬럼으로 이루어진 2차원 테이블 구조
 - 데이터의 행과 열을 갖고 있기 때문에 엑셀의 스프레드시트나 데이터
   베이스의 테이블과 유사한 구조입니다.
 - data.frame() 함수를 이용해서 생성하며 각 컬럼, 행의 이름을 지정할 
   수 있습니다.

예제:   emp <- read.csv("emp3.csv")
         str(emp)

         k1 <- data.frame( x=c(1,2,3,4,5), y=c(2,3,4,5,10) )
         k1
  x  y
1 1  2
2 2  3
3 3  4
4 4  5
5 5 10

문제214. 위의 k1 데이터 프레임에서 5번째행인  5  10 을 출력하시오 !

k1[행, 열]

답:  k1[5,  ] 

    k1.iloc[ 4, : ]  # 판다스 

※ 책 76 페이지 data frame 구성시 옵션중에서 stringsAsFactors=TRUE 라는
   옵션이 있는데 이 옵션을 사용하면 data frame 의 데이터중에 문자형을
   데이터를  자동으로 팩터형태로 변경해줍니다. 

emp <- read.csv("emp3.csv")
str(emp)


emp2 <- read.csv("emp3.csv", stringsAsFactors=TRUE)
str(emp2)

기계를 학습 시킬때는 숫자형을 그대로 숫자로 줘도 되는데 문자형은
factor 로 변환해서 줘야합니다. 

그렇지 않으면 오류가 수행되면서 학습이 되지 않습니다. 

판다스에서 기계학습 시킬때는 위와 같이 factor 로 변환 할 필요는 없습니다.
R 에서만 필요합니다. 

점심시간 문제 검사:   B 반은 라인검사
                            A 반은 카페에 올려주세요 ~

▩ matrix (행렬)

- 같은 데이터 타입을 갖는 2차원 배열구조 

- matrix() 함수를 사용해서 vector 값과 표시할 행과 열을 지정 하면 됩니다.

예:  matrix( c(1:12), nrow=4, ncol=3)

        [,1] [,2] [,3]
[1,]    1    5    9
[2,]    2    6   10
[3,]    3    7   11
[4,]    4    8   12

예: matrix( c(1:12), nrow=4, ncol=3, byrow=T)

        [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9
[4,]   10   11   12

문제215. 아래의 행렬합을 구현하시오 !

  1  2  3        1  4  7
  4  5  6    +  2  5  8   =  ? 
  7  8  9        3  6  9 

a <- matrix( c(1:9), nrow=3, ncol=3, byrow=T)
b <-  matrix( c(1:9), nrow=3, ncol=3)
a + b

문제216. 위의 결과를 파이썬의 numpy 로 구현하시오 !

  1  2  3        1  4  7
  4  5  6    +  2  5  8   =  ? 
  7  8  9        3  6  9 

import numpy as np
a = np.arange(1,10).reshape(3,3)
print (a + a.T)

문제217.  아래의 행렬의 곱을 출력하시오 ! (R)

  1  2  3        1  4  7
  4  5  6    *   2  5  8   =  ? 
  7  8  9        3  6  9 

a <- matrix( c(1:9), nrow=3, ncol=3, byrow=T)
b <-  matrix( c(1:9), nrow=3, ncol=3)
a * b

문제218. 위의 결과를 numpy 로도 해보세요 ~

import numpy as np
a = np.arange(1,10).reshape(3,3)
print (a * a.T)

문제219. 아래의 행렬의 내적값을 출력하시오 !


  1  2  3           1  4  7
  4  5  6    ◎    2  5  8   =  ? 
  7  8  9           3  6  9 

 1x1 + 2x2 + 3x3     1x4 + 2x5 + 3x6     1x7 + 2x8 + 3x9
 4x1 + 5x2 + 6x3     4x4 + 5x5 + 6x6     4x7 + 5x8 + 6x9
         :                          :                          :

답:  a %*% b

문제220. 위의 결과를 numpy 로 하세요 

import numpy as np
a = np.arange(1,10).reshape(3,3)
b = a.T
print ( np.dot(a,b) )

▩ 어레이 (array)

 - 같은 데이터 타입을 갖는 다차원 배열구조
 - matrix 는 2차원 행렬이고 array 는 다차원 행렬
 - array() 함수를 이용해서 다차원 배열을 생성할 수 있다.

예:  array( c(1:12), dim=c(3,4) )  # 2차원 
     array( c(1:12), dim=c(2,2,3) )  # 3차원
     array( c(1:12), dim=c(2,2,2,2) )  # 4차원   

▩ R의 자료구조를 정리

1. 백터(vector)   ----> factor (순서를 갖는 벡터)
2. 행렬(matrix)
3. 어레이(array)
4. 데이터 프레임(data frame)
5. 리스트(list)

▩ 소목차

 1. R 의 자료구조의 종류 
 2. R 에서 데이터를 로드하는 4가지 방법
 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)
 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포)
 5. 범주형 데이터 살펴보기(산포도 그래프)
 6. CrossTable (이원교차표)

▩ 2. R 에서 데이터를 로드하는 4가지 방법

 1.  csv 파일을 로드하는 방법
 2.  xlsx 파일을 로드하는 방법
 3.  txt 파일을 로드하는 방법
 4. database 와 연동해서 R 로 로드하는 방법

▩ 1.  csv 파일을 로드하는 방법

emp <- read.csv("emp3.csv", header=T)

설명:  header=T 는 컬럼명 포함해서 로드합니다. 

▩  2.  xlsx 파일을 로드하는 방법

install.packages("xlsx")
library(xlsx)

dept <- read.xlsx("dept.xls",  1 )
                                     ↑
                                  sheet 번호

▩ 3. txt 파일을 로드하는 방법

niv <- readLines("NIV.txt")
niv

▩ 4. database 와 연동해서 R 로 로드하는 방법

 ※ 먼저 oracle database 에 scott 으로 접속되는지 확인합니다. 

45분까지 쉬세요 ~~

* Oracle 데이터 베이스 연동방법 

1. R에서 패키지를 설치합니다.

install.packages("DBI")
install.packages("RJDBC")

library("DBI")
library("RJDBC")

driver <- JDBC('oracle.jdbc.driver.OracleDriver', 'ojdbc10.jar')

※ 설명:  우리가 사용하는 오라클 버젼 18c 는 ojdbc8.jar 를 이용해야합니다.
           11g 버전과 12c 까지는 ojdbc6.jar 를 이용해도 되었다.
           ojdbc8.jar 를 작업 디렉토리에 둡니다.  
           19c 는 ojdbc10.jar  를 이용합니다.

https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html

oracle_db <- dbConnect (driver, 'jdbc:oracle:thin:@127.0.0.1:1521/xe', 'c##scott', 'tiger')

또는

oracle_db <- dbConnect (driver, 'jdbc:oracle:thin:@127.0.0.1:1521/orcl', 'scott', 'tiger')

emp_query <- 'select * from  emp'

emp_data <- dbGetQuery( oracle_db, emp_query)

emp_data

안되면 위에 메뉴에 세션에 R restart 하세요

문제221.  사원 테이블에서 부서번호가 20번인 사원들의 이름과 월급과 부서번호를
              오라클과 연동해서 출력하시오 !

emp_query <- 'select ename, sal, deptno  from  emp where  deptno = 20'

emp_data <-  dbGetQuery( oracle_db , emp_query)

emp_data


▩ 소목차

 1. R 의 자료구조의 종류 
 2. R 에서 데이터를 로드하는 4가지 방법
 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)
 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포)
 5. 범주형 데이터 살펴보기(산포도 그래프)
 6. CrossTable (이원교차표)

▩  데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)

 평균값  ---> 중앙값 ---> 최빈값 ---> 범위 ---> 사분위수 범위 --> 분산 ---> 표준편차

               ↓                                                        ↓ 
  데이터의 중심이 어딘지 ?                              데이터 분포 파악 ?


예제:  중고차(usedcar.csv)  데이터의 전반적인 관찰

car <- read.csv("usedcars.csv", header=T)
car 
 
컬럼소개:  year : 제조년도
             model : 차 모델
             price :  중고차 가격
             mileage : 마일리지
             color : 차색깔
             transmission : 자동, 수동

str(car)  # 컬럼들과 데이터 유형

summary(car) 

문제222. 위의 str 함수와 summary 함수와 유사한 기능은 판다스에 무엇인가?

car = pd.read_csv("d:\\data\\usedcars.csv")

car.info()        # R 의 str 함수와 유사함
car.describe()  # R 의 summary 함수와 유사함 

문제223.  중고차 가격(price) 데이터의 이상치를 출력하시오 ! (판다스로 하세요)

기준:   IQR = Q3 - Q1  ( 사분위수 범위 )

        Q3 + (IQR *1.5)  값보다 크거나 
        Q1 -  (IQR *1.5) 값보다 작은 값들을 출력하시오 ~

답:  Q3 = car['price'].quantile(0.75)
      Q1 = car['price'].quantile(0.25)
      IQR = Q3 - Q1 

      upper_bound = Q3 + ( IQR *1.5)
      lower_bound =  Q1 - ( IQR * 1.5)
      
      a = car.loc[( car['price'] < lower_bound ) |  (car['price'] > upper_bound ), 'price']
      
문제224. 위의 결과를 R 로 수행하시오 !           45분까지 쉬세요 ~~

a <- boxplot(car$price)
a

Q1 <- boxplot(car$price)$stats[2,1]
Q3 <- boxplot(car$price)$stats[4,1]

IQR <- Q3 - Q1

upper_bound <- Q3 + (IQR*1.5)
lower_bound <-  Q1 - (IQR*1.5) 

car [ (car$price > upper_bound) | (car$price < lower_bound), 'price' ]

문제225.  중고차 (usedcars.csv) 의 모든 컬럼의 이상치가 몇건인지
             아래와 같이 출력되게하는 함수를 생성하시오 !( 파이썬)

car = pd.read_csv("d:\\data\\usedcars.csv")

print( outlier_value( car ) ) 

year            ?
price           4
mileage       ?

■ R 을 활용한 머신러닝 복습 

 1장.  R 기본문법 
 2장.  이 책을 보기 위한 기본 함수들 소개 

▩ 소목차

 1. R 의 자료구조의 종류 5가지 
           1. vector         --->  factor ( vector + 순서)
           2. matrix
           3. array
           4. data frame   ---> stringsAsFactors=TRUE
           5. list 
 2. R 에서 데이터를 로드하는 4가지 방법
          1. csv 파일
          2. text 파일
          3. excel 파일
          4. oracle database 와 연동

 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산, 이상치, 결측치)

     왜 데이터를 관찰해야하는가?

         기계에게 데이터를 주고 학습을 시킬텐데 기계를 잘 학습시키기위해서
         데이터를 관찰해야합니다. ------>  데이터 정제를 말끔하게 해줘야합니다.

         데이터 정제가 필요한 데이터      vs     데이터 정제가 이미 다 된 데이터

      코로나 데이터, 백화점 데이터                  이 책의 예제 데이터

 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포, 산포도 그래프)
 5. 범주형 데이터 살펴보기

 6. CrossTable (이원교차표) ---> 학습한 기계를 평가할때 사용



▩  3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산, 이상치, 결측치)

 1. 데이터를 로드합니다.

   car <- read.csv("d:\\data\\usedcars.csv", stringsAsFactors=TRUE)

 2. 데이터의 구조를 살펴봅니다.

  str(car)                                                                        예측, 분류 
                                                                                         ↑
 ※ 데이터 구조를 살펴보는 이유는 무엇인가 ?    사용할 수 있는 머신러닝 알고리즘이 다르기
                                                                때문입니다. 

                                               1. 전부 숫자 :   knn, 인공신경망, 서포트 벡터 머신

                                               2. 전부 문자 :   나이브베이즈 

                                               3. 문자와 숫자가 혼합되어져 있는 경우 : 의사결정트리,
                                                   랜덤포레스트, 인공신경망

 3. 데이터의 중심성 경향을 파악합니다.

  summary(car)  

 4. 이상치가 많은 컬럼들 어떤것인지 파악합니다. 

  a <- boxpot(car$price)
  a$out

 5. 결측치가 많은 컬럼들이 어떤것인지 확인합니다.

  colSums(is.na(car)) 

▩ 판다스로 위의 5가지를 확인하는 방법 

1. 데이터를 로드 합니다. 

import pandas as pd 

car = pd.read_csv("d:\\data\\usedcars.csv")

2. 데이터의 구조를 살펴봅니다.

car.info()

※ 판다스와 R 과의 차이점은 판다스의 경우는 문자와 숫자 컬럼이 같이 있으면
   문자형 컬럼을 전부 숫자로 변경해줘야합니다. 

3. 데이터의 중심성 경향을 확인합니다.(평균,중앙,최빈,분산,표준편차)

car.describe()

4. 데이터의 이상치를 확인합니다. 

어제 마지막 문제 함수를 사용합니다.

5. 데이터의 결측치를 확인합니다.

car.isnull().sum()
 
▩ 판다스 데이터 프레임의 문자형 컬럼을 숫자로 변경해주는 작업

1. 문자형 컬럼이 무엇이 있는지 확인합니다.

car.info()

2. 문자형 컬럼을 숫자로 변경합니다.

col1='model'
col2='color'
col3='transmission'

a = pd.get_dummies(car[col1])
b = pd.get_dummies(car[col2])
c = pd.get_dummies(car[col3])
car2 = pd.concat([ car, a, b, c ], axis=1 )
car2.head()
car2.drop([col, col2, col3], axis=1, inplace=True)    # 문자형 컬럼을 삭제합니다. 
car2.head() 

▩ 숫자 데이터의 정규화 또는 표준화 작업 수행 

 정규화 작업이란 ?  데이터 프레임열에 들어있는 숫자를 0~1사이의 숫자로
                          변환하는것을 말합니다.

 왜 정규화를 해야하는가?  

  A변수 : 0 ~ 1000 범위
  B변수 :  0 ~ 1 범위 

 이 경우는 A 변수가 B변수에 비해서 상대적으로 큰 숫자값을 갖기 때문에
 A 변수의 영향력이 더 커집니다. 그래서 둘다 min/max 정규화를 써서
 0~1사이의 범위로 만들어줘야 합니다.  

 예: 중고차의 마일리지와 가격을 단위가 서로 다릅니다. 마일리지가 더 숫자가
     높으므로 기계 입장에서는 숫자 더 큰 마일리지가 더 중요한 데이터인가보다
     라고 판단할 수 있다.

정규화 :  0~1 사이의 범위로 변경해주는 작업

                                     x  - min(x)
min/max 정규화 공식 : ----------------------------------
                                   max(x)  - min(x)

표준화:   평균을 0 으로 두고 표준편차를 1로 하는 데이터로 변경하는것 

                               x   -   mean(x)
 표준화 공식  = ------------------------------------------
                                 stdev(x)

예제1.  중고차 데이터의 price 와 mileage 를 정규화 하시오 !

normalize <- function(x) {
                                  return ( (x-min(x))  /  (max(x) - min(x) ) )
                                 }

car_n <- as.data.frame( lapply( car[ 3:4 ], normalize ) )

car2 <- cbind(car[1:2], car_n, car[5:6])

head(car2)

예제2.  중고차 데이터의 price 와 mileage 를 표준화 하시오 !

표준화:   평균을 0 으로 두고 표준편차를 1로 하는 데이터로 변경하는것 

                               x   -   mean(x)
 표준화 공식  = ------------------------------------------
                                    sd(x)

car <- read.csv("d:\\data\\usedcars.csv" ,stringsAsFactors=TRUE)

normalize <- function(x) {
  return ( (x-mean(x))  /  ( sd(x) ) )
}

car_n <- as.data.frame( lapply( car[ 3:4 ], normalize ) )

car2 <- cbind(car[1:2], car_n, car[5:6])

head(car2)

정규화와 표준화를 진행하면  컬럼끼리 단위도 통일되고 이상치에 대한 
영향력이 줄어들게 됩니다.

▩ 판다스로 정규화와 표준화 작업하기

from  sklearn.preprocessing   import   MinMaxScaler   # 정규화 
from  sklearn.preprocessing   import   StandardScaler  # 표준화

1.  데이터를 가지고 표준화를 위한 계산을 수행합니다.
from  sklearn.preprocessing   import   StandardScaler  # 표준화

scaler = StandardScaler()
scaler.fit(car2)

2.  데이터 표준화를 진행합니다.  (위에서 계산된 값으로  데이터를 변경합니다.)
car_n = scaler.transform(car2)

3. car_n 을 데이터 프레임으로 변경합니다.

car3 = pd.DataFrame(car_n)

car3.columns = car2.columns                   

4. 표준화가 되었는지 확인해 봅니다.
car3.describe()

문제226. 중고차 데이터의 price 와 mileage 를 표준화 하시오 ! (판다스로 하세요)


1.  데이터를 가지고 정규화를 위한 계산을 수행합니다.
from  sklearn.preprocessing   import   MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(car2)

2.  데이터 정규화를 진행합니다.  (위에서 계산된 값으로  데이터를 변경합니다.)
car_n = scaler.transform(car2)

3. car_n 을 데이터 프레임으로 변경합니다.

car3 = pd.DataFrame(car_n)

car3.columns = car2.columns                     45분 까지 쉬세요 ~

4. 정규화가 되었는지 확인해 봅니다.
car3.describe()


* 데이터 확인하는 순서

1.  평균값, 중앙값, 최빈값, 분산값, 표준편차값

2. 이상값

3. 결측치 

4. 명목형 데이터를 숫자로 변경(판다스에서만)

5. 정규화 또는 표준화를 수행  ( 사람들이 실험을 해본 결과 머신러닝에서는
                                         표준화 보다는 정규화가 더 좋은 결과 나와서
                                         정규화를 많이 사용합니다.)

▩ 소목차

 1. R 의 자료구조의 종류 
 2. R 에서 데이터를 로드하는 4가지 방법
 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)
 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포)
 5. 범주형 데이터 살펴보기(산포도 그래프)
 6. CrossTable (이원교차표)

▩ 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포)
 
▩ 히스토그램 그래프 

  수치변수의 퍼짐을 그래프로 그리는 방법입니다.
  히스토그램은 값의 갯수 또는 빈도를 나타내는 높이를 갖는 일련의 막대로
  구성되며, 값이 소속되어있는 균등한 빈(bin) 들이 전체값들을 분할 합니다.

예제1. 중고차 데이터의 가격(price) 와 주행거리(mileage) 를 각각 히스토그램
         그래프로 그리시오 !

par( mfrow=c(1,2) )  #  1행 2열로 두개의 그래프를 한 화면에 보여줍니다.
hist( car$price)
hist(car$mileage)

 그림 

그림설명:  중고차 가격이 중앙 양측에 균등하게 나뉘는 경향이 있는 반면,
              차량 주행거리는 오른쪽으로 좀 더 늘어난것으로 보인다.
              이 특징을 왜도(skew) 라고 합니다. 또는 오른쪽으로 꼬리가 길다
            
▩ 왜도와 첨도 (책 100 페이지)

1. 왜도 : 데이터의 좌우로 기울어짐의 정도

 왜도값 > 0  :  오른쪽으로 꼬리가 길다
 왜도값 < 0 :  왼쪽으로 꼬리가 길다

2. 첨도:  위아래 뾰족한 정도

 첨도값이 3 에 가까울 수록 정규분포에 속하고 
 3보다 작은경우 완만한 곡선
 3보다 크면 뾰족한 곡선 

예제1.  주행거리의 왜도값을 출력하시오 !

install.packages("fBasics")
library(fBasics)
skewness(car$mileage)

1.231805  ---------->  왜도값이 0 보다 큰 경우이므로 오른쪽으로 
                             꼬리가 긴 데이터 입니다. 

이런 경우의 데이터는 평균값 > 중앙값  이므로 이상치가 평균값을
한쪽 방향으로 잡아끄는 경우 입니다.

예제2. 주행거리의 데이터로 히스토그램 그래프와 정규분포 그래프를 같이
         그리시오 !

car <- read.csv("usedcars.csv")
m <-  sort(car$mileage)
m
hist(m)
par(new=T)    # 그래프를 겹치게 할 수 있습니다.
plot( m,  dnorm( m, mean=mean(m), sd = sd(m) ),  type='l', 
        axes=FALSE, ann=FALSE)

설명: axes=FALSE 는 x 축을 안나오게 합니다.
       ann=FALSE  는 y 축을 안나오게 합니다. 
       type='l' 은 선입니다. 

그래프 해석:  대부분의 주행거리가 0~60000 사이의 중고차들로 분포되어있습니다.
                 이상치 때문에 오른쪽으로 꼬리가 긴 데이터 분포를 이루고 있습니다.                  

문제227. 중고차 데이터의 가격(price) 을 정규분포 그래프로 시각화하고
            그래프를 해석하시오 !

car <- read.csv("usedcars.csv")
m <-  sort(car$price)
m
hist(m)
par(new=T)    # 그래프를 겹치게 할 수 있습니다.
plot( m,  dnorm( m, mean=mean(m), sd = sd(m) ),  type='l', 
        axes=FALSE, ann=FALSE)

그래프 해석:  1. 제일 높은 빈도는 50개로 차 가격이 12,000~14,000 달러의
                     범위의 차들이 전체 차중에 1/3 입니다.

                 2. 전체 90대의 차량(데이터의 절반)의 가격이 12,000~ 16,000 
                     달러로 구성되어있습니다.

문제228.  보스톤 하우징 데이터의 집값(price) 데이터의 히스토그램 그래프와
             정규분포 그래프를 그리고 그래프를 해석하시오 !
             ( boston.csv )  파이썬 수업 게시판에 1893번 글입니다.

boston <- read.csv("boston.csv")
m <-  sort(boston$price)
m
hist(m)
par(new=T)    # 그래프를 겹치게 할 수 있습니다.
plot( m,  dnorm( m, mean=mean(m), sd = sd(m) ),  type='l', 
        axes=FALSE, ann=FALSE)

그래프 해석:  보스톤 집값의 절반의 데이터가 10만 달러에서 25만달러 
                 사이로 분포 되어있습니다. 

문제229.  보스톤 집값 데이터의 왜도값과 첨도값을 출력하시오 !

library(fBasics)

skewness(boston$price)  #  1.101537

왜도값은 > 0 므로 오른쪽으로 꼬리가 긴 그래프입니다. 

kurtosis(boston$price)  # 1.450984

첨도값이 3보다 작으므로 보스톤 집값은 완만한 곡선을 이루고 있습니다.

▩ 소목차

 1. R 의 자료구조의 종류 
 2. R 에서 데이터를 로드하는 4가지 방법
 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)
 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포, 산포도 그래프)
 5. 범주형 데이터 살펴보기
 6. CrossTable (이원교차표)

▩ 산포도 그래프 


 산포도 그래프는 두개의 수치형 데이터가 서로 연관성을 보이는지 
 확인할 때 유용한 그래프 입니다.

예제. 중고차의 주행거리가 높으면 중고차의 가격이 낮아지는지 산포도 그래프로
       확인하시오 !

plot( car$mileage, car$price, pch=21, col='red', bg='red') 
cor( car$mileage, car$price )

-0.8061494

음의 상관관계가 매우 높게 나타나고 있어서 주행거리가 높을 수록
중고차의 가격이 낮아짐을 보이고 있습니다.

문제230. 우주 왕복선 챌린저호의 폭파 원인중 가장 연관성이 높은 컬럼이
            무엇인지 확인하시오 ! (데이터: challenger.csv ) 
            데이터 게시판 73번

 c <- read.csv("challenger.csv")
 c

distress_ct  : O 형링 파손수

temperature :  온도
field_check_pressure : 압력
flight_num  :  비행기 번호 (비행기 번호가 오래될수록 노후화됨)

cor(c)

o형링 파손수와 가장 연관성이 높은것은 온도입니다. 온도가 낮을 수록
파손수가 높아졌습니다. 

문제231. 삼성전자 주식 등락율과 코스피 지수 등락율과의 상관관계가 있는지
           확인하시오( 산포도 그래프 , 상관계수)
          데이터:  코스피 지수:  K_index.csv ,  삼성전자 주가등락율: S_stock.csv

x 축을 코스피 등락율, y 축을 삼성 주가 등락율로 두세요

k <- read.csv("K_index.csv")
s <- read.csv("S_stock.csv")
x <- merge( k, s , by='date')
x
plot( x$k_rate, x$s_rate, pch=21, col='red', bg='red') 

x[is.na(x)] <-0

cor(x$k_rate,x$s_rate)

 0.5142456

문제232. 삼성전자와 현대자동차 둘중에 코스피 등락율과 더 상관관계가
            높은 주식은 어떤것인지 알아내시오 !

k <- read.csv("K_index.csv")
s <- read.csv("S_stock.csv")
h <- read.csv("H_stock.csv")
x <- merge(merge( k, s , by='date'), h, by='date') 
x
par(mfrow=c(1,2) )
plot( x$k_rate, x$s_rate, pch=21, col='red', bg='red') 
plot( x$k_rate, x$h_rate, pch=21, col='blue', bg='blue') 

x[is.na(x)] <-0

cor(x$k_rate,x$s_rate)  #  0.5142456
cor(x$k_rate,x$h_rate)  #  0.3262717


▩ 소목차

 1. R 의 자료구조의 종류 
 2. R 에서 데이터를 로드하는 4가지 방법
 3. 데이터의 전반적인 관찰(평균, 중앙, 최빈, 표준편차, 분산)
 4. 수치형 데이터 살펴보기(히스토그램 그래프, 정규분포, 산포도 그래프)
 5. CrossTable (이원교차표)

▩ 5. CrossTable (이원교차표)

   머신러닝 모델을 평가할 때 사용하는 도구 입니다. 

   두 명목 변수간의 관계를 관찰 하기 위해 이원 교차표를 사용합니다.
   교차표는 하나의 변수 값이 다른 변수 값에 의해 어떻게 변하는지
   관찰할 수 있다는 점에서 산포도 그래프와 유사합니다. 

예제.  install.packages("gmodels")
        library(gmodels)

        emp <- read.csv("emp3.csv")
        CrossTable( x=emp$deptno, y=emp$job)

문제233. 월급 2500 을 기준으로 직업별 각각 월급이 2500 이상인 사원들과
            월급이 2500 보다 작은 사원들의 분포를 이원교차표로 확인하시오 !

library(data.table)

data.table( emp$sal, emp$sal >= 2500)

emp$sal_tf <-  emp$sal >= 2500
emp

library(gmodels)
CrossTable( emp$job, emp$sal_tf)

※ CrossTable 해석 ?

 1. 행합계
 2. 열합계
 3. 표합계에 대한 셀의 상대적 비율을 나타냄
 4. 카이제곱 통계

" 중고차의 분포가 차종별로 보수적인 색깔이 더 많은지 알 수 있을까? "

문제234.  차 모델의 종류가 무엇이 있는지 차 모델을 중복제거해서 출력하시오!

unique(car$model)

 "SEL" "SE"  "SES"

문제235. 차 색깔의 종류는 무엇이 있는지 중복제거해서 출력하시오 !

unique(car$color)

"Yellow" "Gray"   "Silver" "White"  "Blue"   "Black"  "Green"  "Red"    "Gold" 

보수적 색깔 ?  Black, White, Gray, Silver

보수적이 아닌 색깔?  Yellow, Blue, Green, Red, Gold

문제236.  중고차의 색깔이 보수적이면 TRUE, 아니면 FALSE 인 파생변수를
             책 110 페이지에 나온데로 coservative 라는 이름으로 추가하시오 !

car$conservative <- car$color  %in% c("Black" ,"Gray", "Silver", "White")

table( car$conservative )

FALSE  TRUE 
   51    99 

문제237. 차종 모델별로도 보수적인 색깔이 더 많은지 CrossTable 로 확인하시오

library(gmodels)

CrossTable( car$model,  car$conservative )

※ 분석결과 설명:  모든 차종이 대체적으로 보수적인 색깔이 더 많은 경향을 
   보이고 있습니다. 이것이 과연 우연히 일어난 일인지 아니면 
   연관이 있는것인지 확인을 해볼 필요가 있습니다.
   그럴러면 카이제곱 검정을 하면 됩니다.  CrossTable() 함수를 호출할 때
   chisq=TRUE 를 지정하면 카이제곱 검정의 결과를 얻을 수 있습니다.
   책 112페이지 참조

CrossTable( car$model,  car$conservative, chisq= TRUE )

Pearson's Chi-squared test 
------------------------------------------------------------
Chi^2 =  0.1539564     d.f. =  2     p =  0.92591 
  ↑                           ↑           ↑
 카이제곱값              자유도        P-value 확률 

P-value 확률이 매우 낮다면 두 변수가 연관되어 있다는 강한 증거를 제공합니다.
0.9 이면 1에 가까운 확률이므로 모델과 색깔의 연관성은 우연 때문이고
실제 연관성이 때문이 아닐 가능성이 높습니다. 

■ 카이제곱 검정이란 ?

   통계학에서 검정의 큰 그림 

 카이제곱 분포는 1900 년경에 칼 피어슨에 의해서 개발되어 모집단에
 대한 가설점정이나 교차 분석에 유용하게 사용되는 분포 입니다.

예: 아래의 2x2 크로스 집계표를 보고 두 변수가 연관성이 있는지 살펴보시오

관측빈도   
                   당뇨    정상     전체
비만              10       10       20   
정상체중        15        65      80
전체              25        75      100

귀무가설:   당뇨와 비만은 연관성이 없다.  대립가설:   당뇨와 비만과 연관성이 있다
기대빈도 
                                 당뇨                          정상

비만               100x 20/100 x 25/100     100x 20/100 x 75/100       
정상체중          100x 80/100 x 25/100     100x 80/100 x 75/100    
                                                   ↓

                               당뇨                             정상
기대빈도
비만                           5                                 15
정상체중                    20                                 60

카이제곱값 :    (10-5)^2 /  5  + (10-15)^2 / 15 + (15-20)^2 / 20 + (65-60)^2 /60
   ↓              = 8.33
 관측빈도와 기대빈도의 차이의 제곱을 기대빈도로 나눈 값들의 합


관측빈도   
                   당뇨    정상     전체
비만              10       10       20   
정상체중        15        65      80
전체              25        75      100

카이제곱 값을 구했으면 카이제곱값에 해당하는 확률을 구해야한다.

  2x2  크로스 테이블의 자유도 1이고
  3x2  크로스 테이블의 자유도는 2이다. 
 
 카이제곱값 : 8.33,  자유도: 1 일 때 p-value 확률은 ?

  1 - pchisq( q=8.33, df = 1, lower.tail=TRUE)

 0.003899566
 
 0.004 의 확률이므로 유의수준이 5% (0.05) 이면 귀무가설이 기각되고
 대립가설이 채택되므로 비만과 당뇨는 연관성이 있다. 

정리:
카이제곱값이 높을 수록 ----> 확률을 낮아진다 ----> 두변수의 연관성이 많다.
카이제곱값이 낮을 수록 ----> 확률은 높아진다 ----> 두변수의 연관성이 적다.

10분쉬세요 ~~  50분에 문제 낼께요 ~~

문제238. (오늘의 마지막 문제)  사원 테이블의 직업과 커미션을 받는 유무가
            서로 연관성이 있는지 CrossTable 의 카이제곱 검정값으로 확인하시오

귀무가설 :  커미션을 받는 유무는 직업과 연관성이 없다.
대립가설 :  커미션을 받는 유무는 직업과 연관성이 있다.

유의수준 5% 미만의 확률이 나오면 귀무가설을 기각하고 대립가설을 채택하면
됩니다. 


5시 신호 보냈습니다. ~  마지막 문제 올리시고 자유롭게 자습 또는 스터디하시면
                               됩니다. ~~~

6시 신호 보냈습니다. ~~


import scipy.stats as stats

 

emp = pd.read_csv(r'H:\Rproject\startR\emp3.csv')

emp['comm'] = emp['comm'].fillna(0)

emp['comm'] = emp['comm'].apply(lambda x: x!=0)

result = pd.crosstab(emp['comm'],emp['job'])

 

if stats.chi2_contingency(observed=result)[1] < 0.05:
    print('귀무가설 기각 대립가설 채택')
    print('커미션을 받는 유무는 연관성이 있다.')
else:
    print('귀무가설 채택 대립가설 기각')
    print('직업과 커미션을 받는 유무는 직업과 연관성이 없다')

 


# 귀무가설 기각 대립가설 채택

# 커미션을 받는 유무는 연관성이 있다.



▩ R 과 판다스 수업 목차 

  1장. R 기본 문법
  2장. 이 책을 보기위해 필요한 함수들 소개 
             1. R 의 자료구조
             2. R 로 데이터를 로드하는 방법 4가지
             3. 데이터의 전반적인 관찰(평균,중앙,최빈,표준편차,분산,이상값,결측치)
             4. 수치형 데이터 살펴보기 ( 히스토그램 그래프 + 정규분포 그래프)
                                                 산포도 그래프 + 상관계수
             5. 이원교차표(CrossTable) 확인하는 방법
             6. R 에서의 if 문과 loop 문 사용방법 

▩ R 에서 if 문과 loop 문 사용하는 방법

 1. 함수 생성 방법
 2. if 문 사용방법
 3. loop 문 사용방법

▩ 1. 함수 생성 방법

  함수명 <- function(입력 매개변수명) {    
                                                       실행문 
                                                       return   ( 실행결과를 담은 변수명 )
                                                  }

예제1.  이름을 입력하면 해당 사원의 월급이 출력되는 함수를 생성하시오 !

emp <- read.csv("emp3.csv")

income <- function(name) {  
                                     sal <- emp[  emp$ename==name, "sal" ]
                                     return (sal)
                                   }
income('SCOTT')

예제2.  위의 함수를 수정하는데 이름을 아래와 같이 소문자로 입력해도
          출력되게하시오 !

income('scott')

3000

income <- function(name) {  
                               sal <- emp[  emp$ename== toupper(name), "sal" ]
                               return (sal)
                                   }

income('scott')

예제3. 직업을 입력하면 해당 직업의 토탈월급이 출력되게 하는 함수를 
         생성하시오 !

sum_job <- function(name) {
  sal <-  sum( emp[ emp$job==toupper(name), "sal" ] )
  return (sal)  }

sum_job('salesman')

예제4.  위의 스크립트에 paste 함수를 이용해서 아래와 같이 출력되게하시오 !

sum_job('salesman')

salesman 의 토탈월급을 5600 입니다. 

sum_job <- function(name) {
  sal <-  sum( emp[ emp$job==toupper(name), "sal" ] )
  return ( paste(name, ' 의 토탈월급은 ', sal, ' 입니다') )  }

sum_job('salesman')

예제5. 위의 스크립트를 수정해서 직업을 물어보게 하고 직업을 입력하면
         해당 직업의 토탈월급이 출력되게하시오 !

  힌트:  name <- readline(prompt='직업을 입력하세요 ~')

sum_job()
               직업을 입력하세요 ~  salesman

             salesman 의 토탈월급을 5600 입니다. 


sum_job <- function(name) {
  name <- readline(prompt='직업을 입력하세요 ~')
  sal <-  sum( emp[ emp$job==toupper(name), "sal" ] )
  return ( paste(name, ' 의 토탈월급은 ', sal, ' 입니다') )  }

sum_job()

▩ R 에서 if 문 사용법 

문법:   if  (조건식)  {
                             조건식이 true 일 때 실행하는 실행문
                         }
         else  if ( 조건식 ) {
                                 조건식이 true 일 때 실행하는 실행문
                               }
          else           { 
                             위의 조건식들에 만족하지 않는 경우 실행되는 실행문
                          }

예제1.  if 문을 이용해서 피타고라스의 직각 삼각형 여부를 구현하시오 !

triangle()
                밑변을 입력하세요 ~  3
                높이를 입력하세요 ~  4
                빗변을 입력하세요 ~  5

   직각 삼각형이 맞습니다.  

   아니면
 
   직각 삼각형이 아닙니다. 

triangle  <- function()  {
                     a = as.integer( readline(prompt='밑변의 길이:' )  )
                     b = as.integer( readline(prompt='높이의 길이:' )  )
                     c = as.integer( readline(prompt='빗변의 길이:' )  )

              if ( a^2 + b^2 == c^2 )  { print ('직각 삼각형이 맞습니다') }
              else                            { print ('직각 삼각형이 아닙니다') }
                                                
                                }
                 
triangle()


예제2. 이름을 물어보게하고 이름을 입력하면 해당 사원이 고소득자인지
         중간 소득자인지 저소득자인지가 출력되게하시오 !

 월급 >= 3000  고소득자
 월급 >= 2000  중간 소득자
 나머지           저소득자 

find_name()
               이름을 입력하세요 ~  scott

               고소득자 입니다. 

find_name <- function()  {
                     name = readline(prompt='이름을 입력하세요 ~' )
                     sal <-  emp[ emp$ename==toupper(name), "sal" ]

              if ( sal >= 3000 )     { print ('고소득자 입니다.') }
              else if ( sal >=2000) { print ('중간 소득자입니다.') }
              else                      { print ('저소득자 입니다')  }                                  
                                }
                 
예제3. 어제 마지막 문제의 코드에서 p-value 값만 출력하시오 !
         ( 직업가 커미션 유무가 연관성이 있는가 ? ) 

emp<-read.csv("emp3.csv")

emp$comm[is.na(emp$comm)]<-0
emp$comm_tf  <-  emp$comm > 0

library(gmodels)
a <- CrossTable(x=emp$job, y=emp$comm_tf, chisq = TRUE)
a
p <- a$chisq$p.value
p

예제4.  위의 스크립트를 가지고 함수를 생성해서 아래의 함수를 실행하면
          p-value 값이 출력되게하시오 

example04()

 0.04882159


example04 <- function() { 

  emp<-read.csv("emp3.csv")

  emp$comm_tf <- !is.na(emp$comm)

  library(gmodels)
  a <- CrossTable(x=emp$job, y=emp$comm_tf, chisq = TRUE)
  p <- a$chisq$p.value
  return (p)
                           }

예제5.  위의 함수 스크립트를 수정해서 아래와 같이 출력되게하시오 !

example04()


p-value가 0.007295056 이므로 귀무가설을 기각할만한 충분한 근거가 있다.
따라서 커미션을 받는 유무는 직업과 연관성이 있다고 볼 수 있다.

그렇지 않다면 ?

p-value가 0.679793 이므로 귀무가설을 기각할만한 충분한 근거가 없다.
따라서 커미션을 받는 유무는 직업과 연관성이 없다고 볼 수 있다.

example04 <- function(){
emp <- read.csv('emp3.csv')
emp$comm_tf<- !is.na(emp$comm)
library(gmodels)
r1 <- CrossTable(x=emp$job, y=emp$comm_tf, chisq=TRUE)
a <- r1$chisq$p.value
if (a < 0.05 ) {
        paste('p-value 값은',a,'이므로 대립가설을 채택할만한 충분한 근거가 있다. ,따라서 커미션을 받는 유무는 직업과 연관성이 있다고 볼 수 있다.')} 
else {
        paste('p-value 값은',a,'이므로 귀무가설을 채택할만한 충분한 근거가 있다. ,따라서 커미션을 받는 유무는 직업과 연관성이 없다고 볼 수 있다.')}
}

example04()


■ R에서의 loop문  사용법

문법:  for  ( 루프변수  in  반복할 리스트 )  {      반복할 실행문   } 


예제1:  for  ( i  in  1:10 )  {  print (i)    } 


예제2. 구구단 2단을 출력하시오 !

 2 x 1 = 2
 2 x 2 = 4
    :
 2 x 9 = 18

 for  ( i  in  1:9 )  {  print ( paste('2 x ', i , '=', 2*i )  )  } 

예제3.  아래의 파이썬 코드를 R 코드로 변환하시오 !

import   csv
file = open("d:\\data\\emp5.csv","r")
emp_csv = csv.reader(file)

sal = []

for  i   in  emp_csv:
    sal.append( i[5] )

print(sal)

예제4. 위의 코드를 R 로 구현하시오 !

emp <- read.csv("emp3.csv")   # 컬럼명 있는걸로 수행 
sal2 <- c()   

for  ( i  in  1 : length(emp$sal) )  {  
                                            sal2[i] <- emp$sal[i]
                                          }
print(sal2)

예제2. 위의 결과에서 직업이 SALESMAN 인 사원들의 월급만 sal2 에 담겨지게
         하시오 !

emp <- read.csv("emp3.csv")   # 컬럼명 있는걸로 수행 
sal2 <- c()   
cnt <- 1
for  ( i  in  1 : length(emp$sal) )  {  
                  if ( emp$job[i]=='SALESMAN') {
                                            sal2[cnt] <- emp$sal[i]
                                            cnt <- cnt + 1
                                                          }
                                          }
print(sal2)

예제3. 위의 코드를 이용해서 부서번호가 20번인 사원들의 토탈월급을 
        출력하시오 !

emp <- read.csv("emp3.csv")   # 컬럼명 있는걸로 수행 
sal2 <- c()   
cnt <- 1
for  ( i  in  1 : length(emp$empno) )  {  
                  if ( emp$deptno[i]== 20) {
                                            sal2[cnt] <- emp$sal[i]
                                            cnt <- cnt + 1
                                                          }
                                          }
print(sum(sal2))

예제4.  주사위를 한번 던져서 주사위의 눈을 출력하시오 !

sample( x= 1:6, size=1)

예제5. 주사위를 10000 번 던져서 주사위의 눈이 3이 나올 확률을 출력하시오 !
         ( 카페에 올려주세요)

cnt <- 0
for (i in 1:10000){
    if (sample(x=1:6, size = 1) == 3){
        cnt <- cnt+1
}}

paste(cnt/100,'%')

예제6.  주사위 2개를 동시에 던져서 주사위 2개의 눈의 합이 10이 될 확률을
          출력하시오 !  ( 주사위 2개를 10000번 던지세요) 

cnt <- 0

for  ( i  in 1 : 10000 ) {
           a <- sample( 1:6, size=1)
           b <- sample( 1:6, size=1)
           if ( a + b == 10) { cnt <- cnt + 1 }
                             }
cnt /10000

예제7. 아래와 같이 정상품 3개와 불량품 2개를 만들고 아래의 box 벡터에서
         2개를 랜덤으로 추출하시오 ( 복원 추출 하세요)

box <- c("정상품","불량품","정상품","정상품","불량품")

sample(box, size=2, replace=T)

예제8. 아래의 box 에서 제품을 하나 추출했을때 그 제품이 불량품일 확률을
         어떻게 되는가 ?

box <- c("정상품","불량품","정상품","정상품","불량품")

cnt <- 0
for  ( i  in 1 : 10000 ) {
           box <- c("정상품","불량품","정상품","정상품","불량품")
           if ( sample(box, size=2, replace=T) == '불량품' ) { cnt <- cnt + 1 }
                             }
cnt /10000


■ 3장.  knn 알고리즘 

▩ knn 알고리즘이란 무엇인가?

  그림 ppt

 k nearest   neighbor 의 약자로 k 개의 최근접 이웃이라는 뜻입니다.
 머신러닝 지도학습의 분류에 해당하는 알고리즘입니다.

 새로 들어온 데이터가  기존 데이터의 그룹에 어느 그룹에 속하는지
 찾을 때 거리가 가까운 데이터의 그룹을 자기 그룹으로 선택하는
 아주 간단한 알고리즘입니다.

▩ knn 알고리즘의 장단점 

 - 장점 :  단순하고 효율적이다.  모델을 훈련 시키지 않습니다.

 - 단점 :  적절한  k 값을 모델 개발자가 직접 알아내야 합니다.

▩ knn 의 원리 

  새로 들어온 데이터가 기존 데이터 중에서 (악성종양, 양성종양)
  어느 데이터에 더 인접해 있는지 거리를 계산해서 가장 가까운
  거리에 있는 데이터를 자기의 이웃으로 선택하는것 

 거리를 계산할 때 사용하는 수학식 ?  유클리드 거리 계산식 (책 121페이지 참고)

▩ 유클리드 거리 공식을 R 로 구현하기

예제1. 두점의 좌표를 지정한다.

a = c(2,4)
b = c(5,6)

예제2. 두점 사이의 거리를 구한다.


     sqrt( sum(  (a-b)^2 ) )

예제3. 3차원에서 두점 사이의 거리 구한다. 

a = c( 0, 3, 2)
b = c( 2, 0, 0)

sqrt( sum(  (a-b)^2 ) )  # 4.123106

예제4. a 지점과 b 지점 사이의 거리를 구하는 함수를 생성한다. 

distance(a,b)  # 4.123106

답:  distance <-  function(a,b) {
                                         return  ( sqrt( sum( (a-b)^2 )  )  )
                                       }

예제5. 책120페이지의 그림을 코드로 구현하는데 위에서 만든 distance 함수를
        이용해서 아래의 여러개의 지점과 c(4,4) 지점과의 거리를 구하시오 !

 c(1, 5) 와  c(4,4) 와의 거리 ?
 c(2, 6) 와  c(4,4) 와의 거리 ?
 c(4, 5) 와  c(4,4) 와의 거리 ?
 c(5, 2) 와  c(4,4) 와의 거리 ?
 c(6, 3) 와  c(4,4) 와의 거리 ?
 c(1, 7) 와  c(4,4) 와의 거리 ?

 x = c(1,2,4,5,6,1)
 y = c(5,6,5,2,3,7)     
        
 distance <-  function(a,b) {
                                         return  ( sqrt( sum( (a-b)^2 )  )  )
                                       }
 
 temp <- c()

 for ( i  in  1:6 )  {
                         temp <- append( temp, distance( c(x[i], y[i] ), c(4,4) )  )
                      }

  temp

예제6. 위의 temp 의 요소중에 가장 작은값만 출력하시오 !

min(temp)

문제239.  토마토와 fruits  데이터프레임의 음식과의 거리를 구하시오 ~

fruits <- data.frame( 
                      '재료'=c('사과','베이컨','바나나','당근','셀러리','치즈'),
                      '단맛'=c(10, 2, 10, 7, 4, 1),
                      '아삭한맛'= c(9, 4, 1, 10, 10, 1),
                      '음식종류' = c('과일', '단백질', '과일', '채소', '채소', '단백질') 
                         )


fruits
토마토 = c(6,4)

 distance <-  function(a,b) {
                                         return  ( sqrt( sum( (a-b)^2 )  )  )
                                       }
 
 temp <- c()

 x <- fruits$단맛
 y <- fruits$아삭맛한

 for ( i  in  1:6 )  {
              temp <- append( temp, distance( c(x[i], y[i] ), c(6,4) )  )
                      }

 min(temp)

문제240.  위에서 출력된 거리들을 fruits 의 파생변수로 생성하시오
             ( 파생변수 이름은 : dist )

fruits$dist <- temp
fruits

문제241.  위의 최소거리의 음식종류가 무엇인지 fruits 출력하시오 !

fruits <- data.frame( 
                      '재료'=c('사과','베이컨','바나나','당근','셀러리','치즈'),
                      '단맛'=c(10, 2, 10, 7, 4, 1),
                      '아삭한맛'= c(9, 4, 1, 10, 10, 1),
                      '음식종류' = c('과일', '단백질', '과일', '채소', '채소', '단백질') 
                         )

문제242. 위에서 만든 fruits 의 파생변수인 dist 를 이용해서 순위 파생변수를
            추가하시오 ! ( 거리가 제일 짧은것이 순위가 1위가 되겠금 하시오)

library( dplyr )
fruits$rnk <- dense_rank( fruits$dist )
fruits

문제243. 위의 결과에서 순위가 3위까지인 것의 음식종류를 출력하시오 !

fruits <- data.frame( 
  '재료'=c('사과','베이컨','바나나','당근','셀러리','치즈'),
  '단맛'=c(10, 2, 10, 7, 4, 1),
  '아삭한맛'= c(9, 4, 1, 10, 10, 1),
  '음식종류' = c('과일', '단백질', '과일', '채소', '채소', '단백질') 
)

distance <-  function(a,b) {
  return  ( sqrt( sum( (a-b)^2 )  )  )
}

temp <- c()

x <- fruits$단맛
y <- fruits$아삭한맛

for ( i  in  1:6 )  {
  temp <- append( temp, distance( c(x[i], y[i] ), c(6,4) )  )
}

temp

fruits$dist <- temp
fruits

a <- fruits[ fruits$dist==min(fruits$dist),'음식종류'][1]
a

library( dplyr )
fruits$rnk <- dense_rank( fruits$dist )
fruits

fruits[ fruits$rnk <= 3,  '음식종류' ]

 "단백질" "과일"   "단백질"

문제244. 위의 결과값중에 최빈값을 출력하시오 !

class1 <-  fruits[ fruits$rnk <= 3,  '음식종류' ]

table(class1) [ table(class1) == max(table(class1) ) ] 

문제245. 위의 코드들을 조합해서 아래의 my_knn 이라는 함수로 생성하시오

my_knn(3)

단백질 

knn <- function(num) {  

fruits <- data.frame( 
  '재료'=c('사과','베이컨','바나나','당근','셀러리','치즈'),
  '단맛'=c(10, 2, 10, 7, 4, 1),
  '아삭한맛'= c(9, 4, 1, 10, 10, 1),
  '음식종류' = c('과일', '단백질', '과일', '채소', '채소', '단백질') 
)

distance <-  function(a,b) {
  return  ( sqrt( sum( (a-b)^2 )  )  )
}

temp <- c()

x <- fruits$단맛
y <- fruits$아삭한맛

for ( i  in  1:6 )  {
  temp <- append( temp, distance( c(x[i], y[i] ), c(6,4) )  )
}

fruits$dist <- temp
a <- fruits[ fruits$dist==min(fruits$dist),'음식종류'][1]
library( dplyr )
fruits$rnk <- dense_rank( fruits$dist )

fruits[ fruits$rnk <= num,  '음식종류' ]

b <- names( table(class1) [ table(class1) == max(table(class1) ) ] )
return  (b)

 }

▩ 파이썬으로 knn 구현하기 

예제1.  아래의 데이터 프레임을 파이썬 판다스 데이터 프레임으로 생성하시오

fruits <- data.frame( 
  '재료'=c('사과','베이컨','바나나','당근','셀러리','치즈'),
  '단맛'=c(10, 2, 10, 7, 4, 1),
  '아삭한맛'= c(9, 4, 1, 10, 10, 1),
  '음식종류' = c('과일', '단백질', '과일', '채소', '채소', '단백질') 
)

fruits = pd.DataFrame( { '재료' : ['사과','베이컨','바나나','당근','셀러리','치즈'],
                               '단맛' : [10, 2, 10, 7, 4, 1],
                               '아삭한맛' : [ 9, 4, 1, 10, 10, 1],
                               '음식종류' : ['과일', '단백질', '과일', '채소', '채소', '단백질']
                               } )

fruits

예제2. a 지점과 b 지점 사이의 거리를 구하는 distance 함수를 파이썬으로 
         생성하시오 !

a = [ 0, 3, 2 ]
b = [ 2, 0, 0 ]

distance(a,b) 

#답:  
import  math 

def  distance(a,b):
    sum = 0
    for  i  in  range(len(a)):
        sum = sum + (a[i]-b[i])**2

    return  math.sqrt(sum)

distance(a,b) 

예제3.  아래의 fruits 의 단맛을 x 축으로 두고 아삭한 맛을 y 축으로 두어서 
          토마토 (6,4) 와의 거리를 출력하시오 ! 

fruits = pd.DataFrame( { '재료' : ['사과','베이컨','바나나','당근','셀러리','치즈'],
                               '단맛' : [10, 2, 10, 7, 4, 1],
                               '아삭한맛' : [ 9, 4, 1, 10, 10, 1],
                               '음식종류' : ['과일', '단백질', '과일', '채소', '채소', '단백질']
                               } )

fruits

효선이 코드:

import math
import numpy as np

def distance(a,b) :
    sum=0
    for i in range(len(a)):
        sum += (a[i]-b[i])**2             
    return math.sqrt(sum)

tomato = [6,4]
dist = []
for j in range(6):
    x = fruits.loc[j, '단맛']  
    y = fruits.loc[j, '아삭한맛']
    dist.append(distance( (x,y), tomato))
    
print(np.round(dist, 2))


예제4. 위에서 만든 거리를 fruits 데이터 프레임에 dist 라는 이름으로 
         파생변수를 생성하시오 !

fruits['dist'] =dist

fruits

예제5.  위에 dist 파생변수를 이용해서 순위를 출력하는 파생변수 rnk 를
           fruits 에 생성하시오 ( 순위는 dist 가 제일 작은것을 1위로 하세요)

fruits['rnk'] =list(map(int,fruits['dist'].rank()))

fruits

45분까지 쉬세요 

문제246. (오늘의 마지막 문제)  위의 예제들을 조합해서 my_knn 함수를 
            아래와 같이 생성하시오 !

my_knn(3)

단백질

■ knn 머신러닝 알고리즘을 이용하여 유방암 데이터 분류 데이터 분석(p128)

# 1단계: 데이터 수집       --->  데이터 불러오기, 데이터에 대한 출처와 설명

# 2단계: 데이터 탐색       --->  결측치, 이상치, 명목형 데이터 여부 확인,
                                         데이터 분포확인(히스토그램+정규분포)
                                         데이터 정규화 또는 표준화 작업 수행
# 3단계: 데이터 모델 훈련과 분류  : 모델설정, 모델훈련, 모델예측(분류)

# 4단계: 모델 성능 평가 : 이원교차표를 통해서 정확도 확인
                 
  정확도외에 다른 성능 척도: 민감도,특이도,정밀도, 재현율, ROC 곡선, AUC 수치)

# 5단계: 모델 성능 개선 : 모델의 예측능력, 분류능력을 높이는 작업


▩ 1단계: 데이터 수집

  위스콘신 유방암 진단 데이터셋이며 이 데이터는 569개의 암 조직검사
  예시가 들어있으며, 각 예시는 32개의 특징을 갖는다. 
  그 특징은 디지털 이미지에 존재하는 세포핵의 특성을 나타낸다. 
 
    독립변수                                               종속변수(정답)

  반지름           조밀성                              diagnosis  :  양성(B), 악성(M)
  질감              오목함
  둘레              오목점
  넓이              대칭성
  매끄러움         프랙탈 차원

# 데이터 게시판 54번 

wbcd <- read.csv("wisc_bc_data.csv")

nrow(wbcd)   # 행의 갯수   569개
ncol(wbcd)    # 열의 갯수   32개
str(wbcd)

전부 숫자 데이터이고 정답 데이터만 문자형입니다. 즉 knn 으로 분류하기 
딱 좋은 데이터 입니다. 

▩ 2단계: 데이터 탐색   

 1.  정답에 해당하는 라벨 컬럼의 데이터 분포를 확인 합니다.

  table(wbcd$diagnosis)
  B   M 
357 212 

 악성 데이터와 양성 데이터가 50대 50으로 분포되어있는것이 가장 이상적이나
  보통은 그렇게 분포되어있지 않기 때문에 특별한 방법을 통해서 악성 데이터를
  늘려줄 필요가 있습니다. ( 모델의 예측능력이 떨어질때 고려하면 됩니다.)

2. 이상치를 확인합니다.

library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(wbcd)  # wbcd 의 컬럼명 확인 

a <- grubbs.flag(wbcd$radius_mean)  # wbcd 의 radius_mean 컬럼에 이상치가 있는지 확인 
                                                 #  하기위해 데이터 프레임을 생성 
a[a$Outlier==TRUE,     ]               # a 데이터 프레임의 Outlier 컬럼이 TRUE 인 데이터를 
                                                 찾는다. 

for  ( i  in  3 : length(colnames(wbcd) ) ) {
   
a <- grubbs.flag(wbcd[  ,  colnames(wbcd)[i] ]  )
b <- a[a$Outlier==TRUE, "Outlier" ]
print ( paste( colnames(wbcd)[i], ' --->', length(b) )  )

                                                        }

[1] "radius_mean  ---> 3"
[1] "texture_mean  ---> 1"
[1] "perimeter_mean  ---> 3"
[1] "area_mean  ---> 6"
[1] "smoothness_mean  ---> 1"
[1] "compactness_mean  ---> 2"
[1] "concavity_mean  ---> 4"
[1] "points_mean  ---> 1"
[1] "symmetry_mean  ---> 2"
[1] "dimension_mean  ---> 6"
[1] "radius_se  ---> 7"
[1] "texture_se  ---> 5"
[1] "perimeter_se  ---> 14"
[1] "area_se  ---> 14"
[1] "smoothness_se  ---> 7"
[1] "compactness_se  ---> 12"
[1] "concavity_se  ---> 10"
[1] "points_se  ---> 6"
[1] "symmetry_se  ---> 13"
[1] "dimension_se  ---> 17"
[1] "radius_worst  ---> 1"
[1] "texture_worst  ---> 1"
[1] "perimeter_worst  ---> 1"
[1] "area_worst  ---> 8"
[1] "smoothness_worst  ---> 2"
[1] "compactness_worst  ---> 6"
[1] "concavity_worst  ---> 3"
[1] "points_worst  ---> 0"
[1] "symmetry_worst  ---> 5"
[1] "dimension_worst  ---> 3"

다른 컬럼에 상대적으로 이상치가 많은 컬럼은 dimension_se 컬럼입니다.

3. 결측치를 확인합니다. 

colSums(is.na(wbcd))

모든 컬럼들이 전부 결측치가 없는 데이터 입니다. 

4.  히스토그램 그래프 + 정규분포 그래프를 통해서 데이터들이 정규성을 
    보이는지 확인합니다.

hist(wbcd$dimension_worst)

install.packages("psych")
library(psych)
pairs.panels(wbcd[c("dimension_se", "symmetry_se","perimeter_se")] )

이상치가 많은 컬럼에 대해서 히스토그램 그래프와 정규분포 그래프를 같이
확인을 해보니 오른쪽으로 꼬리가 긴 데이터의 분포를 이루고 있습니다. 

▩ 3단계 : 데이터로 모델 훈련 

1. 데이터의 구조를 확인하여 라벨(정답) 컬럼이 factor 인지 확인합니다. 

str(wbcd)  

wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

str(wbcd)

2.  데이터를 양성과 악성이 잘 섞일 수 있도록 데이터를 섞어 줍니다. 

sample(10)
 
wbcd_shuffle <- wbcd[ sample(nrow(wbcd)) ,  ]

wbcd_shuffle

3.   훈련 할때 필요한 컬럼만 선택합니다.   

wbcd_shuffle2 <- wbcd_shuffle[   , c(-1,-2) ] # 환자번호와 정답을 제외합니다. 

wbcd_shuffle2

4. 컬럼들의 단위가 다 다르므로 데이터를 정규화 합니다. 

normalize <- function(x) {  return ( (x-min(x))  / ( max(x) - min(x) )  )   }
                                
wbcd_n <- as.data.frame( lapply( wbcd_shuffle2, normalize )  ) 

summary(wbcd_n)

5. 전체 569개의 행의 데이터를 훈련(공부) 데이터와 테스트(시험) 데이터로
   나눠줘야합니다. ( 훈련 데이터 9, 테스트 데이터 1 로 나눈다)

nrow(wbcd_n) #569

n_90 <-  round(0.9*nrow(wbcd_n) )
n_90  # 512

wbcd_train <- wbcd_n[ 1:512,   ]            # 훈련 데이터 구성   
wbcd_test  <- wbcd_n[ 513 : 569,  ]        # 테스트 데이터 구성

nrow(wbcd_train)  # 512
nrow(wbcd_test)   # 57

6. 정답도 훈련과 테스트로 나눕니다. 

head(wbcd_shuffle[  ,  2])
 
wbcd_train_label <- wbcd_shuffle[ 1: 512 ,  2]
wbcd_test_label <- wbcd_shuffle[ 513 : 569 ,  2]

length(wbcd_train_label )  # 512
length(wbcd_test_label)    # 57

7. 512개의 훈련 데이터와 훈련 데이터의 정답으로 거리계산한 데이터로
   테스트 데이터 57개를 분류 합니다. 

install.packages("class")
library(class)

result1 <- knn(train=wbcd_train,  test=wbcd_test, cl=wbcd_train_label, k=1)
result1

 [1] B B M B M B M B B B M B B B B M B B M M B M B B B M B B B B M
[32] M B B M M B B M M M B M B M B B B B B B M M B B M B
Levels: B M

wbcd_test_label
[1] B B M B M B M B B B M B B B B M M B M M B M B B B B B B B B M
[32] M B B M M B B M M M B M B M B B B B B B M M B B M B

▩ 4단계. 모델 성능 평가

result1 == wbcd_test_label

sum(result1 == wbcd_test_label)

sum(result1 == wbcd_test_label)/57 *100

[1] 0.9649123

※ 왜 정확도가 자리 마다 다 다르게 나오는 이유는 ?

  데이터를 shuffle 할때 sample 함수를 사용했는데  이때 자리마다 섞는 순서가
  달랐습니다.
  자리마다 똑같은 순서로 섞어지게 했을 필요가 있습니다.

  sample(10)

 set.seed(1)  # seed 값은 숫자 1번부터 ~~~~~~~~~~~~~ 아주 큰수를 
                # 입력하면 되는데 똑같은 seed 값을 입력하면 어느자리에서든
                # 같은 패턴으로 숫자가 섞이게 됩니다.
 sample(10)

 9  4  7  1  2  5  3 10  6  8

문제247.  위의 유방암을 판정하는 분류 모델을 다시 똑같이 만드는데
             이번에는 seed 값을 1로 주고 만들고 정확도를 확인하시오 !


#▩ 1단계: 데이터 수집

wbcd <- read.csv("wisc_bc_data.csv")

nrow(wbcd)   # 행의 갯수   569개
ncol(wbcd)    # 열의 갯수   32개
str(wbcd)

#▩ 2단계: 데이터 탐색   

# 1.  정답에 해당하는 라벨 컬럼의 데이터 분포를 확인 합니다.

table(wbcd$diagnosis)

#2. 이상치를 확인합니다.

library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(wbcd)  # wbcd 의 컬럼명 확인 

a <- grubbs.flag(wbcd$radius_mean)  # wbcd 의 radius_mean 컬럼에 이상치가 있는지 확인 
#  하기위해 데이터 프레임을 생성 
a[a$Outlier==TRUE,     ]               # a 데이터 프레임의 Outlier 컬럼이 TRUE 인 데이터를 


for  ( i  in  3 : length(colnames(wbcd) ) ) {
  
  a <- grubbs.flag(wbcd[  ,  colnames(wbcd)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(wbcd)[i], ' --->', length(b) )  )
  
}

#3. 결측치를 확인합니다. 

colSums(is.na(wbcd))

#4. 히스토그램 그래프 + 정규분포 그래프를 통해서 데이터들이 정규성을  보이는지 확인합니다.

#install.packages("psych")
library(psych)
pairs.panels(wbcd[c("dimension_se", "symmetry_se","perimeter_se")] )

# ▩ 3단계 : 데이터로 모델 훈련 

# 1. 데이터의 구조를 확인하여 라벨(정답) 컬럼이 factor 인지 확인합니다. 

str(wbcd)  

wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

str(wbcd)

#2.  데이터를 양성과 악성이 잘 섞일 수 있도록 데이터를 섞어 줍니다. 

set.seed(1)

wbcd_shuffle <- wbcd[ sample(nrow(wbcd)) ,  ]

wbcd_shuffle

#3.   훈련 할때 필요한 컬럼만 선택합니다.   

wbcd_shuffle2 <- wbcd_shuffle[   , c(-1,-2) ] # 환자번호와 정답을 제외합니다. 

wbcd_shuffle2

#4. 컬럼들의 단위가 다 다르므로 데이터를 정규화 합니다. 

normalize <- function(x) {  return ( (x-min(x))  / ( max(x) - min(x) )  )   }

wbcd_n <- as.data.frame( lapply( wbcd_shuffle2, normalize )  ) 

summary(wbcd_n)

#5. 전체 569개의 행의 데이터를 훈련(공부) 데이터와 테스트(시험) 데이터로
#   나눠줘야합니다. ( 훈련 데이터 9, 테스트 데이터 1 로 나눈다)

nrow(wbcd_n) #569

n_90 <-  round(0.9*nrow(wbcd_n) )
n_90  # 512

wbcd_train <- wbcd_n[ 1:512,   ]            # 훈련 데이터 구성   
wbcd_test  <- wbcd_n[ 513 : 569,  ]        # 테스트 데이터 구성

nrow(wbcd_train)  # 512
nrow(wbcd_test)   # 57

# 6. 정답도 훈련과 테스트로 나눕니다. 

head(wbcd_shuffle[  ,  2])

wbcd_train_label <- wbcd_shuffle[ 1: 512 ,  2]
wbcd_test_label <- wbcd_shuffle[ 513 : 569 ,  2]

length(wbcd_train_label )  # 512
length(wbcd_test_label)    # 57

#7. 512개의 훈련 데이터와 훈련 데이터의 정답으로 거리계산한 데이터로
#   테스트 데이터 57개를 분류 합니다. 

#install.packages("class")
library(class)

result1 <- knn(train=wbcd_train,  test=wbcd_test, cl=wbcd_train_label, k=1)
result1

#▩ 4단계. 모델 성능 평가

result1 == wbcd_test_label

sum(result1 == wbcd_test_label)

sum(result1 == wbcd_test_label)/57 *100

96.49123

▩ 5단계. 모델 개선 

k 값이 1일때 정확도 96.49123 가 나왔습니다.

환자 100명중에 96명은 잘 판정하고 4명은 잘못판정하는 분류 모델 입니다. 

문제248.  정확도를 더 올리기 위한 k 값을 알아내시오 ~

k 값 3부터 정확도 100 인 모델이 되었습니다. 

문제248.  와인 데이터(wine.csv) 데이터를 분류하는 knn 머신러닝 모델을 
             생성하시오 ! (데이터 게시판에 와인 데이터 있습니다.)

정답 컬럼은 Type 입니다. 

# 1. 데이터 로드
# 2. 데이터 탐색과 준비
# 3. 데이터 모델 훈련
# 4. 모델 성능 평가
# 5. 모델 성능 개선 

#▩ 1단계: 데이터 수집

wine <- read.csv("wine.csv")

nrow(wine)   # 행의 갯수   569개
ncol(wine)    # 열의 갯수   32개
str(wine)

#▩ 2단계: 데이터 탐색   

# 1.  정답에 해당하는 라벨 컬럼의 데이터 분포를 확인 합니다.

table(wine$Type)

#2. 이상치를 확인합니다.

library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(wine)  # wine 의 컬럼명 확인 


for  ( i  in  3 : length(colnames(wine) ) ) {
  
  a <- grubbs.flag(wine[  ,  colnames(wine)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(wine)[i], ' --->', length(b) )  )
  
}

#3. 결측치를 확인합니다. 

colSums(is.na(wine))

#4. 히스토그램 그래프 + 정규분포 그래프를 통해서 데이터들이 정규성을  보이는지 확인합니다.

#install.packages("psych")
library(psych)
pairs.panels(wine[c("Ash", "Magnesium","Proanthocyanins")] )

# ▩ 3단계 : 데이터로 모델 훈련 

# 1. 데이터의 구조를 확인하여 라벨(정답) 컬럼이 factor 인지 확인합니다. 

str(wine)  

wine <- read.csv("wine.csv", stringsAsFactors=TRUE)

str(wine)

#2.  데이터를 양성과 악성이 잘 섞일 수 있도록 데이터를 섞어 줍니다. 

set.seed(1)

wine_shuffle <- wine[ sample(nrow(wine)) ,  ]

wine_shuffle

#3.   훈련 할때 필요한 컬럼만 선택합니다.   

wine_shuffle2 <- wine_shuffle[   , c(-1) ] # 정답만 제외합니다. 

wine_shuffle2

#4. 컬럼들의 단위가 다 다르므로 데이터를 정규화 합니다. 

normalize <- function(x) {  return ( (x-min(x))  / ( max(x) - min(x) )  )   }

wine_n <- as.data.frame( lapply( wine_shuffle2, normalize )  ) 

summary(wine_n)

#5. 전체 178개의 행의 데이터를 훈련(공부) 데이터와 테스트(시험) 데이터로
#   나눠줘야합니다. ( 훈련 데이터 9, 테스트 데이터 1 로 나눈다)

nrow(wine_n) #178

n_90 <-  round(0.9*nrow(wine_n) )
n_90  # 160

wine_train <- wine_n[ 1:160,   ]            # 훈련 데이터 구성   
wine_test  <- wine_n[ 161 : 178,  ]        # 테스트 데이터 구성

nrow(wine_train)  # 160 
nrow(wine_test)   # 18

# 6. 정답도 훈련과 테스트로 나눕니다. 

head(wine_shuffle[  ,  1])

wine_train_label <- wine_shuffle[ 1: 160 ,  1]
wine_test_label <- wine_shuffle[ 161 : 178 ,  1]

length(wine_train_label )  # 160
length(wine_test_label)    # 18

#7. 512개의 훈련 데이터와 훈련 데이터의 정답으로 거리계산한 데이터로
#   테스트 데이터 57개를 분류 합니다. 

#install.packages("class")
library(class)

result1 <- knn(train=wine_train,  test=wine_test, cl=wine_train_label, k=1)
result1

#▩ 4단계. 모델 성능 평가

result1 == wine_test_label

sum(result1 == wine_test_label)

sum(result1 == wine_test_label)/18 *100

* k값과 정확도를 비교하는 for loop 문

for (i in c(seq(1,150,2))){
  result1 <- knn(wine_train, wine_test, cl=wine_train_label, k=i)
  a <- sum(result1 == wine_test_label)/length(wine_test_label)*100
  print(paste(i,round(a,2)))
}

▩ 정확도외에  모델을 평가하는 다른 방법 (p 138)

1. 유방암 데이터의 예측 결과에 대한 이원교차표를 출력하시오 !

#▩ 1단계: 데이터 수집

wbcd <- read.csv("wisc_bc_data.csv")

nrow(wbcd)   # 행의 갯수   569개
ncol(wbcd)    # 열의 갯수   32개
str(wbcd)

#▩ 2단계: 데이터 탐색   

# 1.  정답에 해당하는 라벨 컬럼의 데이터 분포를 확인 합니다.

table(wbcd$diagnosis)

#2. 이상치를 확인합니다.

library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(wbcd)  # wbcd 의 컬럼명 확인 

a <- grubbs.flag(wbcd$radius_mean)  # wbcd 의 radius_mean 컬럼에 이상치가 있는지 확인 
#  하기위해 데이터 프레임을 생성 
a[a$Outlier==TRUE,     ]               # a 데이터 프레임의 Outlier 컬럼이 TRUE 인 데이터를 


for  ( i  in  3 : length(colnames(wbcd) ) ) {
  
  a <- grubbs.flag(wbcd[  ,  colnames(wbcd)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(wbcd)[i], ' --->', length(b) )  )
  
}

#3. 결측치를 확인합니다. 

colSums(is.na(wbcd))

#4. 히스토그램 그래프 + 정규분포 그래프를 통해서 데이터들이 정규성을  보이는지 확인합니다.

#install.packages("psych")
library(psych)
pairs.panels(wbcd[c("dimension_se", "symmetry_se","perimeter_se")] )

# ▩ 3단계 : 데이터로 모델 훈련 

# 1. 데이터의 구조를 확인하여 라벨(정답) 컬럼이 factor 인지 확인합니다. 

str(wbcd)  

wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

str(wbcd)

#2.  데이터를 양성과 악성이 잘 섞일 수 있도록 데이터를 섞어 줍니다. 

set.seed(1)

wbcd_shuffle <- wbcd[ sample(nrow(wbcd)) ,  ]

wbcd_shuffle

#3.   훈련 할때 필요한 컬럼만 선택합니다.   

wbcd_shuffle2 <- wbcd_shuffle[   , c(-1,-2) ] # 환자번호와 정답을 제외합니다. 

wbcd_shuffle2

#4. 컬럼들의 단위가 다 다르므로 데이터를 정규화 합니다. 

normalize <- function(x) {  return ( (x-min(x))  / ( max(x) - min(x) )  )   }

wbcd_n <- as.data.frame( lapply( wbcd_shuffle2, normalize )  ) 

summary(wbcd_n)

#5. 전체 569개의 행의 데이터를 훈련(공부) 데이터와 테스트(시험) 데이터로
#   나눠줘야합니다. ( 훈련 데이터 9, 테스트 데이터 1 로 나눈다)

nrow(wbcd_n) #569

n_90 <-  round(0.9*nrow(wbcd_n) )
n_90  # 512

wbcd_train <- wbcd_n[ 1:512,   ]            # 훈련 데이터 구성   
wbcd_test  <- wbcd_n[ 513 : 569,  ]        # 테스트 데이터 구성

nrow(wbcd_train)  # 512
nrow(wbcd_test)   # 57

# 6. 정답도 훈련과 테스트로 나눕니다. 

head(wbcd_shuffle[  ,  2])

wbcd_train_label <- wbcd_shuffle[ 1: 512 ,  2]
wbcd_test_label <- wbcd_shuffle[ 513 : 569 ,  2]

length(wbcd_train_label )  # 512
length(wbcd_test_label)    # 57

#7. 512개의 훈련 데이터와 훈련 데이터의 정답으로 거리계산한 데이터로
#   테스트 데이터 57개를 분류 합니다. 

#install.packages("class")
library(class)

result1 <- knn(train=wbcd_train,  test=wbcd_test, cl=wbcd_train_label, k=1)
result1

#▩ 4단계. 모델 성능 평가

result1 == wbcd_test_label

sum(result1 == wbcd_test_label)

sum(result1 == wbcd_test_label)/57 *100

library(gmodels)

CrossTable( x=wbcd_test_label, y=result1, prop.chisq=FALSE)

Positive  ( 관심범주   O )  ---> 암환자
Nagative (관심범주가 X )  ---> 정상환자

                          예측
                  정상              암            관심범주로 예측했는데 
실제  정상      32(TN)          2 (FP)       관심범주로 못 맞춘것
        암        0  (FN)          23(TP)  <--- 관심범주로 예측했는데
                                                     관심범주로 잘 맞춘것 
    관심범주가 아닌것을 예측했는데 
    관심범주가 아닌것으로 잘 맞춘것          

 관심범주가 아닌것으로 예측했는데
 관심범주가 아닌것으로 잘 못맞춘것 

※ 이원 교차표 설명 

 Positive ---> 관심범주 (암, 스팸메일)

  TP :    True 는 잘 판단했다.  Positive : 암환자로 
                                ↓
                      암환자로 잘 판단했다.
                                 ↓
             (암환자를) 암환자로 잘 판단했다.

  FP:     False 는 잘못 판단했다.  Positive:  암환자로
                                  ↓
                     암환자로 잘못 판단했다.
                                   ↓
          (정상환자를) 암환자로 잘못 판단했다.

  TN:   True 는 잘 판단했다.  Nagative : 정상환자로 
                                   ↓
                     정상환자로 잘 판단했다.
                                    ↓
        (정상환자를) 정상환자로 잘 판단했다.

   FN :  False : 잘못 판단했다. Nagative : 정상환자로
                                    ↓
                  정상환자로 잘못 판단했다.
                                       ↓
       (암환자를) 정상환자로 잘못 판단했다.

 책 142 페이지의 표

   k값    거짓부정    거짓긍정     부정확하게 분류된 백분율
   1          1             3                    4%
   5          2             0                    2%
   11        3             0                    3% 
   15        3             0                    3%
   21        2             0                    2%
   27        4             0                    4%
   
■ 판다스로 유방암 판정 knn 모델  생성하기 

#1. 데이터를 로드합니다.
import  pandas  as  pd

wbcd = pd.read_csv("d:\\data\\wisc_bc_data.csv")
wbcd

※ 판다스는 R 과는 다르게 strtigsAsFactors =True 를 지정하지 않아도 됩니다. 

#2. 데이터를 확인합니다. 
wbcd.info()  # 컬럼명과 데이터 타입을 확인할 수 있습니다. 
wbcd.shape # 몇행 몇열인지 확인할 수 있습니다. 
wbcd.describe() # R 에서의 summary() 함수와 같은 결과를 출력합니다.

#3. 결측치를 확인합니다.
wbcd.isnull().sum()

#4. 이상치를 확인합니다.

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 5)|(x[i]<Q1-IQR*5)].count())
        
outlier_value(wbcd)

설명: area_se  와 dimension_se 의 이상치가 보이므로 모델평가후에
       정확도를 더 높이기 위해서 이 두 컬럼의 이상치를 중앙값으로 치환
       해볼 필요가 있습니다. 
                                                        
#5. 명목형 데이터가 있는지 확인합니다.

wbcd.info()

#6. 데이터를 정규화 합니다.
from   sklearn.preprocessing  import  MinMaxScaler

wbcd2 = wbcd.iloc[ :  , 2: ]  # 환자번호와 diagnosis 제외합니다. 
wbcd2

scaler = MinMaxScaler()   

scaler.fit(wbcd2)   #  최대최소법으로 데이터를 계산합니다.

wbcd2_scaled = scaler.transform(wbcd2)  # 위에서 계산한 내용으로 데이터를 
wbcd2_scaled                                    # 변환해서 wbcd2_scaled 에 담습니다.

wbcd2_scaled.shape  # (569, 30)     numpy array 형태로 변경되었습니다. 

y = wbcd['diagnosis'].to_numpy()    # 정답 데이터를 numpy array 로 변경합니다.
print(y)

#7. 훈련데이터와 테스트데이터로 데이터를 분리합니다.(훈련90%, 테스트:10%)

from sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( wbcd2_scaled, y, test_size=0.1, random_state=1)

# x_train : 훈련 데이터 , x_test : 테스트 데이터,
# y_train: 훈련데이터의 정답, y_test: 테스트 데이터의 정답 

# random_state=1 은 어느 자리에서든 동일한 정확도를 보이는 모델을 만들기 위해서
# 설정한 것입니다. 

# test_size=0.1 로 했기 때문에 훈련 90%, 테스트 10% 로 나뉜것 입니다. 

print( x_train.shape)  # (512, 30)
print( x_test.shape)   # (57, 30)
print( y_train.shape)  # (512, )
print( y_test.shape)   # (57, )

#8. 모델을 설정합니다.
from  sklearn.neighbors   import  KNeighborsClassifier

model = KNeighborsClassifier( n_neighbors= 5 )  # knn 모델생성

#9. 모델을 훈련시킵니다.
model.fit( x_train, y_train )

#10. 훈련된 모델로 테스트 데이터를 예측합니다.
result = model.predict(x_test)
result

#11. 모델을 평가합니다.
y_test==result
sum(y_test==result)/57*100

또는 

from  sklearn.metrics  import  accuracy_score

acurracy = accuracy_score( y_test, result)
acurracy

#12. 모델의 성능을 높입니다.

from  sklearn.metrics  import  confusion_matrix
a = confusion_matrix( y_test, result )
print(a)
[[43  0]
 [ 1 13]]

tn, fp, fn, tp = confusion_matrix(y_test, result).ravel()

print( tn, fp, fn, tp )  # 43, 0, 1, 13

문제249.  FN 를 0 으로 만들면서 정확도가 가장 좋은 K 값을 무엇인지 
            알아내세요 ~~  답글로 올려주세요 ~~

accu = []
x_train,x_test,y_train,y_test = train_test_split(wbcd2_scaled, y, test_size=0.1, 
                                                 shuffle=True, random_state=3)
for i in range(1,512,2):   
    model = KNeighborsClassifier(n_neighbors = i)
    model.fit(x_train, y_train)
    result = model.predict(x_test)
    accuracy = accuracy_score(y_test, result)
    tn,fp,fn,tp = confusion_matrix(y_test, result).ravel()
    if fn == 0:
        accu.append((i,accuracy,fn))
k, accuracy, fn = sorted(accu,key=lambda x:x[1].max(),reverse=True)[0]
print(f'k값 {k}값,accuracy값 {accuracy},fn값 {fn}')

문제250.(오늘의 마지막 문제) 와인 데이터(wine.csv) 를 가지가 와인의 종류를
            분류하는 머신러닝 모델을 파이썬으로 구현하시오 !

머신러닝 공부를 효율적을 하는 방법

1. 미리 배울 내용을 읽어온다.
2. 수업 때 미리 읽어온 내용을 생각하면서 수업을 듣는다.
3. 5시 이후에 자습할때 본인 분석하고 싶은 데이터를 하나 정해서 
   재미있게 분석해본다. 

■ 4장. 나이브 베이즈 알고리즘 

 머신러닝의 종류 3가지? 

 1. 지도학습   : 정답이 있는 데이터를 기계가 학습 : 
            분류 : knn (3장), naivebasyes(4장)
            회귀:

 2. 비지도학습 : 정답이 없는 데이터를 기계가 학습

 3. 강화학습 :  환경을 기계가 스스로 학습하면서 알아가는 학습방법 
                    ↓
               바둑, 오목, 기타

▩ 나이브 베이즈 알고리즘이 사용이 되는 분야 ?

   1. 스팸 이메일 필터링과 같은 텍스트 분류
   2. 컴퓨터 네트워크에서 침입이나 비정상적인 행위 탐지
   3. 일련의 관찰된 증상에 따른 의학적 질병 진단

▩ 나이브 베이즈 이론 설명 

  R 목차 ---> 나이브 베이즈 이론 ppt 

예제1:  비아그라가 포함되어져 있는 메일이 스팸메일일 확률을 구하시오 !

                                      
                              P(비아그라 | 스팸) * P(스팸)
 P(스팸 | 비아그라 ) =  -------------------------------
                                       P(비아그라)


위의 결과 확률이 50% 를 넘어가게 되면 이 메일은 햄보다는 스팸메일일 
가능성이 더 크다. 

p 154 페이지 그림4.5

      빈도표 --------------------------------> 우도표

         비아그라                                  비아그라
         예   아니오    총계                     예    아니오   총계
스팸    4     16         20              스팸  4/20   16/20   20/100
햄       1     79         80                햄   1/80   79/80   80/100
          5     95        100                     5/100  95/100


                              P(비아그라 | 스팸) * P(스팸)         4/20   *  20/100
 P(스팸 | 비아그라 ) =  ------------------------------- = ------------------------
                                       P(비아그라)                           5/100

                                                                   = 0.8

▩  비아그라 뿐만 아니라 다른 단어들도 여러개 있는 경우의 스팸일 확률 ?

P( 스팸  | 비아그라  ∩  돈   ∩  식료품 ∩  주소삭제 ) = ?

비아그라 단어 하나만 가지고 스팸 메일인지를 분류하려면 정확하게
분류가 안될 수 있으니 다른 단어들도 같이 포함시켜야 합니다.

 수학기호 :  ￢   :  존재하지 않는다(부정)
                 ∃   :  존재한다 (긍정)

 P( 스팸  | 비아그라  ∩  ￢돈   ∩  ￢식료품 ∩  구독취소 ) = ?

 비아그라와 구독취소는 포함되어져있는데 돈과 식료품은 포함되지 않은
 메일이 스팸일 확률 ?  

            P(B|A) * P(A)    P(비아그라  ∩  ￢돈   ∩  ￢식료품 ∩  구독취소 | 스팸) * P(스팸)
P(A|B) = -------------- = -----------------------------------------------------------------------
                 P(B)           P(비아그라  ∩  ￢돈   ∩  ￢식료품 ∩  구독취소 ) 

   P(비아그라 | 스팸) * P(￢돈 | 스팸) * P( ￢식료품 | 스팸) * P(구독취소 | 스팸 ) * P(스팸)
= -------------------------------------------------------------------------------------------------
               P(비아그라  ∩  ￢돈   ∩  ￢식료품 ∩  구독취소 ) 

   P(비아그라 | 스팸) * P(￢돈 | 스팸) * P( ￢식료품 | 스팸) * P(구독취소 | 스팸 ) * P(스팸)
= ----------------------------------------------------------------------------------------------
                                       분모 무시 

분모는 타깃 클래스(스팸이나 햄) 에 종속되지 않기 때문에 상수값으로 취급하며, 
당분간 무시할 수 있다. 따라서 스팸에 대한 조건부 확률은 다음과 같이 표현된다. 

  P(비아그라 | 스팸) * P(￢돈 | 스팸) * P( ￢식료품 | 스팸) * P(구독취소 | 스팸 ) * P(스팸) =  0.012
         4/20                    10/20              20/20                   12/20               20/100

  P(비아그라 | 햄) * P(￢돈 | 햄) * P( ￢식료품 | 햄) * P(구독취소 | 햄 ) * P(햄) =  0.002
          1/80                66/80        71/80                  23/80           80/100

      빈도표 --------------------------------> 우도표

 156페이지 아래에 표를 참고하세요!

 스팸의 전체 우도 ?   0.012   
 햄의 전체 우도 ?   0.002
  
                           0.012 
스팸일 확률 ? ---------------------=  0.857
                     0.012 + 0.002

                            0.002
햄일 확률 ?   ---------------------- = 0.143
                       0.012 + 0.002


이 메세지에 있는 단어 패턴에 대해 85.7% 의 확률로 메세지가 스팸이고 
14.3% 의 확률로 햄이라고 예상한다. 

■ 라플라스 추정기 (P159) 

  P(비아그라 | 스팸) * P(￢돈 | 스팸) * P( ￢식료품 | 스팸) * P(구독취소 | 스팸 ) * P(스팸) =  
         4/20                    10/20              0/20                   12/20               20/100

= 0

위와 같이 분자 하나가 0 이면 전체가 0 이 되면서 스팸의 우도가 0 이 되어 버립니다.
그러면서 더 이상 계산을 진행 할 수 없게 됩니다. 그래서 수학자 라플라스가 어떻게
했나면 ?  0 을 1로 만들어주면서 아래와 같이 

스팸의 우도?

  P(비아그라 | 스팸) * P(￢돈 | 스팸) * P( ￢식료품 | 스팸) * P(구독취소 | 스팸 ) * P(스팸) =  
         5/24                    11/24              1/24                   13/24               20/100

햄의 우도?

  P(비아그라 | 햄) * P(￢돈 | 햄) * P( ￢식료품 | 햄) * P(구독취소 | 햄 ) * P(햄) =  0.002
          2/84                67/84        72/84                  24/84            80/100

 아주 작은값을 하나 더해서 계산이 될 수 있도록하는데 이 값을 라플라스 값이라고 합니다. 

  라플라스 값을 주어서 나이브베이즈 모델의 성능을 올리는데 사용합니다.

하이퍼 파라미터 ?  머신러닝의 성능을 높이기 위해서 모형 개발자가 직접 조정해줘야하는
                          파라미터 

※ knn 일때는 k값이 하이퍼 파라미터 였는데 나이브 베이즈에서는 라플라스 값이 
   하이퍼 파라미터 입니다. 

▩ R 을 이용해서 나이브 베이즈 머신러닝 모델 만들기 

  " 정상버섯과 독버섯을 분류하는 머신러닝 모델을 생성하기 "

 데이터셋 :  데이터 게시판 

# 1. 데이터를 로드합니다.
mush <- read.csv( "mushrooms.csv", stringsAsFactors=TRUE)
str(mush)

맨앞에 있는 type 이 라벨(정답) 입니다. 

table(mush$type)
 edible     poisonous 
     4208      3916 

독버섯과 정상버섯의 비율이 어떻게 되는지 확인하시오!

prop.table(table(mush$type))

   edible        poisonous 
0.5179714     0.4820286 

두개가 딱 절반이어서 독버섯도 잘 학습 할 수 있고 정상버섯도 잘 학습 할 수
있게 되어있습니다. 

전체 건수가 어떻게 되는지 확인합니다.
dim(mush)

 8124   23

# 2. 결측치를 확인합니다.
colSums(is.na(mush))

# 3. 이상치를 확인합니다.

명목형 데이터여서 이상치를 확인할 필요가 없습니다.

# 4. 명목형 데이터가 있는지 확인합니다.

전부 명목형 데이터 입니다. 

# 5. 데이터를 정규화 합니다.

전부 명목형 데이터이므로 정규화 작업도 필요하지 않습니다. 

# 6. 훈련 데이터와 테스트 데이터로 데이터를 분리합니다.

데이터 shuffle 과 데이터 분리를 효율적이면서도 편하게 수행할 수 있는
패키지를 이용해서 분리해보겠습니다.

install.packages("caret")
library(caret)
set.seed(1)
k <- createDataPartition(mush$type, p=0.8 , list=F)  # 훈련 데이터 80%, 테스트 20%
train_data <- mush[ k,   ]
test_data  <- mush[-k,  ]

dim(train_data)   #  6500   23
dim(test_data)    #  1624   23

prop.table( table(train_data$type) ) 

prop.table( table(test_data$type) ) 

훈련 데이터와 테스트 데이터의 독버섯과 정상버섯이 거의 50:50 으로 
균등하게 분포 되어있습니다. 
 
# 7. 나이브 베이즈 모델을 생성합니다.
install.packages("e1071")
library(e1071)

model <-  naiveBayes( type~ . ,  data= train_data )
model

버섯 데이터로 빈도표를 만들고서 우도표를 생성했다. 

# 8. 훈련 데이터와 라벨(정답)으로 모델을 훈련 시킵니다.

7번에서 다 수행했습니다. 

# 9. 훈련된 모델로 테스트 데이터를 예측합니다.

result <- predict( model,  test_data[   , -1] ) 

테스트 데이터의 정답을 제외하고 예측합니다. 
result

# 10. 모델의 성능을 평가합니다.

sum(result == test_data[  , 1 ]) / length(test_data[ , 1] )

0.9378079

어제는 유방암 데이터가 전부 숫자여서 knn 알고리즘을 이용해서 기계학습 시켰고
오늘은 독버섯 데이터가 전부 명목형 이어서 naivebayes 를 이용해서 기계학습
시켰습니다. 

# 11. 모델의 성능을 높입니다. 

model2 <- naiveBayes( type~ . , data= train_data, laplace=0.0001)

result2 <- predict( model2,  test_data[   , -1] ) 

sum(result2 == test_data[  , 1 ]) / length(test_data[ , 1] )

 0.9950739

문제251.  직업과 성별과 결혼 유무등의 데이터를 가지고 영화장르를 예측하는데
             나이브 베이즈 모델을 생성하시오 ! (데이터셋: movie.csv)

# 1. 데이터를 로드합니다.
movie <- read.csv( "movie2.csv", stringsAsFactors=TRUE)

# 2. 결측치를 확인합니다.
colSums(is.na(movie))

# 3. 이상치를 확인합니다.

#명목형 데이터여서 이상치를 확인할 필요가 없습니다.

# 4. 명목형 데이터가 있는지 확인합니다.

#전부 명목형 데이터 입니다. 

# 5. 데이터를 정규화 합니다.

# 6. 훈련 데이터와 테스트 데이터로 데이터를 분리합니다.

#install.packages("caret")
library(caret)
set.seed(1)
k <- createDataPartition(movie$장르, p=0.8 , list=F)  # 훈련 데이터 80%, 테스트 20%
train_data <- movie[ k,   ]
test_data  <- movie[-k,  ]

dim(train_data)   #  6500   23
dim(test_data)    #  1624   23

# 7. 나이브 베이즈 모델을 생성합니다.
#install.packages("e1071")
library(e1071)

model <-  naiveBayes( 장르~ . ,  data= train_data )
model

# 8. 훈련 데이터와 라벨(정답)으로 모델을 훈련 시킵니다.



# 9. 훈련된 모델로 테스트 데이터를 예측합니다.

result <- predict( model,  test_data[   , -6] ) 


# 10. 모델의 성능을 평가합니다.

sum(result == test_data[  , 6 ]) / length(test_data[ , 6] )
# 11. 모델의 성능을 높입니다. 

model2 <- naiveBayes( 장르~ . , data= train_data, laplace=0.0001)

result2 <- predict( model2,  test_data[   , -6] ) 

sum(result2 == test_data[  , 6 ]) / length(test_data[ , 6] )

문제252.  직업이 학생이고 결혼을 안했으며 이성친구가 없는 20대 남자가
             선호하는 영화 장르가 무엇으로 예측되는지 지금 방금 만든
             나이브베이즈 모델에 데이터를 입력해서 출력하시오 !

1. 방금생성한 정확도 100% 의 모델로 예측

test_data2 <- data.frame( 나이='20대', 성별='남', 직업='학생', 결혼여부='NO',
                                 이성친구='NO')

result  <-  predict( model2, test_data2, laplace=0)
result

문제253. 독감 데이터로 나이브 베이즈 모델로 생성해서 독감환자인지
            아닌지 분류하는 모델을 만드시오 ! (데이터셋:  flu.csv )

patient_id   : 환자번호
chills         :  오한
runny_nose  : 콧물
headache   : 두통
fever   : 열
flue   : 독감여부

flu <-  read.csv("flu.csv" , header=T, stringsAsFactors=TRUE)

train_data <- flu[ 1:7, -1]  # flu[ 행, 열 ]
                                  # 행에 1:7은 1행~7행까지 가져와라~ 
                                  # 열에 -1 은 환자 id 가 첫번째여서 -1로 환자id 를 제외함
test_data <- flu[ 8, -1 ]    # 8번째행만 가져오는데 환자id 컬럼 제외하고 가져와라 


model3 <-  naiveBayes( train_data[ , 1:4] ,  train_data$flue, laplace=0) 

# 설명: train_data[   , 1:4 ]  는 오한,콧물,두통,열 컬럼만 가져와라 ~
# 설명:  naiveBayes(  정답없는 훈련 데이터,   정답,   라플라스값 )

model3 

# 5. 테스트 데이터  test_data 를 가지고 예측을 해봅니다.

result3 <-  predict( model3, test_data[  , 1:4] )
#설명: test_data[   , 1:4 ]  는 오한,콧물,두통,열 컬럼만 가져와라 ~

#예측값 확인
result3                        

#실제값 확인
test_data[   , 5] 

▩ 파이썬으로 나이브 베이즈 모델 생성하기 

#1. 데이터를 로드합니다.
import pandas  as  pd

mush = pd.read_csv("d:\\data\\mushrooms.csv")
#2. 결측치를 확인합니다.
mush.isnull().sum()

#3. 이상치를 확인합니다.

전부 명목형 데이터 입니다. 

#4. 명목형 데이터를 확인합니다.

R에서는 데이터를 명목형인 상태 그대로 훈련을 시켰는데 파이썬에서는 전부 숫자로
변경해줘야 합니다. 

# 정답을 뺀 데이터만 선별합니다.
x = mush.iloc[ : , 1 : ]
x.head()

# 정답 데이터를 y 변수에 담습니다. 
y = mush.iloc[ : ,  0]
y.head()

# 명목형 데이터를 숫자로 변경합니다. 

mush2 = pd.get_dummies(x)
mush2.head()
mush2.shape  # (8124, 117)  컬럼의 갯수가 23개에서 117개로 늘어났습니다. 
mush2.info()    # 전부 숫자인지 확인합니다.  

#5. 정규화를 수행합니다.
from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(mush2)
mush2_scaled = scaler.transform(mush2)
mush2_scaled   # numpy  array 형태로 생성함

y = mush.iloc[ : ,  0].to_numpy()
y

#6. 훈련 데이터와 테스트 데이터를 분리 합니다.
from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( mush2_scaled, y, test_size=0.2, random_state=1)

print( x_train.shape)  # (6499, 117)
print( x_test.shape )  # (1625, 117)

#7. 나이브 베이즈 모델을 생성합니다.

#1. BernoulliNB :  이산형 데이터를 분류할 때 적합 
#2. GaussianNB : 연속형 데이터를 분류할 때 적합
#3. MultinomialNB   : 이산형 데이터를 분류할 때 적합

from  sklearn.naive_bayes  import  BernoulliNB 
#from  sklearn.naive_bayes  import  GaussianNB
#from  sklearn.naive_bayes  import  MultinomialNB           

model = BernoulliNB()

#8. 훈련 데이터로 모델을 훈련시킵니다.
model.fit(x_train, y_train)

#9. 테스트 데이터를 예측합니다.
result = model.predict(x_test)

#10. 모델을 평가합니다.
sum(result == y_test) / len(y_test)
# 0.939076923076923

from  sklearn.metrics  import  accuracy_score
accuracy = accuracy_score(y_test, result)
accuracy
# 0.939076923076923

from  sklearn.metrics  import  confusion_matrix
a = confusion_matrix( y_test, result )
print(a)
[[815   5]
 [ 94 711]]

tn, fp, fn, tp = confusion_matrix(y_test, result).ravel()
print( tn, fp, fn, tp)

#11. 모델의 성능을 올립니다. 

#from  sklearn.naive_bayes  import  BernoulliNB 
from  sklearn.naive_bayes  import  GaussianNB
#from  sklearn.naive_bayes  import  MultinomialNB           

model2 =  GaussianNB( var_smoothing=0.001)
model2.fit( x_train, y_train)
result2 = model2.predict(x_test)

sum(result2 == y_test) / len(y_test)
# 0.9950769230769231

from  sklearn.metrics  import  confusion_matrix
a = confusion_matrix( y_test, result2 )
print(a)

[[814   6]
 [  2 803]]

문제254. for loop 문을 이용해서 GaussianNB 의 FN 값이 0 이 되는 var_smoothing
           값이 무엇인지 알아내시오 !

import  numpy  as  np
from  sklearn.metrics  import  confusion_matrix
a = confusion_matrix( y_test, result2 )

errors=[]
for  i  in  np.arange(0.001, 0.01, 0.001):
    model3 = GaussianNB(var_smoothing=i)
    model3.fit( x_train, y_train)
    result3 = model3.predict(x_test)
    fn = confusion_matrix( y_test, result3 )[1][0]
    errors.append(fn)

for  s, i  in   zip(  np.arange(0.001, 0.01, 0.001), errors):
    print ( s , '--------------->', i )

[[814   6]
 [  2 803]]

결론:  독버섯 데이터를 나이브베이즈 모델중 GaussianNB 로 학습 시켰을때
         정확도는  0.995 까지 개선되었습니다. 그런데 FN 값이 2개가 나타나고
         있어서 독버섯 데이터를 분류하는 모델은 FN 값을 0 으로 만드는게
        중요하므로 모델을 다른 모델로 변경해서 0으로 만들어야합니다. 

현업에서 머신러닝 데이터 분석시 가장 중요한것 ? 마감날짜 엄수

▩ 명목형 변수를 숫자로 변경해주는 코드 2가지 ?

1. pd.get_dummies  : 컬럼을 만들어서 0 과 1로 데이터를 생성 
  
        컬럼               데이터                컬럼 
 예:  버섯색깔:   red,blue, green ------>   red, blue, green 
                                                      0     0      0      데이터
                                                      1     1      1

2. LabelEncoder  :  컬럼안에 있는 명목형을 숫자로 변경

        컬럼               데이터             데이터
  예:  버섯색깔:   red, blue, green --->  0, 1, 2              45분까지 쉬세요 ~~



문제255. (오늘의 마지막 문제)  나이가 20대이고 성별이 여자이며 
           직업이 IT 이고 결혼을 안했으며 이성친구가 없는 사람이 
           선택할 가능성이 높은 영화 장르는 ?  ( 데이터셋: movie.csv)
           

■ R 을 활용한 머신러닝 with Pandas 복습

1장.  R 기본 문법 
2장.  이 책을 보기위한 기본 문법 
3장.  knn (분류 머신러닝 모델)
          - 이론
          - R   ( 유방암 데이터)
          - 파이썬 ( 와인 데이터) 
4장.  naivebayes (분류 머신러닝 모델)
          - 이론
          - R   ( 독버섯 데이터)
          - 파이썬 (영화 장르 데이터)

▩  5장. 의사결정트리 

 의사결정트리 ?  데이터들이 가진 속성들로 부터 분할 기준 속성을 판별하고,
                      분할 기준 속성에 따라 트리 형태로 모델링하는 분류 예측 모델을
                      의사결정트리 모델이라고합니다. 

 회귀분석, 의사결정트리는 현업에서 고객들과 데이터 분석가들이 선호하는
 머신러닝 알고리즘 입니다. 신경망이 정확도를 휠씬 뛰어난데 신경망의 경우는
 신경망 내부가 블랙박스이다 보니 설명이 안되서 고객들이 이해를 못하는 경우가  
 많습니다. 의사결정트리와 회귀분석은 설명이 가능해서 왜 이렇게 예측하고 
 분류했는지 설명이 가능해서 선호하는 분류 모델입니다.

             표                                           의사결정트리 그림 
    수중  지느러미  물고기
1  YES      YES         YES                                   수중
2  YES      NO         NO                   
3   NO      YES        NO                     NO                    지느러미
4   NO      NO        NO  
                                                                          NO         YES 
                                                                      



의사결정트리는 어떻게 데이터를 분류하는가?

         1. 정보획득량이 높은 컬럼을 선별하고 그것부터 먼저 물어봅니다.

         2. 물어보면서 자식 노드로 내려올때 데이터의 분류가 점점 명확해져야
            되므로 부모 노드 때 보다 자식 노드때의 데이터가 더 불순도(entropy)
            가 낮아져야 합니다. 점점 순도가 높아지겠금 가지를 치고 내려오면서
            데이터를 분류하는 것입니다.


순수도 ?   목표변수의 특정범주에 개체들이 포함되어져 있는 정도
                            (관심범주o, 관심범주x) 

부모마디의 순수도에 비해서 자식 마디들의 순수도가 증가하도록 자식 마디를
형성해 나가면서 의사결정트리를 만든다.

불순도(entropy) 의 수학공식?  p202 페이지 





정보획득량의 수학공식 ?      p 203 페이지

                               분할전 엔트로피 - 분할후 엔트로피 

의사결정트리 ppt 참고

* 결혼 유무에 대한 정보 획득량 구하기 

분할 후 엔트로피: 

(8/16)*(-(7/8)*log2(7/8) -(1/8)*log2(1/8)) 
+ (8/16)*(-(3/8)*log2(3/8) -(5/8)*log2(5/8))

정복획득량 구하기 :  0.9545 - 0.7489992

■ 화장품 구매에 영향을 크게 미치는 변수(컬럼) 가 무엇인지 정보획득량을
    구하시오 !( 데이터 : skin.csv )

skin <- read.csv("skin.csv", stringsAsFactors=T)
skin

install.packages("FSelector")
library(FSelector)

wg <- information.gain(cupon_react ~ . ,  skin, unit='log2' )

wg
※ 설명:  information.gain( 라벨컬럼 ~ 모든컬럼, 데이터프레임명, unit='log2')

              attr_importance
cust_no      0.00000000
gender       0.06798089
age           0.00000000
job           0.03600662
marry        0.18350551
car            0.02487770

결혼 유무가 정보획득량이 제일 높게 나타나고 있습니다. 

문제256. 지방간을 일으키는 원인중에 가장 큰 영향력을 보이는 요인은 무엇인지
            정보획득량을 구해서 알아내시오 !  ( 데이터: fatliver2.csv)

fat <- read.csv("fatliver2.csv", stringsAsFactors=T)

library(FSelector)

wg <- information.gain( FATLIVER ~ . , fat,  unit='log2')

wg
> wg
        attr_importance
AGE            0.032256902
GENDER       0.028650604
DRINK         0.012189492
SMOKING    0.009812076

■ 정보획득량을 판다스로 구하기 

예제1.  skin.csv 를 판다스 데이터 프레임으로 만드세요

import pandas as pd 
skin = pd.read_csv("d:\\data\\skin.csv")
skin

예제2. skin 데이터 프레임에서 정답 컬럼인 cupon_react 컬럼의 데이터만
         x 라는 변수에 담으시오 

x = skin['cupon_react']
x

예제3. 구매여부와 결혼유무와의 정보획득량을 구하기 위해서 확률을 구하기
         쉽게 판다스의 crosstab  함수를 이용해서 아래의 결과를 출력하시오!

ct = pd.crosstab( x, skin['marry'], margins=True)
ct

marry	NO	YES	All
cupon_react			
NO	9	8	17
YES	1	12	13
All	10	20	30

예제4. 위의 ct 데이터 프레임에서 데이터를 추출해서 아래의 before 라는
        리스트를 만든시오 !  before=[ 17/30, 13/30 ] 

marry	NO	YES	All
cupon_react			
NO	9	8	17
YES	1	12	13
All	10	20	30

ct.loc['NO','All']  # 17
ct.loc['YES','All']  # 13
ct.loc['All', 'All']  # 30

before = [ ct.loc[ i, 'All'] / ct.loc['All','All'] for  i  in ['NO','YES'] ]
before 

예제5. 위에서 만든 before 리스트의 확률을 이용해서 분할전 엔트로피를
         구하시오 !   before=[ 17/30, 13/30 ] 

print(before_entropy)

0.9871377743721863

import  numpy  as  np

before_entropy = np.sum( [ -i * np.log2(i)  for  i  in  before] )
print(before_entropy)

예제6. 분할후 엔트로피를 구하기 쉽도록 확률이 아래의 after 리스트에 담기게
         하시오 ! 

after = [ 9/10, 1/10, 8/20, 12/20] 

marry	NO	YES	All            ct.loc['NO','NO']  # 9
cupon_react			   ct.loc['YES','NO']  # 1
NO	9	8	17            ct.loc['NO','YES']  # 8
YES	1	12	13            ct.loc['YES','YES']  # 12
All	10	20	30            ct.loc['All','NO']  # 10
                                                     ct.loc['All','YES']  # 20
after =[]
for  i  in  ['NO','YES']:                               12시 신호 보냈습니다.
    for  k  in  ['NO','YES']:
        #print( k,i ,  'All', i )                          점심시간 문제: A 반 라인검사
        after.append(ct.loc[k,i] /ct.loc['All',i] )                        B 반 카페올림

print (after)  # [0.9, 0.1, 0.4, 0.6]

예제7.  위에서 만든 두개의 리스트를 이용해서 분할 후 엔트로피를 출력하시오

marry=[  10/30, 20/30 ]            # 결혼 유뮤에 대한 확률
after = [ 9/10, 1/10, 8/20, 12/20]  # 결혼 유무 분할 한 후의 확률

print( after_entropy)

값 
marry=[  10/30, 20/30 ]            # 결혼 유뮤에 대한 확률
after = [ 9/10, 1/10, 8/20, 12/20]  # 결혼 유무 분할 한 후의 확률

after_entropy = marry[0] * np.sum([-i*np.log2(i) for  i  in  after[0:2]]) +
                     marry[1] * np.sum([-i*np.log2(i) for  i  in  after[2: ]]) 

print (after_entropy) 

예제8.  화장품 고객 데이터의 결혼여부에 대한 정보 획득량을 출력하시오 !

print ( before_entropy - after_entropy )  # 0.18350551353931355

예제9. R 에서 출력한 결혼 유무에 대한 정보획득량과 결과가 똑같은지 확인하시오

library(FSelector)

wg <- information.gain(cupon_react ~ . ,  skin, unit='log2' )
> wg
        attr_importance
cust_no      0.00000000
gender       0.06798089
age           0.00000000
job           0.03600662
marry        0.18350551
car           0.02487770

■ (R 로 의사결정트리 모델 구현하기1) 화장품 구매 고객중 구매가 예상되는
    고객은 누구인지 분류하는 의사결정트리 모델 만들기

nrow(skin)  # 30건 밖에 안되므로 20건으로 학습 시키고 10건으로 테스트 합니다. 

#1. 의사결정트리에 필요한 패키지를 설치합니다.

install.packages("C50")
library(C50)

#2. 화장품 고객 데이터를 로드합니다.
skin <- read.csv("skin.csv", stringsAsFactors=T)
head(skin)

#3. 화장품 고객 데이터를 훈련(80%), 테스트(20%) 로 나눕니다.
library(caret)
set.seed(1)
train_num <- createDataPartition( skin$cupon_react, p=0.8, list=F)

length(train_num) # 25

train_data <- skin[ train_num,   ]
test_data  <- skin[ - train_num,   ]

nrow(train_data)  # 25
nrow(test_data)   # 5 

train_data
#4. 훈련 데이터로 분류 모델을 만듭니다. 
library(C50)

model <- C5.0( train_data[  , c(-1,-7) ], train_data[   , 7] )   
                          ↑                       ↑    
    라벨을 제외한 훈련 데이터      훈련 데이터의 라벨

model

※설명: tree : 5  가지를 5개 만들었음

summary(model)  

Decision tree:  25개로 분류

marry = NO: NO (7)  # 결혼 안했으면 다 구매x
marry = YES:
:...car = YES: YES (7/1)  # 결혼을 했는데 차가 있으면 구매7명인데 1명은 오분류
    car = NO:             # 차가 없는 사람중에서 
    :...job = NO: NO (4)  # 직업이 없으면 다 구매 안했음 (4명)
        job = YES:          #  직업이 있으면 
        :...age <= 20: NO (2)  # 나이가 20 이하이면 구매 안했음(2명)
            age > 20: YES (5)  # 나이가 20보다보면 구매했음 (5명)

#5. 훈련한 모델로 테스트 데이터 10개를 예측합니다.

result <- predict( model,  test_data[  , c(-1, -7) ] )
result 

 NO  NO  YES YES NO 

#6. 모델의 성능(정확도)을 확인합니다. 

sum( result == test_data[   , 7] )  # 5개중에 3개 맞췄습니다. 
sum( result == test_data[   , 7] ) / 5  # 0.6  

#7. 모델의 성능을 높입니다. 

library(C50)

model <- C5.0( train_data[  , c(-1,-7) ], train_data[   , 7], trials = 5 )   

※ 훈련 데이터에서 샘플을 추출해서 5개의 의사결정트리를 만들어서 5개의 의사결정트리
   모델이 다수결에 의해서 훈련 데이터를 분류합니다.

model

Classification Tree
Number of samples: 25 
Number of predictors: 5 

Number of boosting iterations: 5 
Average tree size: 4.6 

summary(model)

Trial	    Decision Tree   
-----	  ----------------  
	  Size      Errors  

   0	     5    1( 4.0%)
   1	     3    5(20.0%)
   2	     5    4(16.0%)
   3	     5    3(12.0%)
   4	     5    2( 8.0%)
boost	          0( 0.0%)   <<


	   (a)   (b)    <-classified as
	  ----  ----
	    14          (a): class NO
	          11    (b): class YES

훈련 데이터에 대해서는 정확도 100%의 의사결정트리 모델이 나왔습니다. 

trails = 5 를 써서 약한 학습자 5명을 생성해서 5명을 이용해서 강한 학습자를 만들어냄 

result2 <- predict(model, test_data[   , c(-1, -7) ]
result2

sum(result2 == test_data[ , 7])

훈련 데이터에 대해서는 100% 의 정확도를 보이는 모델이지만 테스트 데이터는
5개중에 3만 맞췄습니다. 이런 현상을 과대접합(overfitting) 이라고 합니다. 

30개는 데이터가 너무 작아서 의사결정트리 + 앙상블을 구현하기 적절하지 않습니다. 


문제257.  아이리스 (iris) 꽃을 분류하는 분류 모델을 생성하시오 !  (의사결정트리)
             ( 데이터 셋:  iris2.csv )  정답 컬럼: Species

#1. 필요한 패키지를 설치합니다.
library(C50)
#2.  데이터를 로드합니다.
iris <- read.csv("iris.csv", stringsAsFactors=T)
head(iris)

#3.  훈련 데이터와 테스트데이터로 데이터를 분리합니다. (훈련 8, 테스트 2)
library(caret)
set.seed(1)
train_num <- createDataPartition( iris$Species, p=0.8, list=F)

length(train_num) 

train_data <- iris[ train_num,   ]
test_data  <- iris[ - train_num,   ]

nrow(train_data)  
nrow(test_data)   

#4. 훈련 데이터로 의사결정트리 모델을 만듭니다.
library(C50)

model <- C5.0( train_data[  , -5 ], train_data[   , 5] )   
  
model
#5. 훈련 모델에 테스트 데이터를 넣고 예측합니다. 

result <- predict( model,  test_data[  , -5 ] )
result 

#6. 모델의 성능을 확인합니다

sum( result == test_data[   , 5] )  
sum( result == test_data[   , 5] ) / 30 

library(gmodels)
CrossTable( test_data[ , 5],  result )


#7. 모델의 성능을 높입니다. 

model2 <- C5.0( train_data[  , -5 ], train_data[   , 5], trials=10 )   
  
summary(model2)

result2 <- predict( model2,  test_data[  , -5 ] )
result2 

sum( result2 == test_data[   , 5] )  
sum( result2 == test_data[   , 5] ) / 30 

library(gmodels)
CrossTable( test_data[ , 5],  result2 )

# 8.  trails 를 하지 않은 의사결정트리 모델을 시각화 하시오 ! 

library(C50)

model <- C5.0( train_data[  , -5 ], train_data[   , 5] )   

plot(model)


문제258.  화장품 구매 여부 분류 모델을 시각화 하시오 ! 

skin <- read.csv("skin.csv", stringsAsFactors=T)
head(skin)

#3. 화장품 고객 데이터를 훈련(80%), 테스트(20%) 로 나눕니다.
library(caret)
set.seed(1)
train_num <- createDataPartition( skin$cupon_react, p=0.8, list=F)

train_data <- skin[ train_num,   ]
test_data  <- skin[ - train_num,   ]

nrow(train_data)  # 25
nrow(test_data)   # 5 

train_data
#4. 훈련 데이터로 분류 모델을 만듭니다. 
library(C50)

model <- C5.0( train_data[  , c(-1,-7) ], train_data[   , 7] )   
plot(model)

▩ (의사결정트리 실습3) 은행대출채무를 불이행할 것 같은 고객이 누구인지?

은행에서 의사결정트리 모델을 어떻게 활용을 하는가 ?

 은행에 대출을 신청을 했는데 대출 거부를 당했으면 은행에서는 대출거분된
 당사자에게 왜 신청거절되었는지 설명을 해줘야합니다.  이때 이 모델을 
 사용합니다. 

# 1. 데이터 로드
# 2. 데이터 탐색 
# 3. 훈련과 테스트로 데이터를 분리 (훈련 데이터:9, 테스트 데이터 1)
# 4. 훈련 데이터로 모델을 생성합니다.
# 5. 훈련된 모델을 테스트 데이터를 예측합니다.
# 6. 모델의 성능을 평가합니다.
# 7. 모델의 성능을 개선합니다. 

# 1. 데이터 로드

credit <- read.csv("credit.csv",  stringsAsFactors=TRUE)
head(credit)
str(credit)

p207 의 데이터 설명:  독일의 한 신용기관에서 얻은 대출 정보가 있는 데이터

정답(라벨) 컬럼 : default ---->  yes: 대출금 상환 안함 
                                         no : 대출금 상환함

prop.table( table(credit$default) )

no yes 
0.7 0.3 

30%의 해당하는 사람들이 대출금을 상환하지 않고 있음.
머신러닝 모델을 만들어서 대출금을 상환하지 않는 고객의 비율을 최대한 낮춰
보는 것이 이 머신러닝 모형을 만드는 목적입니다. 

# 2. 데이터 탐색 
str(credit) 

checking_balance : 예금계좌
saving_balance :  적금계좌

amount :  대출금액(250마르크 ~ 18424 마르크)  
             100마르크가 우리나라돈으로 6~7만원 

amount 의 데이터를 히스토그램 그래프로 그리시오 

hist(credit$amount)
summary(credit$amount)
  Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    250    1366    2320    3271    3972   18424 

설명: 최소 250마르크(천칠백 오십만원) 에서 18424 마르크( 12억 8천만원)
        사이로 구성되어있습니다.        

예금계좌에 입금된 돈의 분포를 확인하시오 !

table(credit$checking_balance)

 < 0 DM   > 200 DM    1 - 200 DM    unknown 
     274         63            269              394 

1000개의 고객 계좌중에서 200마르크 이상의 계좌가 63개
아예 계좌가 없는 고객이 274 계좌, 1~200 마르크 사이가 269 명이 있습니다. 


# 3. 훈련과 테스트로 데이터를 분리 (훈련 데이터:9, 테스트 데이터 1)

library(caret)
set.seed(1)   #  어느 자리에서든 동일한 방법으로 훈련과 테스트 데이터를 분리하기 위해서

train_num <- createDataPartition( credit$default , p=0.9, list=F)

train_num   # 1000개의 데이터 중에 90% 해당하는 데이터를 샘플링한 인덱스 번호

train_data <- credit[ train_num,   ]
test_data  <- credit[ -train_num,  ]

nrow(train_data)  #900
nrow(test_data)   #100

# 4. 훈련 데이터로 모델을 생성합니다.
library(C50) # 엔트로피 지수를 이용해서 순수도를 구하고 분류하는 패키지 

# 문법: model <- C5.0 ( 라벨을 뺀 나머지 데이터, 라벨 컬럼 데이터 )
# ncol(train_data)  # 17

model <-  C5.0( train_data[  , -17], train_data[  , 17] )

# 설명: 900 개의 훈련 데이터로 학습한 모델을 생성했습니다.  

summary(model)

# 설명:  checking_balance(예금계좌) 에 200마르크 이상의 돈이 있는 사람들 중에서
  대출금액이 8648 보다 큰  31명의 사람들의 대출금을 상환하지 않았고
  대출금액이 8648 보다 작은 사람들중에 집이 월세인 사람들 26명이 대출금을 상환하지 않았습니다
                                                      집이 자가소유인 8명은 대출금을 상환했습니다. 
                                                      집이 자가 소유이면서 적금계좌에 500마르크가 있는
                                                      사람들중에 적금을 부은 개월수가 16개월 이상이면
                                                      대출금을 상환했고 16개월보다 작으면 대출금을
                                                      상환하지 않았습니다. 
           
45분까지 쉬세요 ~~

# 5. 훈련된 모델을 테스트 데이터를 예측합니다.

문법:  result <- predict( 모델,  라벨을 뺀 테스트 데이터 )

result <- predict( model, test_data[   , -17] )

table(result)

 no  yes 
 81   19 
# 테스트 100명에 대해서 81명은 대출금을 상환했고 19명은 상환하지 않았다고 
# 예측하고 있습니다. 

# 6. 모델의 성능을 평가합니다.
# 문법:  table( 실제값, 예측값)

table( test_data[  , 17],  result )

           no yes
    no    59  11
    yes   22   8

# 100명중에 67명을 정확하게 예측했으므로 정확도를 67% 의 모델입니다.
# 채무이행할거 예측했는데 채무를 불이행한 FN 값이 22명이나 되는 모델이므로 
# 성능개선이 필요합니다. 

library(gmodels)

CrossTable(  test_data[  , 17],  result ) 

# 7. 모델의 성능을 개선합니다. 

의사결정트리의 성능을 높이려면 trials 의 갯수를 조정합니다.
trials 는 의사결정 나무의 갯수를 결정하는 하이퍼 파라미터 입니다. 

model2 <-  C5.0( train_data[  , -17],  train_data[  , 17], trials=100)

result2 <- predict( model2, test_data[  , -17] )

table(test_data[  , 17], result2)

       no yes
  no  60  10
  yes 20  10

trials 를 100 으로 지정했더니 FN 값이 2가 줄었습니다. 

문제259.  trials 를 200, 300, 400, 500 으로 했을때 정확도가 어떻게 달라지는지 실험하세요

Error: number of boosting iterations must be between 1 and 100

의사결정 나무의 갯수는 1 ~ 100까지만 지정할 수 있습니다. 

▩ 위의 모델의 성능을 더 높이려면 ?

                     1. 머신러닝 알고리즘을 다른 알고리즘으로 변경합니다.

                     2. 기존 의사결정 트리를 그대로 사용하고 분류를 해나가는 기준을 변경

                        1. 엔트로피 지수 : 엔트로피지수로 정보획득량을 구해서 의사결정나무를 구성
                        2. 카이제곱 값 : 카이제곱검정으로 정보획득량을 구해서 의사결정나무를 구성
                        3. 지니지수   :  지니지수로 정보획득량을 구해서 의사결정나무를 구성 


▩ 의사결정 패키지 2가지 

 1. C50  : 엔트로피 지수로 정보획득량을 구해서 의사결정나무를 구성
 2. party : 카이제곱 검정으로 정보획득량을 구해서 의사결정나무를 구성 

 C50은 trials 를 이용해서 바로 성능개선을 할 수 있는게 장점이고
 party 는 의사결정트리를 시각화 할 수 있는 장점이 있습니다. 

▩ 독일은행 채무불이행자 예측을 C50 말고 party 패키지를 이용해서 수행 

# 1. party 패키지를 설치합니다.
install.packages("party")
library(party)

# 2. 독일 은행 데이터를 불러옵니다.
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)
nrow(credit)  # 1000

# 3. 훈련 데이터와 테스트 데이터로 분리 합니다.

train_num <- createDataPartition( credit$default , p=0.9, list=F)

train_num   # 1000개의 데이터 중에 90% 해당하는 데이터를 샘플링한 인덱스 번호

train_data <- credit[ train_num,   ]
test_data  <- credit[ -train_num,  ]

nrow(train_data)  #900
nrow(test_data)   #100

# 4. 모델 생성

#C5.0 패키지 문법 :  model <- C50( 라벨을 뺀 데이터, 라벨 데이터 )
#party 패키지 문법 : model <- ctree( 라벨 ~ .  ,  data=훈련 데이터 프레임명 )
                                            

model <- ctree( default ~  .  , data=train_data)

# 5. 모델 예측

result <- predict( model, test_data[  ,  -17] )
table(result)

# 6. 모델 평가
table( test_data[  , 17],  result )

        no yes
  no  66   4
  yes 23   7

73% 의 정확도를 보이는 모델이 생성되었습니다. 

# 7. 모델 시각화 

plot(model)

문제260.  iris 데이터를 분류하는 의사결정 트리 모델을 생성하시오 !  
            ( 책에 나오는 C50 패키지로 수행하세요 ~) 

 데이터셋:  iris2.csv  

# 1. 필요한 패키지를 설치 합니다.
#install.packages("C50")
library(C50)

# 2. 데이터를 로드합니다.
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)
nrow(iris)

# 3. 훈련 데이터와 테스트 데이터로 분리합니다.( 훈련: 9, 테스트 :1 )
set.seed(1)
train_num <- createDataPartition( iris$Species, p=0.9, list=F)

train_data <- iris[ train_num,   ]
test_data <- iris[ -train_num,   ]

# 4. 모델 생성

model <- C5.0(  train_data[   , -5], train_data[  , 5]  )

# 5. 모델 예측
result <-  predict( model,  test_data[  , -5] )

# 6. 모델 평가
table( test_data[  , 5] , result )
   Iris-setosa               5               0              0
  Iris-versicolor           0               5              0
  Iris-virginica            0               0              5

설명: knn 모델에서는 100% 의 정확도가 나오지 않았는데 의사결정트리에서는 100% 의
       정확도가 나왔습니다. 

점심시간문제:   경제적 자유를 위한 데이터 분석:  A반 : 카페에 올리세요
                                                                 B반:  라인검사 입니다. 

▩ R을 활용해서 의사결정트리 모델 구현 :  1.  화장품 고객
                                                       2. 독일 은행 데이터
                                                       3. iris 데이터 

▩ 독버섯 데이터와 판다스를 이용해서 의사결정트리 모델 만들기

#1. 데이터를 로드합니다.
import  pandas  as  pd

mush = pd.read_csv("d:\\data\\mushrooms.csv")
mush.shape  # (8124, 23)
mush.head()

#2. 결측치를 확인합니다.
mush.isnull().sum()

설명: 결측치가 있다면 그 컬럼의 중앙값, 최빈값, 평균값등으로 대치합니다. 

#3. 이상치를 확인합니다.

명목형 데이터이므로 이상치를 확인은 생략합니다.

설명: 이상치가 있다면 이상치를 중앙값, 최빈값, 평균값등으로 치환합니다. 

#4. 명목형 데이터를 숫자로 변경합니다. ( R 과 다른점이 이 부분입니다.)

mush2 = pd.get_dummies( mush.iloc[ : , 1: ] )
mush2.head()
x = mush2.to_numpy()     #  숫자로 변경한 훈련 데이터를 numpy array 로 변경합니다. 
x

y = mush['type'].to_numpy() # 정답도 numpy array 로 변경합니다. 
y

#5. 훈련 데이터와 테스트 데이터를 나눕니다. (8대2)
from  sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test =  train_test_split( x,  y, test_size=0.2, random_state=1 )

print(x_train.shape)  # (6499, 117)
print(x_test.shape)   # (1625, 117)
print(y_train.shape)  # (6499,)
print(y_test.shape)   # (1625,)

#6. 훈련 데이터를 정규화 합니다.

전부 0 과 1이므로 정규화 작업을 생략합니다. 

#7. 테스트 데이터를 정규화 합니다.

전부 0 과 1이므로 정규화 작업을 생략합니다.

#8. 의사결정트리 모델을 생성합니다.
from  sklearn.tree  import   DecisionTreeClassifier

model = DecisionTreeClassifier( criterion='entropy',  max_depth=5)

#설명:   criterion 은 entropy 와 gini 가 있습니다. 
#         max_detph 는 가지의 깊이를 나타냄, 너무 깊으면 훈련 데이터의 정확도는 높은데
#         테스트 데이터의 정확도가 낮아지는 과대적합 현상이 발생합니다. 

#9. 모델을 훈련 시킵니다.

model.fit( x_train, y_train )

#10. 테스트 데이터를 예측합니다.

result = model.predict( x_test )
result

#11. 모델을 평가합니다.

sum(result == y_test) / len(y_test)

1.0

from  sklearn.metrics  import  confusion_matrix

a = confusion_matrix( y_test, result )
print(a)

[[820   0]
 [  0 805]]

#12. 모델의 성능을 개선합니다. 


문제261. 위의 모델을 다시 만드는데 이번에는 entropy 지수가 아니라 gini 지수로 해서
            결과를 보시오 ~ 

 gini 지수 테스트 한 결과 entropy 로 했을때 보다 정확도가 떨어졌습니다. 
 fn 값이 2가 증가했습니다. 

문제262.  iris 데이터로 판다스를 활용한 의사결정트리 모델을 생성하시오 ~

#1. 데이터를 로드합니다.
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")

#2. 결측치를 확인합니다.
iris.isnull().sum()

#3. 이상치를 확인합니다.

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( iris.iloc[ :, 1: ] )

Sepal.Width 4
Petal.Length 0
Petal.Width 0

# Sepal.Width 의 이상치 4개를 Sepal.Width의 평균값으로 치환합니다.

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)])

outlier_value( iris.loc[ :, ['Sepal.Width'] ] )

15    4.4 
32    4.1
33    4.2
60    2.0

mean = iris['Sepal.Width'].mean()  # Sepal.Width의 컬럼의 평균값을 mean 에 담고

iris.loc[ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width'] = mean  # 이상치를 평균값으로 
                                                                                            # 치환
# 이상치가 있는지 다시 확인해봅니다. 
def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( iris.iloc[ :,  :-1 ] )

Sepal.Length 0
Sepal.Width 0
Petal.Length 0
Petal.Width 0

#4. 명목형 데이터를 숫자로 변경합니다.
iris.info()

#5. 훈련 데이터와 테스트 데이터를 나눕니다. 

x = iris.iloc[ : , :-1 ].to_numpy()   # 정답을 뺀 전체 데이터를 numpy arrary 로 변환
y = iris.iloc[ : , -1 ].to_numpy()    # 정답을 numpy  array 로 변환 

from  sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test =  train_test_split( x, y, test_size=0.2, random_state=1)

print(x_train.shape) # (120, 4)
print(x_test.shape)  # (30, 4)
print(y_train.shape) # (120,)
print(y_test.shape)  # (30,)

#6. 훈련 데이터를 정규화 합니다.
from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()   # 정규화 모델생성
scaler.fit(x_train)             # 훈련데이터를 가지고 정규화 계산
x_train2=scaler.transform(x_train)  # 계산된 내용으로 데이터를 변환해서 x_train2 에 담는다.

#7. 테스트 데이터를 정규화합니다. 
from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()   # 정규화 모델생성
scaler.fit(x_test)             # 테스트 데이터를 가지고 정규화 계산
x_test2=scaler.transform(x_test)  # 계산된 내용으로 데이터를 변환해서 x_test2 에 담는다.

#8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier

model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)

#9. 모델 훈련
model.fit( x_train2, y_train)

#10. 모델 예측
result = model.predict( x_test2 )

#11. 모델 평가
sum( y_test == result ) / len(y_test)

0.9333333333333333                                             45분까지 쉬세요 !

from  sklearn.metrics  import  confusion_matrix

a = confusion_matrix( y_test, result )
print(a)

[[11  0  0]
 [ 0 11  2]
 [ 0  0  6]]

모델 만드는 순서 정리

#1. 데이터 로드
#2. 결측치 확인
#3. 이상치 확인 
#4. 명목형 --> 숫자형
#5. 훈련과 테스트로 데이터를 분리 
#6. 훈련 데이터 정규화
#7. 테스트 데이터 정규화
#8. 모델 생성
#9. 모델 훈련
#10. 모델 예측
#11. 모델평가 : 0.933333333

문제263.   위의 결과는 이상치를 평균값으로 치환하고 0.933333 의 정확도가 나왔는데
              이번에는 그냥 이상치를 그냥 그대로 두고 정확도가 어떻게 나오는지 실험하시오 !

#1. 데이터 로드
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")

#2. 결측치 확인
iris.isnull().sum()

#3. 이상치 확인 

#4. 명목형 --> 숫자형

#5. 훈련과 테스트로 데이터를 분리 
x = iris.iloc[ : , :-1 ].to_numpy()   # 정답을 뺀 전체 데이터를 numpy arrary 로 변환
y = iris.iloc[ : , -1 ].to_numpy()    # 정답을 numpy  array 로 변환 

from  sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test =  train_test_split( x, y, test_size=0.2, random_state=1)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#6. 훈련 데이터 정규화
from  sklearn.preprocessing  import  MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train2 = scaler.transform(x_train)

#7. 테스트 데이터 정규화
from  sklearn.preprocessing  import  MinMaxScaler
scaler = MinMaxScaler()   # 정규화 모델생성
scaler.fit(x_test)             # 훈련데이터를 가지고 정규화 계산
x_test2=scaler.transform(x_test)  # 계산된 내용으로 데이터를 변환해서 x_train2 에 담는다.

#8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier
model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)

#9. 모델 훈련
model.fit( x_train2, y_train)

#10. 모델 예측
result = model.predict( x_test2 )

#11. 모델평가 : 이상치 제거했을때의 정확도 : 0.933333333
sum( y_test == result ) / len(y_test)

설명: 0.9의 정확도가 출력되었습니다. 이상치를 제거했을때는 0.9333 이 나왔으므로
       이상치를 제거한 경우가 더 성능이 좋게나왔습니다. 

문제264.  다시 이상치를 제거한 코드로 구현하는데 이번에는 정규화를 할때
            훈련을 할때 계산했던 방법으로 훈련 데이터 뿐만 아니라 테스트 데이터로
            transform 되게 하시오 !
            
#1. 데이터 로드
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")

#2. 결측치 확인
iris.isnull().sum()

#3. 이상치 확인 

# 이상치가 있는 컬럼을 확인합니다. 
def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( iris.iloc[ :,  :-1 ] )

# 이상치가 있는 컬럼의 데이터를 확인합니다. 

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)])

outlier_value( iris.loc[ :, ['Sepal.Width']] )

# 이상치 값을 평균값으로 치환합니다.

mean = iris['Sepal.Width'].mean()  # Sepal.Width의 컬럼의 평균값을 mean 에 담고

iris.loc[ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width'] = mean  # 이상치를 평균값으로 
                                

#4. 명목형 --> 숫자형

#5. 훈련과 테스트로 데이터를 분리 
x = iris.iloc[ : , :-1 ].to_numpy()   # 정답을 뺀 전체 데이터를 numpy arrary 로 변환
y = iris.iloc[ : , -1 ].to_numpy()    # 정답을 numpy  array 로 변환 

from  sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test =  train_test_split( x, y, test_size=0.2, random_state=1)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#6. 훈련 데이터 정규화
from  sklearn.preprocessing  import  MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train2 = scaler.transform(x_train)

#7. 테스트 데이터 정규화

x_test2=scaler.transform(x_test)  # 계산된 내용으로 데이터를 변환해서 x_train2 에 담는다.

#8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier
model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)

#9. 모델 훈련
model.fit( x_train2, y_train)

#10. 모델 예측
result = model.predict( x_test2 )

#11. 모델평가 : 이상치 제거했을때의 정확도 : 0.933333333
sum( y_test == result ) / len(y_test)

0.9666666666666667   

설명: 0.9의 정확도가 출력되었습니다. 이상치를 제거했을때는 0.9333 이 나왔으므로
       이상치를 제거한 경우가 더 성능이 좋게나왔습니다. 

       이번에는 훈련 때 사용했던 정규화 계산방법으로 테스트 데이터를 정규화 했더니
        정확도가 0.9666666666666667 으로 더 향상되었습니다. 

문제265.  위에서 실험했을때는 정규화를 했는데 이번에는 표준화를 해서 테스트 하세요~

정규화:  데이터를 0 ~ 1 사이로 변환
표준화:  평균을 0 으로 두고 평균 중심으로 데이터를 변환한다. (-1 ~ 1 )

정규화:  from   sklearn.preprocessing  import  MinMaxScaler
표준화:  from   sklearn.preprocessing  import  StandardScaler


#1. 데이터 로드
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")

#2. 결측치 확인
iris.isnull().sum()

#3. 이상치 확인 

# 이상치가 있는 컬럼을 확인합니다. 
def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( iris.iloc[ :,  :-1 ] )

# 이상치가 있는 컬럼의 데이터를 확인합니다. 

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)])

outlier_value( iris.loc[ :, ['Sepal.Width']] )

# 이상치 값을 평균값으로 치환합니다.

mean = iris['Sepal.Width'].mean()  # Sepal.Width의 컬럼의 평균값을 mean 에 담고

iris.loc[ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width'] = mean  # 이상치를 평균값으로 
                                

#4. 명목형 --> 숫자형

#5. 훈련과 테스트로 데이터를 분리 
x = iris.iloc[ : , :-1 ].to_numpy()   # 정답을 뺀 전체 데이터를 numpy arrary 로 변환
y = iris.iloc[ : , -1 ].to_numpy()    # 정답을 numpy  array 로 변환 

from  sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test =  train_test_split( x, y, test_size=0.2, random_state=1)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#6. 훈련 데이터 정규화
from  sklearn.preprocessing  import  StandardScaler
scaler = StandardScaler()
scaler.fit(x_train)
x_train2 = scaler.transform(x_train)

#7. 테스트 데이터 정규화

x_test2=scaler.transform(x_test)  # 계산된 내용으로 데이터를 변환해서 x_train2 에 담는다.

#8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier
model =  DecisionTreeClassifier(criterion='entropy', max_depth=5)

#9. 모델 훈련
model.fit( x_train2, y_train)

#10. 모델 예측
result = model.predict( x_test2 )

#11. 모델평가 : 이상치 제거했을때의 정확도 : 0.933333333
sum( y_test == result ) / len(y_test)

0.9666666  # 이상치를 평균값으로 치환했을때 

설명: 정규화로 했을때와 표준화로 했을때의 정확도 차이는 없었습니다.

문제266. 위의 실험에서 이상치를 평균값으로 치환했는데 이번에는 중앙값으로
            치환하고 정확도를 확인하세요~

mean = iris['Sepal.Width'].mean()   # 평균값
median = iris['Sepal.Width'].median()  # 중앙값 
mode = iris['Sepal.Width'].mode()[0]   # 최빈값

[오늘의 마지막 문제] 위의 iris 분류 모델의 의사결정트리의 분류 수학공식을
                             entropy 가 아니라 gini 로 변경하고 결과를 확인하시오!


5시 신호 보냈습니다. 

나머지 시간은 자유롭게 테스트 하고 싶은 내용 테스트  또는 스터디하시면 
됩니다. 

6시 신호 보냈습니다.






내일 시각화

https://zephyrus1111.tistory.com/123


#1. 데이터 로드
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")

#2. 결측치 확인
iris.isnull().sum()

#3. 이상치 확인 

# 이상치가 있는 컬럼을 확인합니다. 
def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( iris.iloc[ :,  :-1 ] )

# 이상치가 있는 컬럼의 데이터를 확인합니다. 

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)])

outlier_value( iris.loc[ :, ['Sepal.Width']] )

# 이상치 값을 평균값으로 치환합니다.

mean = iris['Sepal.Width'].mean()  # Sepal.Width의 컬럼의 평균값을 mean 에 담고

iris.loc[ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width'] = mean  # 이상치를 평균값으로 
                                

#4. 명목형 --> 숫자형

#5. 훈련과 테스트로 데이터를 분리 
x = iris.iloc[ : , :-1 ].to_numpy()   # 정답을 뺀 전체 데이터를 numpy arrary 로 변환
y = iris.iloc[ : , -1 ].to_numpy()    # 정답을 numpy  array 로 변환 

from  sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test =  train_test_split( x, y, test_size=0.2, random_state=1)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

#6. 훈련 데이터 정규화
from  sklearn.preprocessing  import  StandardScaler
scaler = StandardScaler()
scaler.fit(x_train)
x_train2 = scaler.transform(x_train)

#7. 테스트 데이터 정규화

x_test2=scaler.transform(x_test)  # 계산된 내용으로 데이터를 변환해서 x_train2 에 담는다.

#8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier
model =  DecisionTreeClassifier(criterion='gini', max_depth=5)

#9. 모델 훈련
model.fit( x_train2, y_train)

#10. 모델 예측
result = model.predict( x_test2 )

#11. 모델평가 : 이상치 제거했을때의 정확도 : 0.933333333
sum( y_test == result ) / len(y_test)


from sklearn.tree import export_graphviz
from graphviz import Source

export_graphviz(model,                             # 모델
               out_file= "d:\\data\\iris_tree1.dot",  # 저장경로 설정
               feature_names=iris.columns[1:],   # 변수명
               class_names=iris['Species'],         # 종속변수
               rounded = True,
               filled = True)

Source.from_file("d:\\data\\iris_tree1.dot")

▩ R 을 활용해서 의사결정트리 모델 만들기
      1. 화장품 고객
      2. iris 꽃 데이터
      3. 독일은행 데이터

▩ 판다스로 의사결정트리 모델 만들기 
      1. 화장품 고객
      2. iris 꽃 데이터
      3. 독일은행 데이터
      4. 백화점 데이터  

▩ 기계학습 모델을 만드는 순서 

# 1. 데이터를 로드
# 2. 데이터 탐색(결측치 확인)
# 3. 데이터 탐색(이상치 확인)
# 4. 데이터 탐색(명목형 데이터)
# 5. 훈련 데이터와 테스트 데이터를 분리
# 6. 훈련 데이터로 정규화 계산
# 7. 계산된 내용으로 훈련데이터를 변형
      계산된 내용으로 테스트 데이터를 변형
# 8. 모델 생성
# 9. 모델 훈련
# 10. 모델 예측
# 11. 모델 평가
# 12. 모델 개선

▩ 판다스로 독일 은행의 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape  # (1000, 17)

# 2. 데이터 탐색(결측치 확인)
credit.isnull().sum()

# 3. 데이터 탐색(이상치 확인)
credit.info()

def outlier_value(x):
    for i in x.columns[x.dtypes=='int64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( credit )

이상치가 보이는 컬럼들은 일단 두고 모델 생성후에 조정해보기로 합니다.

# 4. 데이터 탐색(명목형 데이터)
credit.info()

credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )
credit2.info()

# 5. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
x
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성
y

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

print ( x_train.shape)  # (900, 44)
print ( x_test.shape)   # (100, 44)
print ( y_train.shape)  # (900, )
print ( y_test.shape)   # (100, )

# 6. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 7. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

print( x_train2.max(),  x_train2.min() )
print( x_test2.max(), x_test2.min()  )

# 8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5)

# 9. 모델 훈련
# 모델명.fit(훈련 데이터, 정답 데이터)

model.fit( x_train2,  y_train)

# 10. 모델 예측

result = model.predict( x_test2 )
result

# 11. 모델 평가

sum( result == y_test ) / len(y_test)

0.76

from  sklearn.metrics  import  confusion_matrix

a = confusion_matrix( y_test, result )
a
array([[58, 12],
        [12, 18]], dtype=int64)

# 12. 모델 개선

1. 의사결정트리 나무의 가지수인 max_depth 를 늘리는 방법
2. 의사결정트리 + 앙상블 기벌 = Random forest 로 모델을 변경합니다. 
3. 이상치를 보이는 컬럼의 데이터를 평균값으로 변경
4.  도메인 지식이 있다면 파생변수를 생성 

문제267.  max_depth 를 5에서 7로 늘리고 정확도를 확인하시오 ~

0.77

array([[60, 10],
       [13, 17]], dtype=int64)

max_depth 를 5 에서 7로 늘렸더니 정확도가 0.02 로 올라갔고 FN 값은 오히려 1 증가했습니다.

문제268.  의사결정트리의 앙상블 기법을 추가한 RandomForest 로 모델을 변경하고 
            실험하시오 !

 앙상블 ?  여러 약한 학습자들을 모아서 하나의 강한 학습자로 만드는 기법 (11장)

기존 모델:  from   sklearn.tree  import  DecisionTreeClassifier
              model = DecisionTreeClassifier(criterion='entropy', max_depth=10)
                                     ↓
변경 모델:  from  sklearn.ensemble   import  RandomForestClassifier 
               model = RandomForestClassifier(random_state=1)

문제269.  와인의 품질을 분류하는 머신러닝 모델을 생성하시오 ! (wine.csv)

# 1. 데이터를 로드
import  pandas  as pd

wine = pd.read_csv("d:\\data\\wine.csv")
wine.head()

# 2. 결측치를 확인
wine.isnull().sum()

# 3. 이상치를 확인

def outlier_value(x):
    for i in x.columns[x.dtypes.isin(['int64', 'float64'])]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( wine )

Magnesium 4
Proline 0

# 4. 명목형 변수 확인
print(wine.info() )

# 5. 훈련 데이터와 테스트 데이터 분리
x = wine.iloc[:, 1:].to_numpy()
y = wine.iloc[: , 0 ].to_numpy()

from sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=1)
                   
# 6. 정규화 진행
from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

x_train2 = scaler.transform(x_train)
x_test2 =  scaler.trainsform(x_test)

# 7. 모델 생성
from sklearn.ensemble  import  RandomForestClassifier

model = RandomForestClassifier()

# 8. 모델 훈련
model.fit( x_train2, y_train)

# 9. 모델 예측
result = model.predict(x_test2)

# 10. 모델 평가
sum(result==y_test) / len(y_test)

# 11. 모델 개선 

▩  롯데 백화점 고객 데이터를 가지고 고객의 성별을 예측하는 머신러닝 모델 만들기 

# 1. 데이터를 로드

import  pandas  as  pd
df_x = pd.read_csv("d:\\data\\X_train.csv", encoding="euckr")
df_y = pd.read_csv("d:\\data\\y_train.csv")

df_x
df_y

# 설명 : 성별: 0 : 여자, 1 : 남자 

# 2. 결측치를 확인

df_x.isnull().sum()

# 설명: 환불금액 컬럼이 전체 3500건중에 2295건이 결측치이므로 컬럼을 삭제합니다.

df_x.drop(['환불금액'], axis=1, inplace=True) 

df_x.isnull().sum()

# 3. 이상치 확인
df_x.info()

def outlier_value(x):
    for i in x.columns[x.dtypes=='int64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 3.0)|(x[i]<Q1-IQR*3.0)].count())
        
outlier_value( df_x )

cust_id 0
총구매액 164
최대구매액 149
내점일수 92
구매주기 57

위에서 보이는 이상치값을 모델 평가할 때 조정해보겠습니다. 

# 4. 명목형 변수 확인
df_x.info()

df_x2 = pd.get_dummies( df_x )
df_x2.head()

# 5. 훈련 데이터와 테스트 데이터 분리
x = df_x2.iloc[ : , 1: ].to_numpy()
y = df_y['gender'].to_numpy()

from  sklearn.model_selection   import  train_test_split

x_train, x_test, y_train, y_test  = train_test_split( x, y, test_size=0.2, random_state=1)
print(x_train.shape) # (2800, 72)
print(y_train.shape) # (2800,)
print(x_test.shape) # (700, 72)
print(y_test.shape) # (700,)                

# 6. 정규화
from  sklearn.preprocessing  import   MinMaxScaler 

scaler.fit(x_train)

x_train2 = scaler.transform(x_train)
x_test2 =  scaler.transform(x_test)

# 7. 모델생성
from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5)

# 8. 모델훈련

model.fit( x_train2, y_train)

# 9. 모델예측

result = model.predict(x_test2)

# 10. 모델 평가

sum( result == y_test )  /  len(y_test)

0.6242857142857143

# 11. 모델 성능 개선 

문제270.  위의 성별 예측 모델을 랜덤포레스트(의사결정트리+앙상블) 로 변경해서 실험하시오

변경전: 
from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5)
                      
변경후:
from   sklearn.ensemble   import  RandomForestClassifier

model = RandomForestClassifier()

0.6442857142857142

※ 모델의 성능을 높이는 방법  ?     1. 이상치를 제거하거나 다른 값으로 치환합니다. 
                                              2.  파생변수를 생성해야 합니다.

       
문제271.  총구매액의 이상치를 평균값으로 치환하고 모델의 정확도를 보는 실험을 하시오!

총구매액 164
최대구매액 149
내점일수 92
구매주기 57

# 3. 이상치 확인

# 총구매액의 이상치를 평균값으로 치환

Q1 = df_x['총구매액'].quantile(0.25)
Q3 = df_x['총구매액'].quantile(0.75)
IQR = Q3 - Q1 
val1 = Q3 + ( IQR * 3 )   # 이상치의 기준 금액을 확인한다.
print(val1)  # 411,790,570.0

mean = df_x.loc[ df_x['총구매액'] < val1, '총구매액'].mean()  # 기준금액보다 작은 데이터의 평균
                                                                               # 값을 구하고 
df_x.loc[ df_x['총구매액'] > val1, '총구매액'] = mean           # 기준금액보다 더 큰 데이터를 
                                                                              # 평균값으로 치환합니다. 

설명:  총구매액과 최대구매액의 이상치를 평균값으로 치환했더니 정확도가 64 --> 62 로 
         떨어졌습니다. 그래서 이상치 처리는 하지 않겠습니다. 

문제272.  파생변수를 추가해서 모델의 정확도를 더 올리시오 !
              남성과 여성을 예측하는것이므로 주구매상품에 남성이 포함되어져 있으면 1
              아니면 0 이 출력되는 파생변수를 추가하고 정확도를 확인하세요~

성별분류 랜덤포레스트 전체코드에서 맨위에 데이터 로드 다음에 아래의 코드를 추가합니다.

df_x['주구매상품'].unique()

mask = df_x['주구매상품'].apply(lambda  x : '남성' in  x )
mask2 = mask.astype(int)
mask2
df_x['man_product'] = mask2
df_x.head()

설명:  위의 파생변수를 추가했더니 아주 살짝 정확도가 올라갔습니다. 

문제273. 타이타닉 데이터의 생존자를 예측하는 분류 모델을 생성하시오 !
           ( 데이터셋 :  titanic.csv )   

# 1. 데이터 로드
tat = pd.read_csv("d:\\data\\tatanic.csv")
tat.head()

# 설명:  survived  가 정답:  1 생존자, 0 사망자

# 필요한 컬럼만 선별합니다.

tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked','class']]
tat2.info()	

#2.  결측치를 확인합니다.
tat2.isnull().sum()

# 나이의 결측치를 나이의 평균값으로 치환하세요 ~
mean = tat2['age'].mean()
tat2['age'].fillna( mean, inplace=True)
tat2.isnull().sum()

# embarked 의 결측치를 최빈값으로 치환하세요 ~
mode = tat2['embarked'].mode()[0]
tat2['embarked'].fillna( mode, inplace=True)
tat2.isnull().sum()

#3.  이상치를 확인합니다.                  

tat2.info()

def outlier_value(x):
    for i in x.columns[x.dtypes.isin(['int64','float64'])]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( tat2)

#4.  명목형 데이터를 확인합니다.

tat3 = pd.get_dummies(tat2)
print(tat3)

#5. 훈련 데이터와 테스트 데이터를 분리 합니다. 
x = tat3.iloc[ : , 1: ].to_numpy()
y = tat3.iloc[ : , 0 ].to_numpy()
x
y

from  sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
print(x_train.shape)  # (712, 13)
print(x_test.shape)  # (179, 13)
print(y_train.shape)  # (712,)
print(y_test.shape) # (179,)

#6. 정규화
from  sklearn.preprocessing  import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

#7.   모델 생성
from  sklearn.ensemble  import  RandomForestClassifier

model = RandomForestClassifier()

#8.   모델 훈련
model.fit(x_train2, y_train)

#9.   모델 예측
result = model.predict(x_test2)

#10.   모델 평가

sum(result == y_test) / len(y_test)

0.7932960893854749


문제274. 위의 타이타닉 생존자 예측 기계학습 모델의 성능을 더 올리시오 
            ( 여자와 아이먼저 ~ )  

0.793 

 여자이거나 아이면 1 아니면 0을 출력하는 파생변수를 women_child 라는 
  이름으로 추가하시오 !      아이의 나이의 기준을 10살보다 작으면 아이입니다

# 필요한 컬럼만 선별합니다.

tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked','class']

mask = ( tat2['sex']=='female') | ( tat2['age'] <10 )
mask
tat2['women_child'] = mask.apply(int)
tat2

전체코드:

# 1. 데이터 로드
tat = pd.read_csv("d:\\data\\tatanic.csv")
tat.head()

# 설명:  survived  가 정답:  1 생존자, 0 사망자

# 필요한 컬럼만 선별합니다.

tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked','class']]
tat2.info()	


#파생변수를 추가합니다.

mask = ( tat2['sex']=='female') | ( tat2['age'] <10 )
mask
tat2['women_child'] = mask.apply(int)

#2.  결측치를 확인합니다.
tat2.isnull().sum()

# 나이의 결측치를 나이의 평균값으로 치환하세요 ~
mean = tat2['age'].mean()
tat2['age'].fillna( mean, inplace=True)
tat2.isnull().sum()

# embarked 의 결측치를 최빈값으로 치환하세요 ~
mode = tat2['embarked'].mode()[0]
tat2['embarked'].fillna( mode, inplace=True)
tat2.isnull().sum()

#3.  이상치를 확인합니다.                  

tat2.info()

def outlier_value(x):
    for i in x.columns[x.dtypes.isin(['int64','float64'])]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( tat2)

#4.  명목형 데이터를 확인합니다.

tat3 = pd.get_dummies(tat2)
print(tat3)

#5. 훈련 데이터와 테스트 데이터를 분리 합니다. 
x = tat3.iloc[ : , 1: ].to_numpy()
y = tat3.iloc[ : , 0 ].to_numpy()
x
y

from  sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
print(x_train.shape)  # (712, 13)
print(x_test.shape)  # (179, 13)
print(y_train.shape)  # (712,)
print(y_test.shape) # (179,)

#6. 정규화
from  sklearn.preprocessing  import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

#7.   모델 생성
from  sklearn.ensemble  import  RandomForestClassifier

model = RandomForestClassifier()

#8.   모델 훈련
model.fit(x_train2, y_train)

#9.   모델 예측
result = model.predict(x_test2)

#10.   모델 평가

sum(result == y_test) / len(y_test)

문제274.  위의 머신러닝 모델을  의사결정트리 모델의 정확도는 어떻데 되는지 확인하시오!

from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5,random_state=1))

0.8100558659217877

답:

# 1. 데이터 로드
tat = pd.read_csv("d:\\data\\tatanic.csv")
tat.head()

# 설명:  survived  가 정답:  1 생존자, 0 사망자

# 필요한 컬럼만 선별합니다.

tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked','class']]
#tat2.info()

#파생변수를 추가합니다.

mask = ( tat2['sex']=='female') | ( tat2['age'] < 12 )
#mask
tat2['women_child'] = mask.apply(int)

#2.  결측치를 확인합니다.
#tat2.isnull().sum()

# 나이의 결측치를 나이의 평균값으로 치환하세요 ~
mean = tat2['age'].mean()
tat2['age'].fillna( mean, inplace=True)


# embarked 의 결측치를 최빈값으로 치환하세요 ~
mode = tat2['embarked'].mode()[0]
tat2['embarked'].fillna( mode, inplace=True)


#3.  이상치를 확인합니다.    
              
def outlier_value(x):
    for i in x.columns[(x.dtypes == 'float64') | (x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( tat2)


#4.  명목형 데이터를 확인합니다.

tat3 = pd.get_dummies(tat2)


#5. 훈련 데이터와 테스트 데이터를 분리 합니다. 
x = tat3.iloc[ : , 1: ].to_numpy()
y = tat3.iloc[ : , 0 ].to_numpy()

from  sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
#print(x_train.shape)  # (712, 13)
#print(x_test.shape)  # (179, 13)
#print(y_train.shape)  # (712,)
#print(y_test.shape) # (179,)

#6. 정규화
from  sklearn.preprocessing  import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

#7.   모델 생성
from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=1)

#8.   모델 훈련
model.fit(x_train2, y_train)

#9.   모델 예측
result = model.predict(x_test2)

#10.   모델 평가

sum(result == y_test) / len(y_test)

문제275.  위의 기계학습 모델을 naivebayes 모델로 변경하고 정확도를 확인하시오 !

from  sklearn.naive_bayes  import  GaussianNB

model = GaussianNB(var_smoothing=0.04) 

랜덤포레스트 : 0.79
의사결정트리 : 0.81 
나이브베이즈 : 0.77              

문제276. ( 오늘의 마지막 문제) 위의 머신러닝 모델의 knn 정확도까지 완성시키시오

랜덤포레스트 : 0.79
의사결정트리 : 0.81 
나이브베이즈 : 0.77   
knn 모델   :  ?  

■ R 가 파이썬을 활용해서 머신러닝 모델을 구현 

1. 파이썬 문법과 판다스 문법
2. 1장. R 기본문법
3. 2장. knn 으로 분류 모델 만들기
4. 3장. naivebayes 로 분류 모델 만들기
5. 4장. Decision Tree 로 분류 모델 만들기 with RandomForest 

 데이터:   1.  유방암 데이터
             2.  독버섯 데이터
             3.  독일 은행 데이터 
             4.  iris 데이터
             5.  wine 데이터
             6.  titanic 데이터 

■ 5장.  oneR 알고리즘 (p 227)

▩ 규칙 기반 알고리즘:  1. oneR 알고리즘
                               2. Riper 알고리즘 

▩  1R 알고리즘 

    하나의 사실(조건)만 가지고 간단하게 데이터를 분류하는 알고리즘 입니다.
    하나의 사실만 가지고 분류를 하다 보니 간단하지만 오류가 많습니다. 

예: 가슴통증의 유무에 따라 심장질환이 있는지 분류하고자 하면
    가슴통증 하나만 보고 심장질환이 있다고 분류하기에는 오류가 많이집니다.
    왜냐하면 식도염, 폐질환도 가슴통증이기 때문입니다.

▩ Riper 알고리즘 

  " 복수개의 사실(조건)을 가지고 분류하는 알고리즘 입니다."

예: 가슴통증이 있으면서 호흡곤란이 있으면 심장질환이다. 



알고리즘이 데이터를 보고 패턴을 발견합니다. (조건을 발견합니다.)

예: 독버섯 분류 조건을 발견한 알고리즘 p 245

▩ 독버섯 데이터와 식용버섯 데이터를 oneR 알고리즘으로 분류하는 실습(p237)

1. 버섯 데이터를 로드한다. 
mush <- read.csv("mushrooms.csv", stringsAsFactors=T)
head(mush)
데이터 설명: UCI 머신러닝 저장소에서 제공하는 데이터이며 23종의 버섯과
                 8124개의 버섯샘플에 대한 정보가 포함되어있습니다.
                 버섯샘플 22개의 특징은 갓모양, 갓색깔, 냄새, 주름크기, 주름색,
                 줄기모양, 서식지와 같은 특징이 있습니다. 

2. 결측치를 확인합니다.
colSums(is.na(mush))

3. 이상치를 확인합니다.
# 전부 명목형 데이터 입니다. 

4. 훈련 데이터와 테스트 데이터로 나눕니다.
set.seed(1)
library(caret)
train_num <-  createDataPartition( mush$type, p=0.8, list=F)
train_data <- mush[ train_num,  ] #훈련 데이터 80%
test_data <- mush[ -train_num,  ] #테스트 데이터 20%
nrow(train_data) # 6500
nrow(test_data)  # 1624 

5. 정규화 작업 수행
# 명목형 데이터 이므로 정규화 작업을 할 필요가 없습니다. 

6. 훈련 데이터로 모델을 훈련시킵니다.(oneR 알고리즘)

install.packages("OneR")
library(OneR)         #  한가지 조건만 가지고 분류하는 알고리즘 
                         # 장점: 간단하다.  단점: 정확하게 분류하지 못한다.

model <- OneR( type ~ . , data = train_data )
# 문법: model <- oneR(정답컬럼 ~ 모든컬럼,  data= 훈련 데이터프레임명)

model     #  알고리즘이 데이터에서 발견한 패턴을 확인할 수 있습니다. 

버섯 냄새 한가지만 가지고 다음과 같이 분류했습니다. 
Rules:
If odor = almond   then type = edible
If odor = anise    then type = edible
If odor = creosote then type = poisonous
If odor = fishy    then type = poisonous
If odor = foul     then type = poisonous
If odor = musty    then type = poisonous
If odor = none     then type = edible
If odor = pungent  then type = poisonous
If odor = spicy    then type = poisonous

summary(model)

p-value < 2.2e-16

귀무가설:  냄새로 독버섯과 정상버섯을 분류할 수 없다
대립가설:  냄새로 독버섯과 정상버섯을 분류할 수 있다.

p value 값이 2.2e-16 작으므로 대립가설을 채택할 충분한 근거가 있다

7. 훈련된 모델에 테스트 데이터를 넣어서 예측합니다.

result <- predict(  model,  test_data[  , -1] )
result

8. 모델을 평가합니다. 

sum(test_data[  , 1] ==result) / nrow(test_data)

0.9858374

9. 이원교차표를 확인해서 FN 값이 몇개가 있는지 확인합니다. 

library(gmodels)

CrossTable( test_data[  , 1] , result )

설명: 정확도는 98% 이나 FN 값이 23개가 나와서 FN 값을 줄일 수 있도록
       개선할 필요가 있습니다. 

▩ oneR 알고리즘이 어떻게 냄새로 버섯을 분류할려고 했는가 ?

  냄새가 다른 컬럼(변수)들 보다 분류를 하는데 있어서 더 중요한 컬럼이었나?

예제1. 버섯 데이터의 컬럼들의 정보획득량을 출력하세요 ~

library(FSelector)

w1 <- information.gain( type ~ .,  mush,  unit="log2")
w1
str(w1)  # 데이터 프레임 입니다.

예제2.  orderBy 함수를 이용해서 정보획득량이 높은것부터 출력되게하시오 !

library(doBy)
orderBy( ~ -attr_importance, w1 )

설명: order 가 독버섯과 식용버섯을 분류하는데 가장 정보를 많이주는
       컬럼입니다. 그래서 oneR 알고리즘이 냄새(order) 하나만 가지고
       분류를 한 것 입니다. 

▩ 규칙 기반 알고리즘 중 하나인 Riper 알고리즘으로 독버섯 분류하기 (p243)

# 1. 데이터를 로드합니다.
mush <- read.csv("mushrooms.csv", stringsAsFactors=T)

# 2. 결측치가 있는지 확인합니다.
colSums(is.na(mush))

# 3. 훈련데이터와 테스트 데이터로 분리합니다.
library(caret)
set.seed(1)
train_num <-  createDataPartition( mush$type, p=0.8, list=F)
train_data <- mush[ train_num,  ] #훈련 데이터 80%
test_data <- mush[ -train_num,  ] #테스트 데이터 20%
nrow(train_data) # 6500
nrow(test_data)  # 1624 

# 4. 훈련 데이터로 모델을 훈련시킵니다.
install.packages("RWeka")
library(RWeka)

model2 <- JRip( type ~ . , data=train_data)

model2   #  p 245 페이지에 아래의 내용에 대한 해석이 나옵니다. 

(odor = foul) => type=poisonous (1732.0/0.0)
(gill_size = narrow) and (gill_color = buff) => type=poisonous (921.0/0.0)
(gill_size = narrow) and (odor = pungent) => type=poisonous (205.0/0.0)
(odor = creosote) => type=poisonous (155.0/0.0)
(spore_print_color = green) => type=poisonous (52.0/0.0)
(stalk_surface_below_ring = scaly) and (stalk_surface_above_ring = silky) => type=poisonous (58.0/0.0)
(habitat = leaves) and (cap_color = white) => type=poisonous (7.0/0.0)
(stalk_color_above_ring = yellow) => type=poisonous (3.0/0.0)
 => type=edible (3367.0/0.0)

summary(model2)
    a    b   <-- classified as
 3367    0 |    a = edible
    0 3133 |    b = poisonous

설명: 위의 작은 이원교차표에서 훈련 데이터에 대해서 100% 의 정확도를 
        보여주는 결과가 나오고 있습니다. 

# 5. 훈련된 모델에 테스트 데이터를 넣어서 예측을 합니다.

result2 <- predict( model2,  test_data[  , -1] )

# 6. 모델을 평가합니다. 

sum( result2 == test_data[  , 1] ) / nrow(test_data) 

설명:  나이브 베이즈 모델  :     ? 
         oneR 알고리즘      :   98 %
         riper 알고리즘      :  100%

요청하신 머신러닝 데이터 분석의 결과로 선택된 모델은 riper 규칙기반 알고리즘입니다.
위의 표와 같이 3개의 머신러닝 모델에서 가장 완벽한 정확도를 보이는 모델을 선택했습니다.


문제277. 명목형 데이터만 있는 독버섯 데이터에서는 riper 분류 알고리즘이 놀라운 성능을
            보였습니다. 이번에는 수치형 데이터만 있는  iris 데이터를  riper 알고리즘으로
           분류했을때도 성능이 잘 나오는지 확인하세요 !

#1. 데이터 로드

iris <- read.csv("iris2.csv", stringsAsFactors=T)

#2. 결측치 확인
colSums(is.na(iris))

#3. 이상치 확인

library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(iris)  # wbcd 의 컬럼명 확인 

for  ( i  in  1 :4  ) {
  
  a <- grubbs.flag(iris[  ,  colnames(iris)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(iris)[i], ' --->', length(b) )  )
  
}

#4. 훈련과 테스트 분리
library(caret)
set.seed(1)
train_num <- createDataPartition( iris$Species, p=0.9, list=F)
train_data <- iris[ train_num,  ]
test_data  <- iris[ -train_num,  ]

#5. 훈련된 데이터로 모델 생성

library(RWeka)

model3 <- JRip( Species~ . , data=train_data)

model3  

#6. 훈련된 모델에 테스트 데이터를 입력해서 예측

result2 <- predict( model3,  test_data[  , -5] )

# 7. 모델을 평가합니다. 

sum( result2 == test_data[  , 5] ) / nrow(test_data) 

100% 의 정확도의 모델이 만들어졌습니다.  ( 훈련 90%, 테스트 10%)

점심시간 문제:  버블정렬을 파이썬으로 구현하세요

R 로 분류한 데이터 :  버섯 데이터 ---> 정확도 100% 
                              iris           ----> 정확도 100%

문제278.  wine 데이터를 Riper 알고리즘으로 분류하는 머신러닝 모델을 만드시오 !

# 1. 데이터 로드

wine <- read.csv("wine.csv", stringsAsFactor=T)

# 2. 결측치 확인

colSums(is.na(wine))

# 3. 이상치 확인

library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

for  ( i  in  2 : 14  ) {
  
  a <- grubbs.flag(wine[  ,  colnames(wine)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(wine)[i], ' --->', length(b) )  )
  
}

[1] "Alcohol  ---> 0"
[1] "Malic  ---> 0"
[1] "Ash  ---> 1"
[1] "Alcalinity  ---> 0"
[1] "Magnesium  ---> 2"
[1] "Phenols  ---> 0"
[1] "Flavanoids  ---> 0"
[1] "Nonflavanoids  ---> 0"
[1] "Proanthocyanins  ---> 1"
[1] "Color  ---> 1"
[1] "Hue  ---> 0"
[1] "Dilution  ---> 0"
[1] "Proline  ---> 0"

# 4. 훈련 데이터와 테스트 데이터 분리
library(caret)
set.seed(1)
train_num <-  createDataPartition( wine$Type, p=0.9, list=F)

train_data<- wine[ train_num,   ]
test_data <- wine[ -train_num,  ]

nrow(train_data)  # 162
nrow(test_data)   # 16

# 5. 훈련 데이터로 모델 생성
library(RWeka)

model <- JRip( Type ~ . , data=train_data)
summary(model)

# 6. 테스트 데이터 예측
result <- predict( model, test_data[  ,  -1] )

# 7. 모델평가
sum( result == test_data[  , 1])  / nrow(test_data)

[1] 0.9375


▩ Riper 알고리즘을 파이썬으로 구현하기 (독버섯 )  

R 에서의 Riper 패키지는 이진 분류, 다중 분류 다 분류가 가능한 패키지 입니다.

파이썬의 Riper 패키지는  이진 분류만 가능합니다. 

실습하기에 앞서서 먼저 아나콘다 프롬프트창에서 아래의 패키지를 설치합니다.

pip  install wittgenstein

# 1. 데이터 로드
import  pandas  as  pd
mush = pd.read_csv("d:\\data\\mushrooms.csv")

# 2. 훈련 데이터와 테스트 데이터 분리
x = mush.iloc[ : , 1: ]  # 라벨을 제외한 데이터
y = mush.iloc[ : , 0 ]   # 라벨

# label encoder 를 사용해서 정답 컬럼을 0 과 1 로 변경합니다. 

파이썬 Riper 패키지가 라베을 0 과 1 또는 False 또는 True 로 되어있는 데이터만 받기때문에
label encoder 로 변환해줍니다.

from  sklearn.preprocessing  import  LabelEncoder

encoder = LabelEncoder()
encoder.fit(y)
y2 = encoder.transform(y)

# 관심범주 확인하는 코드 

print(encoder.classes_) 

['edible' 'poisonous']
    0           1  

x2 = x.to_numpy()

# y2 는 이미 numpy array 이므로 변환하지 않습니다. 
from  sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( x2, y2, test_size=0.2, random_state=1)

print( x_train.shape)  # (6499, 22)
print( x_test.shape)   # (1625, 22)
print( y_train.shape)  # (6499,)
print( y_test.shape )  # (1625,)

# 3. 모델 생성
import wittgenstein as lw

model = lw.RIPPER()

# 4. 모델 훈련

model.fit( x_train, y_train)

# 5. 모델 예측
result = model.predict( x_test)
result

# 6. 모델 평가

sum(result==y_test) /len(y_test)

1.0

문제279. 독일 은행 데이터에서 채무 불이행자를 예측하는 분류 모델을 Riper 알고리즘으로
            만드시오 ( 데이터셋 : credit.csv )

#1. 데이터 로드
import  pandas  as  pd

credit = pd.read_csv("d:\\data\\credit.csv")

#2. 명목형 데이터를 숫자로 변경합니다.
credit2 = pd.get_dummies(credit.iloc[ : , 0:-1])
credit2.head()

#3. 데이터 분리 
x = credit2.to_numpy()
y = credit['default']

#4. 정답 컬럼을 label encoder 로 0 또는 1로 변경
from  sklearn.preprocessing  import  LabelEncoder

encoder = LabelEncoder()
encoder.fit(y)
y2 = encoder.transform(y)

#5. 훈련 데이터와 테스트 데이터를 분리합니다. 
from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test  =  train_test_split( x, y2, test_size=0.1, random_state=1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# 관심범주 확인하는 코드 

print(encoder.classes_) 
# ['no' 'yes']

#6. 모델 생성
import wittgenstein as lw

model = lw.RIPPER(random_state=5)

#7. 모델 훈련 
model.fit(x_train, y_train)

#7. 모델 예측
result = model.predict(x_test)

#8. 모델 평가 
sum( result==y_test) / len(y_test)

4시가 되면 신호보내고서 시험 시작하겠습니다.

시험명:  능력단위 프로그래밍언어 활용 ncs 문제

시험문제갯수 : 8개

시험 시간 : 1시간 30분

답에 저번처럼 코딩을 한 결과 화면을 캡쳐해서 붙여넣으세요.

시험 다 보면 제출하고 자유롭게 자습하시면 됩니다.

http://itwill.atosoft.net/worknet/SLogin.asp


■ 6장. 회귀분석 

  회귀분석은 하나의 변수가 나머지 다른 변수들과의 선형관계를 갖는가의 여부를 분석하는 
  방법으로 하나의 종속변수(예측하고자 하는 값)와 독립변수 사이의 관계를 명시하는것을 
  말합니다. 

예:  집값에 가장 영향을 주는 요소가 무엇인가 ?

 - 독립변수 :  종속변수에 영향을 주는 변수(평수, 역세권, 학군,.......)
 - 종속변수 :  서로 관계를 가지고 있는 변수들중에서 다른 변수에 영향을 받는 변수(집값)

    집값

■ 최소 제곱 추정법( p257)

최적의 a(기울기) 와 b(절편) 을 결정하기 위해서는 최소제곱으로 알려진 추정기법을 사용합니다. 
실제값과 예측값 사이의 수직 직선인 오차(잔차)를 제곱해서 구한 총합을 알아야 합니다. 

방사능 수치
                                                             평수 
                                      






                                                        시간

예제1: 어느 실험실에서 10시간, 20시간, 30시간, 40시간 마다 물질의 방사능 수치를 
         측정한 자료가 있을때 35시간에 물질의 방사능 수치는 얼마로 예측되는가?
        ( x 축 시간, y 축: 방사능 수치)

x = c(10, 20, 30, 40)
y = c(300, 250, 200, 150)


a =  cov(x,y) / var(x)
print(a)  # 기울기 -5



b = mean(y) + 5 * mean(x)
print(b)  # 350

y = -5*35 + 350 

문제280. 위의 단순회귀 직선의 방정식을 구해서 y 값을 예측하는 함수를 다음과 같이 생성하시오

simple_regression(35)    # 175 

simple_regression <- function ( x_num ) {
                                               x = c(10, 20, 30, 40)
                                               y = c(300, 250, 200, 150)
                                               a =  cov(x,y) / var(x)         # p259  기울기
                                               b = mean(y) - a * mean(x) # p258  절편
                                               y_hat = a*x_num + b 
                                               print(y_hat)
                                               }

simple_regression(35)   

문제281.  탄닌 함유량과 애벌래의 성장간의 실험표를 이용해서 탄닌 함유량이 9일 때 성장률이 
             어떻게 되는지 알아내는 함수를 다음과 같이 생성하시오! ( 데이터: regression.txt)

reg <- read.table("regression.txt", header=T)
reg
reg_func(9)  #   ?

reg_func <- function ( x_num ) {
  x = reg$tannin
  y = reg$growth
  a =  cov(x,y) / var(x)         # p259  기울기
  b = mean(y) - a * mean(x) # p258  절편
  y_hat = a*x_num + b 
  print(y_hat)
}

reg_func(9)    # 0.8055556

문제282.  위의 함수를 파이썬으로 구현하시오 ! (카페에 답글로 달아주세요)

reg_func(9)  # 0.8055556

import pandas as pd
reg=pd.read_csv("d:\\data\\regression.txt", sep = "\t")
reg

import numpy as np

data=pd.read_table('d:\\data\\regression.txt')

def reg_func(x_num):
    x=data['tannin']
    y=data['growth']
    a=x.cov(y)/x.var()
    b=np.mean(y)-(a*np.mean(x))
    return a*x_num+b

reg_func(9)

def reg_func(num):
    x = reg.tannin
    y = reg.growth
    a = np.cov(x,y)[0,1]/x.var()  # 정승재 코드 
    b = np.mean(y) - a*np.mean(x)

    res = a*num+b
    return res

reg_func(9)

▩ R 함수 lm 을 이용해서 단순회귀분석 실습 1

 " 탄닌 함유량과 애벌래 성장간의 관계에 대한 회귀식을 도출하기"

1. 데이터를 로드합니다.
reg <- read.table("regression.txt", header=T)
reg

2. 데이터를 산포도 그래프로 시각화 합니다.
attach(reg)
plot( growth ~ tannin,  data=reg, pch=21, col='blue', bg='blue')

※ 설명:  plot( y ~ x, data= 데이터프레임명)

3. 회귀분석을 해서 회귀계수인 기울기와 절편을 구합니다.

model <- lm( growth ~ tannin, data=reg )
model 

Call:
lm(formula = growth ~ tannin, data = reg)

Coefficients:
(Intercept)       tannin  
     11.756       -1.217 
       ↑              ↑
     절편           기울기

4. 2번에서 시각화한 산포도 그래프에 회귀직선을 겹쳐서 그립니다.

attach(reg)
plot( growth ~ tannin,  data=reg, pch=21, col='blue', bg='blue')  # 산포도 그래프 
model <- lm( growth ~ tannin, data=reg )                              # 회귀 모델 생성 
abline( model,  col='red')                                                    # 회귀모델의 직선의 그래프

5. 그래프 제목을 회귀직선의 방정식으로 출력되게 합니다. 

model$coefficients[2]  # 기울기
model$coefficients[1]  # 절편

title(  paste( '성장률=', model$coefficients[2], ' x  탄닌 + ', model$coefficients[1] )  )

문제283.  위의 단순회귀 직선 그래프 그리는 코드를 참고해서 광고비가 매출에 미치는 영향조사를
              위한 회귀분석 그래프를 그리시오 !  ( 데이터: simple_hg.csv )

cost : 광고비
input : 매출액 

45분까지 쉬세요 ~~~

그래프 또는 코드를 답글로 올려주세요 ~~


# 1. 데이터로드
shg <- read.csv("simple_hg.csv")

# 2. 데이터를 산포도 그래프로 시각화
attach(shg)
plot(input~cost,data=reg,pch=21,col='blue',bg='blue')

# 3. 회귀분석을 해서 회귀계수인 기울기와 절편을 구하기
model <- lm(input~cost,data=reg)
model

# 4. 2번에서 시각화한 산포도 그래프에 회귀직선을 겹쳐서 그림
abline(model,col='red')

# 5. 그래프 제목을 회귀직선의 방정식으로 출력
model$coefficients[2] # 기울기
model$coefficients[1] # 절편

title(paste('매출 =',model$coefficients[2],'x 광고비 +',model$coefficients[1]))

# 6.  오차 그리는 코드 

y_hat <- predict( model,  cost= cost )  # input 매출액 예측값 출력
y_hat    #  예측값 출력 

join <- function(i)  {                        # join 이라는 이름의 함수를 생성

lines( c( cost[i], cost[i] ),  c( input[i], y_hat[i] ), col='green')  # 녹색 라인 그래프

                           }

sapply( 1:19, join )  # 1부터 19를 join 함수에 입력한다. 

12시 신호 보냈습니다. 

※  오차와 잔차와의 차이 ?

 1. 오차 :  모집단에서 실제값이 회귀선과 비교했을때의 차이 (실제값과 예측값과의 차이)

 2. 잔차 :  표본에서 실제값이 회귀선과 비교했을때의 차이(실제값과 예측값의 차이)

문제284.  책의 데이터 예제인 우주 왕복선 챌린저호의 폭파원인을 분석하기 위해서 
              y 축을 o형링 파손수로 두고 x 축을 온도로 두어서 산포도 그래프와 회귀 직선을
             겹쳐서 그리시오 ! ( 데이터 :  challenger.csv , 데이터 게시판 286 번)

데이터 소개 :  distress_ct : o 형링 파손수
                   temperature :  온도
                   field_check_pressure  : 압력
                   flight_num : 비행기 번호



# 1. 데이터로드
cha<- read.csv("challenger.csv")

# 2. 데이터를 산포도 그래프로 시각화
attach(cha)
plot(distress_ct~temperature,data=cha,pch=21,col='blue',bg='blue')

# 3. 회귀분석을 해서 회귀계수인 기울기와 절편을 구하기
model <- lm(distress_ct~temperature,data=cha)
model

# 4. 2번에서 시각화한 산포도 그래프에 회귀직선을 겹쳐서 그림
abline(model,col='red')

# 5. 그래프 제목을 회귀직선의 방정식으로 출력
model$coefficients[2] # 기울기
model$coefficients[1] # 절편

title(paste('o형링 파손수 =',model$coefficients[2],'x 온도 +',model$coefficients[1]))

즐거운 점심식사 되세요 ~~~   점심시간 중으로 1안,2안 뎃글 달아주세요~~ 다수결로 정하겠습니


단순 회귀분석 ---------->  상관관계 ----------------------->  다중회귀분석
                                       ↓
                                   다중공선성 


▩ 다중 공선성 ( variance  inflation  factor )

 회귀분석에서 사용된 모형의 일부  설명변수(독립변수) 가 다른 독립변수와의 상관정도가 높아
 데이터 분석시 부정적인 영향을 미치는 현상을 말합니다.

 두 독립변수들끼리 서로에게 영향을 주고 있다면 둘 중 하나의 영향력을 검증할 때
 다른 하나의 영향력을 완전히 통제할 수 없게 됩니다.

예:   아파트 가격,  평수, 역과의 거리 


      종속변수       평수와 역과의 거리는 상관관계가 강하지 않아서 회귀분석 결과에 큰 
                        영향을 미치지는 않습니다. 


예:  학업성취도,  일평균 음주량,  혈중 알코올 농도
        

    종속변수       일평균 음주량과 혈중 알코올 농도는 서로 상관관계가 아주 높다.


  음주가 학업성취도에 미치는 영향을 알아보려고 회귀분석을 하려고 한다.
  일평균 음주량과 혈중 알코올 농도는 서로 아주 강한 상관정도를 보인다. 

  실제로 x1 과 x2 의 값이 증가 또는 감소할수록 y 값이 증가 또는 감소할 것인데
  이중 하나는 굉장히 불안정한 계수값을 보이게 된다.

  공선성은 두개의 독립변수들 간의 관계를 의미하는데 
  예를들어 두개의 독립변수들 간의 상관관계 계수가 1이면 완전한 공선성을 보인다고 하고,
   계수가 0 이면 공선성이 없음을 의미합니다. 
  특히 3개 이상의 변수들간의 관계를 다중 공선성이라한다.
  한 독립변수가 종속변수에 대한 설명력이 높더라도 (다중) 공선성이 높으면
 설명력이 낮은것 처럼 나타납니다.

  다중 공선성을 알아보기 위한 가장 간단한 방법은 독립변수들간의 상관관계를 조사하는것
  입니다.  독립변수들 간의 높은 상관관계(일반적으로 0.9 이상) 은 공선성을 판단하는 지표이다 

  공선성을 보다 엄격하게 점검하려면 팽창계수(vif) 를 확인하면 됩니다.

현업기준 :  팽창계수(vif) 가 보통 10보다 큰것을 골라내고 
                엄격하게 하려면 5보다 큰것을 골라냅니다.
                느슨하게 하려면 15또는 20으로 주로 골라냅니다. 

▩ 다중 공선성 확인 실습

install.packages("car")
library(car)

data(Boston, package="MASS")

Boston

model  <-  lm( medv ~  . ,  data=Boston)

vif(model) > 10  # 다중 공선성을 보이는 변수들 확인

전부 false 여서 다중 공선성을 보이는 컬럼들은 없습니다.  집값에 영향을 미치는 독립변수들
중에서 서로 상관관계가 높게 나타나는 독립변수들이 없다는 의미입니다. 

▩ 상관관계 데이터 분석 p260

 두 변수 간의 상관관계는 변수들의 관계가 직선에 가깝게 따르는 정도를 나타내는 숫자입니다.
 상관관계는 -1 에서 +1 사이의 범위에 있습니다. 
 
 -1 에 가깝거나 1에 가까우면 완벽한 선형관계를 나타내는 반면, 0 에 가까운 상관관계는
 선형관계가 없음을 나타냅니다.

p260  피어슨 상관계수 구하는  공식 :  

       두 변수의  공분산을 표준편차의 곱으로 나눈값으로 상관계수를 구합니다.





예제1.  우주왕복선 챌린저호의 온도와 O형링 파손수간의 상관계수를 구하시오 !

cha <- read.csv("challenger.csv", header=T)

cor(  cha$temperature,  cha$distress_ct )

 -0.5111264

설명: 피어슨 상관계수가 -0.51 이므로 음의 상관관계를 보이고 있습니다. 
        온도와 오형링 손상간의 상대적인 강도가 -0.51 로 최대값인 -1 의 절반정도이기 때문에
        적당히 강한 음의 선형관계가 있음을 의미합니다. 

문제285.  챌린저호의 O 형링 파손과 상관관계가 높은 컬럼은 아래의 3개중에 어떤것인지
             확인하시오 !

 1. 온도
 2. 압력
 3. 비행기 번호( 비행기의 노후화와 연관이 있는 번호)

답 :  cor( cha )

> cor( cha )
                      distress_ct temperature field_check_pressure flight_num
distress_ct               1.0000000 -0.51112639           0.28466627  0.1735779
temperature            -0.5111264  1.00000000           0.03981769  0.2307702
field_check_pressure   0.2846663  0.03981769           1.00000000  0.8399324
flight_num               0.1735779  0.23077017           0.83993237  1.0000000

문제286. 위의 상관관계를 R 로 시각화해서 출력하시오 !

install.packages("psych")
library(psych)
pairs.panels( cha )

※ 상관계수의 종류 2가지 

1. 피어슨 상관계수  :  등간척도나 비례척도의 데이터에서 두 변수의 공분산을 
                             두변수의 표준편차의 곱으로 나눈값입니다.

                            두 변수간의 선형관계의 크기를 측정하는 값으로 비선형적인 
                            상관관계는 나타내지 못합니다. 

2. 스피어만 상관계수 :  두 변수간의 비선형적인 관계도 나타낼 수 있는 값입니다.

                               두 변수를 모두 순위로 변환시키고 두 순위 사이의 스피이어만
                               상관계수를 구합니다. 

문제287.  삼성전자와 현대 자동차 둘 중에 코스피 등락율과 더 상관관계가 높은 주식이
             어떤것인지 알아내시오 !

데이터 :  K_index.csv,   S_stock.csv,  H_stock.csv 
           (코스피등락율)  (삼성전자)  (현대 자동차)

k <- read.csv("K_index.csv", header=T, stringsAsFactors=F)
s <- read.csv("S_stock.csv", header=T, stringsAsFactors=F)
h <- read.csv("H_stock.csv", header=T, stringsAsFactors=F)

cor( na.omit(k$k_rate),  na.omit(s$s_rate) ) # 0.5142455
cor( na.omit(k$k_rate),  na.omit(h$h_rate) ) # 0.3262777

설명: 현대 자동차보다 삼성전자가 코스피 등락율과 더 높은 상관관계를 보입니다.
       na.omit 함수는 결측치를 제외시키는 함수 입니다.

문제288.  코스피 등락율과 삼성전자 수익율 등락율의 plot 그래프를 그리고 그 그래프에
             회귀직선을 그으시오 !

k <- read.csv("K_index.csv", header=T, stringsAsFactors=F)
s <- read.csv("S_stock.csv", header=T, stringsAsFactors=F)
h <- read.csv("H_stock.csv", header=T, stringsAsFactors=F)

all_data <- merge(  merge(k,s, by='date'), h, by='date')  # 3개의 테이블 조인 

attach(all_data) 
plot( k_rate, s_rate, col='blue')   #  x축을 k_rate 하고 y축을 s_rate 로 해서 산포도 그래프
model_s <-  lm( s_rate ~ k_rate,  data=all_data)  # 종속변수를 s_rate 로 하고 독립변수를
abline(model_s, col='red')                               # k_rate 로 해서 회귀계수를 구합니다. 

문제289. 위에 구한 기울기와 절편을 이용해서 위의 그래프의 제목을 회귀직선의
            직선의 방정식으로 제목을 붙이시오 ~

 3시 신호보냈습니다. 

title( paste('삼성등락율=', model_s$coefficients[2], ' x 코스피 등락율 + ' , model_s$coefficients[1]))

▩ 파이썬으로 단순회귀 분석 구현하기 

 " 탄닌 함유량과 애벌래 성장간의 관계에 대한 회귀식을 도출하기 "

#1. 데이터를 로드합니다.
import pandas as pd
reg=pd.read_csv("d:\\data\\regression.txt",sep = "\t")
reg

#2. 종속변수와 독립변수를 지정합니다. 
x = reg[['tannin']]  # 독립변수 
y = reg[['growth']]  # 종속변수 
y
#3. 모델을 설정합니다.
from  sklearn.linear_model  import  LinearRegression

model = LinearRegression()

#4. 모델을 훈련시킵니다.

model.fit( x, y )

#5. 기울기와 절편을 구합니다.

print ( '기울기 :  ' , model.coef_  )  #  [[-1.21666667]]

print ('절편 :  ',  model.intercept_ )  #  [11.75555556]

 성장률 = -1.216 * 탄닌함유량  +  11.755

# 6. 탄닌 함유량이 9일 때의 성장률을 예측하시오 !

result = model.predict([[9]])
print(result)  # 0.80555556

# 7.  위의 회귀 직선을 시각화 하시오 !

y_hat = model.predict(x)  # 회귀직선에 넣고 예측한 값

import  matplotlib.pyplot  as  plt     # 그래프 그리기 위한 모듈
import  seaborn  as   sns               # 그래프 그리기 위한 모듈

plt.figure( figsize=(10,5) )                 # 그래프 사이즈 가로 10, 세로 5 

ax1 = sns.distplot( y,  hist=False, label='y', color='red')    #  실제값을 라인 그래프로 시각화
ax2 = sns.distplot( y_hat, hist=False, label='y_hat', ax=ax1) # 예측값을 라인그래프로 시각화
plt.show()
plt.close()

훈련 데이터에 대해서 실제값과 예측값이 얼마나 일치하는지를 시각화함 

#8.  훈련 데이터를 얼마나 잘 설명하는지를 나타내는 지표인 결정계수값을 출력하시오 !
     ( 1에 가까울 수록 데이터에 대한 설명력이 높습니다.)

r_square =  model.score( x, y )
print (r_square)  # 0.8156632653061224

문제290.  키와 체중 데이터를 이용해서 단순 선형 회귀 모델을 생성하고 
              예측값과 실제값과의 일치성에 대한 시각화 그래프를 그리고 결정계수값을 
             출력하시오 ! ( R 머신러닝 게시판에 데이터 있습니다.)

weight=[ 72, 72, 70, 43, 48, 54, 51, 52, 73, 45, 60, 62, 64, 47, 51, 74, 88,64, 56, 56  ]
tall = [ 176, 172, 182, 160, 163, 165, 168, 163, 182, 148, 170, 166, 172, 169, 163, 170, 182, 174, 164, 160 ]  

독립변수:  키
종속변수: 체중

df_dict = {  'weight' : [72, 72, 70, 43, 48, 54, 51, 52, 73, 45, 60, 62, 64, 47, 51, 74, 88,64, 56, 56  ],
          'tall' : [ 176, 172, 182, 160, 163, 165, 168, 163, 182, 148, 170, 166, 172, 169, 163, 170, 182, 174, 164, 160 ]  }

df =  pd.DataFrame(df_dict)
df 

x = df[['tall']]    # 독립변수
y = df[['weight']]  # 종속변수 


#3. 모델 설정

from sklearn.linear_model import LinearRegression

model = LinearRegression()

#4. 모델 훈련

model.fit(x,y)

#5. 기울기/절편 구하기

print('기울기 : ',model.coef_)
print('절편 : ', model.intercept_)


y_hat = model.predict(x)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure (figsize = (10,5))

ax1 = sns.distplot(y,hist=False,label='y',color='k') # 실제값
ax2 = sns.distplot(y_hat,hist=False,label='y_hat',ax=ax1) # 예측값
plt.show()
plt.close()

#8. 결정계수값 출력

r_square = model.score(x,y)
r_square


▩  위의 키와 체중과의  plot 그래프를 그리고 회귀 직선을 그리기 

sns.lmplot( x ='tall', y='weight', data=df, line_kws={'color':"red"} )

또는 

sns.regplot( x, y )

설명:  직선은 단순회귀 직선이고 분홍색은 신뢰구간입니다. 

문제291.  탄닌함유량과 성장률간의 plot 그래프와 회귀직선 그래프를 그리시오 !


답: 
#1. 데이터를 로드합니다.
import pandas as pd
reg=pd.read_csv("d:\\data\\regression.txt",sep = "\t")
reg

#2. 종속변수와 독립변수를 지정합니다. 
x = reg[['tannin']]  # 독립변수 
y = reg[['growth']]  # 종속변수 
y
#3. 모델을 설정합니다.
from  sklearn.linear_model  import  LinearRegression

model = LinearRegression()

#4. 모델을 훈련시킵니다.

model.fit( x, y )

#5. 기울기와 절편을 구합니다.

print ( '기울기 :  ' , model.coef_  )  #  [[-1.21666667]]

print ('절편 :  ',  model.intercept_ )  #  [11.75555556]

# 성장률 = -1.216 * 탄닌함유량  +  11.755

# 6. 탄닌 함유량이 9일 때의 성장률을 예측하시오 !

result = model.predict([[9]])
print(result)  # 0.80555556

#7. 시각화 합니다. 
sns.regplot( x, y )

문제292.  책에 있는 데이터인 우주왕복선 챌린저호의 o형링 파손수를 y 축으로 하고
             x 축을 온도로해서 plot 그래프 및 회귀직선을 시각화하시오 !

#1. 데이터를 로드합니다.
import pandas as pd
cha=pd.read_csv("d:\\data\\challenger.csv")
cha

#2. 종속변수와 독립변수를 지정합니다. 
x = cha[['temperature']]  # 독립변수 
y = cha[['distress_ct']]  # 종속변수 
y
#3. 모델을 설정합니다.
from  sklearn.linear_model  import  LinearRegression

model = LinearRegression()

#4. 모델을 훈련시킵니다.

model.fit( x, y )

#5. 기울기와 절편을 구합니다.

print ( '기울기 :  ' , model.coef_  )  #  [[-1.21666667]]

print ('절편 :  ',  model.intercept_ )  #  [11.75555556]

#7. 시각화 합니다. 
sns.regplot( x, y )

문제293.  광고비가 매출에 미치는 영향을 알아보기 위해 기존 데이터인 광고비와 
             매출액 대한 회귀직선을 구하고 광고비를 26 이 들었을때 예상되는 매출액이
              얼마인지 출력하시오 !  260,000,000 만원
              독립변수, 종속변수 둘다  천만원 단위 입니다. 

# 1. 데이터 로드
shg  = pd.read_csv("d:\\data\\simple_hg.csv",)
shg

#2. 종속변수와 독립변수로 분리
x = shg[['cost']]
y = shg[['input']]

#3. 모델설정
from  sklearn.linear_model  import  LinearRegression

model = LinearRegression()

#4. 모델훈련
model.fit(x,y)

#5. 모델예측 
model.predict([[26]])  # 119.77786987                45분까지 쉬세요 ~~

문제294. (오늘의 마지막 문제)  미국 대학교 입학점수 데이터를 가지고 단순회귀 곡선을 
              그리시오 ! (데이터셋:  sports.csv )

x 축 :  학과점수(academic)
y 축 :  승인점수(acceptance)

 답글로 그래프와 코드를 올려주세요 ~~

 5시 신호 보냈습니다. ~~~~
 6시 신호 보냈습니다. ~~~~
 나머지 시간은 자유롭게 자습 또는 스터디 하시면 됩니다.  



▩ 다중회귀 분석 (p262)

* 회귀분석의 유형

1. 단순회귀 : 독립변수가 1개이며 종속변수와의 관계가 직선
2. 다중회귀 : 독립변수가 k개이며 종속변수와의 관계가 선형(1차함수)
3. 다항회귀 : 독립변수가 종속변수와의 관계가 1차함수 이상인 관계
4. 곡선회귀 : 독립변수가 1개이며, 종속변수와의 관계가 곡선
5. 로지스틱 회귀 : 종속변수가 범주형(이진변수) 인 경우
6. 비선형 회귀 : 회귀식의 모양이 미지의 모수들의 선형관계로 이루어져 있지 않는 모형 

단순회귀 --------------> 상관관계 ------------------> 다중회귀
                                   ↓
                              다중공선성

머신러닝으로 무엇을 하고자 하는가 ?   1. 분류 : knn, naivebayes, decision tree, random forest
                                                   2. 예측 : regression

다중선형회귀란 ?   단순 선형회귀 분석의 목적이 하나의 독립변수만을 가지고 종속변수를 
                         예측하기 위한 회귀모형을 만들기 위한 것이었다면 다중 회귀분석의 목적은
                         여러개의 독립변수들을 가지고 종속변수를 예측하기 위한 회귀모형을 
                         만드는 것입니다.

예:  집값에 영향을 미치는 요소가 단순히 평수 하나만 있는게 아닙니다.

  집값 <----------------- 평수, 교통, 학군, 범죄율, 층수, 방의 갯수, 한강뷰 .........
    ↑
 종속변수 

질문1 : 집값에 영향을 미치는 요소가 위의 여러가지 독립변수들 중에서 어떤것인가 ?
질문2 : 집값을 예측하기 위한 다중회귀식은 어떻게 되는가 ?

현업예: 태양열전지를 만드는데 있어서 전력효율을 최대화 할 수 있는 가장 좋은 
          태양열 전지 재료의 조합이 어떻게 되는가 ?

 전력양 = 기울기1 * 재료1 + 기울기2 * 재료2 + ..............  + b  ( p263 식)





여러개의 회귀계수을 알아내기위한 그림이 책 263페이지 그림 6.5 에 나옵니다. 







배타값을 알아낼려면 행렬로 연산해서 알아내야 합니다. 그럴려면 아래의 4가지 용어를 
알아야 합니다. (카페에 그림을 참고하세요)

1. 전치행렬  :  행과 열을 교환한 행렬 
2. 단위행렬  :  0으로 이루어진 행렬에서 대각선이 1인 행렬
3. 역행렬    :   자기 자신과 행렬곱(내적) 했을때 단위행렬이 되는 행렬

4. 의사역행렬  : 기본적으로 역행렬을 구하려면 행렬이 정방행렬이어야하는데
                     직사각형 행렬도 역행렬을 구할 수 있도록 구현한 행렬

예제1.  아래의 방정식의 해를 구하시오 ! (게시글 755번)  손으로 구현


예제2.  위의 방정식을 R 로 구현하기 위해서 아래의 A 행렬과 B 행렬을 만드시오 !(R로)


A = matrix( c(1, 3, 2, 4), nrow=2, ncol=2, byrow=T )
A

B = matrix( c(-1, 2), nrow=2, ncol=1 )
B

예제3. 아래의 행렬곱을 수행해서 미지수 x1, x2 행렬을 출력하시오 !

solve(A) %*% B

문제259.  위의 행렬곱을 파이썬으로 구현하시오 ( 카페에 올려주세요 )

import numpy as np

A = np.array([1,3,2,4]).reshape(2,2)  
B = np.array([-1,2]).reshape(2,1)  
A_inv = np.linalg.inv(A)   # 역행렬 구합니다. 
np.dot(A_inv,B)              # 행렬 내적 합니다. 






책에서 위의 식이 없는 이유는 X 행렬이 정방행렬이 아니기 때문에 역행렬을 위와같이
바로 구할 수 없기 때문입니다.   50분까지 쉬세요 ~~
 
그러면 정방행렬이 아닌 직사각형 행렬에 대한 역행렬을 구하려면 어떻게 해야하는가 ?

자기 자신의 행렬에 자기 자신의 전치행렬을 곱하면 정방행렬이 됩니다.
이 정방행렬의 역행렬을 구하면 됩니다. 


▩ 책 264페이지의 중간에 나온 회귀계수를 구하는 식을 구현하기 위해 알아야할 내용

예제1. 아래의 행렬을 R 로 구현하고 아래의 행렬의 전치행렬을 구하시오






a <- matrix(  c(1,2,3,4,5,6), nrow=2, ncol=3, byrow=T)
a
t(a)

예제2. 아래의 단위행렬을 만드시오 !  대각선의 원소가 모두 1이고 나머지 원소는 모두 0 인 행렬


b <- diag(3)
b


예제3. 자기 자신과 단위행렬을 내적하면 자기 자신이 되는지 확인하시오 !









a <- matrix( c(1,2,3,4,5,6,7,8,9), nrow=3, ncol=3, byrow=T)
b <- diag(3)
a%*%b

예제4. 아래의 a 행렬의 역행렬을 R 로 구하시오 !






a = matrix( c(1,2,3,4), ncol=2, nrow=2, byrow=T)
a
solve(a)

 -2.0  1.0
 1.5 -0.5


예제5.  자기자신과 자기자신의 역행렬을 내적하면 단위행렬이 되는지 확인하시오 !






a = matrix( c(1,2,3,4), nrow=2, ncol=2, byrow=T )

round(a %*% solve(a) )

정리:   1. 자기자신과 단위행렬을 내적하면 자기자신이 출력된다.
         2. 자기자신과 자기자신의 역행렬을 내적하면 단위행렬이 출력된다. 

예제6.  기본적으로 역행렬은 정방행렬일 때만 구할 수 있습니다.
          직사각형 행렬이면 역행렬을 구할 수 없습니다. 
          직사각형 행렬의 역행렬을 구하려면 의사 역행렬을 구해야 합니다. 
          이 말이 맞는지 아래의 직사각형 행렬의 역행렬을 구해 봅니다.






a <- matrix( c(1,2,3,4,5,6),  nrow=2, ncol=3, byrow=T)
solve(a)

Error in solve.default(a) : 'a' (2 x 3) must be square

예제7. 아래의 행렬에 아래의 행렬의 전치행렬을 내적하고 그 결과행렬의 역행렬을 구하시오!





                                                점심시간 문제:  B 반은 라인검사
                                                                     A 반은 카페에 올려주세요 ~~

a <-  matrix( c(1,2,3,4,5,6), nrow=2, ncol=3, byrow=T)
b <- a %*% t(a)
solve(b)                             

▩ 다중회귀에서 기울기 행렬을 만들기 위해 알아야할 내용 다시 정리

 1.  자기 자신과 단위행렬을 내적하면 자기 자신이 된다.
 2.  자기 자신과 자기 자신의 역행렬을 내적하면 단위행렬이 된다.
 3.  직사각형 행렬은 역행렬을 구할 수 없으므로 의사 역행렬을 이용해서 역행렬을 구한다. 

  



▩ 위의 식을 R 의 함수로 구현하기 

reg <- function(y, x )  {
                            x  <- as.matrix(x)     # 행렬로 변환하는 코드
                            x  <- cbind( Intercept=1, x )  # 절편을 추가하는 코드       
                            b  <- solve( t(x) %*% x ) %*% t(x) %*% y  # 기울기 구하는 수학식
                            colnames(b)  <- "estimate"   # 컬럼명을 지정 
                            print (b)     }





예제1.  우주 왕복선 챌린저호의 o 형링 파손원인중 가장 영향이 큰 요소가 온도, 압력,
          비행기 노후화를 나타내는 비행기 번호중 어떤것인지 기울기를 알아내시오 ~

cha <-  read.csv("challenger.csv", header=T)
reg( y=cha$distress_ct,  x=cha[  , 2:4 ] )




문제260. 스마트폰 만족도(종속변수)에 영향을 미치는 요소중 가장 영향력이 있는 
            독립변수는 무엇인가 ? ( 데이터:  multi_hg.csv)

smart <- read.csv("multi_hg.csv", header=T)
reg( y=smart$만족감, x=smart[  , 1:3 ] )

           estimate
Intercept  3.5136006
외관        0.2694261
편의성     0.2105249       
유용성     0.1623154

문제261. 위에서 우리가 직접 만든 reg 함수와 R 의 회귀에 관련한 내장함수인 lm 과
            동일한 결과가 나오는지 lm 함수를 이용해서 위의 결과를 출력하시오 !

smart <- read.csv("multi_hg.csv", header=T)

attach(smart)
lm( 만족감 ~ 외관 + 편의성 + 유용성, data=smart )

Coefficients:
(Intercept)         외관       편의성       유용성  
     3.5136       0.2694       0.2105       0.1623  

문제262.  위의 reg 함수를 파이썬으로 구현하고 아래와 같이 실행되게하시오 !( 답글로 올려주세요)

cha = pd.read_csv("d:\\data\\challenger.csv")
reg( cha['distress_ct'],  cha.iloc[ :  , 1:4 ] )

Intercept                 3.527093383
temperature            -0.051385940               
field_check_pressure  0.001757009
flight_num               0.014292843

reg <- function(y, x )  {
                            x  <- as.matrix(x)     # 행렬로 변환하는 코드
                            x  <- cbind( Intercept=1, x )  # 절편을 추가하는 코드       
                            b  <- solve( t(x) %*% x ) %*% t(x) %*% y  # 기울기 구하는 수학식
                            colnames(b)  <- "estimate"   # 컬럼명을 지정 
                            print (b)     }

답:
import pandas as pd
import numpy as np
cha = pd.read_csv('C:\\Users\\sjjung\\Desktop\\BigData\\R\\challenger.csv')

def reg(y,x):
    x_c = x.columns
    x['Intercept'] = 1
    x = x.to_numpy()
    b = np.dot(np.dot(np.linalg.inv(np.dot(x.T,x)),x.T),y)
    for i in range(len(x_c)):
        print(x_c[i],b[i])

reg(cha['distress_ct'],cha.iloc[:,1:4])

▩ 분류할 때도 데이터를 정규화하고 모델을 학습 시켰는데 회귀분석에서도 정규화를 해야하는가?

 책의 예제인 미국 보험회사에서 미국 국민의 의료비를 가지 보험 비용을 산정하는 예

 1. 정규화를 하는 경우:

  보험비용에 가장 영향을 크게 미치는 변수가 무엇인지 확인할 때

  종속변수에 대한 독립변수의 영향도를 확인하고 싶을 때 

 예: 의료비에 가장 큰 영향을 주는 컬럼이 무엇인가?  부양가족수, 비만여부, 흡연여부, 나이, 사는지역

 2. 정규화를 하지 않는 경우:

 나이가 한 살 늘어날때 의료비가 얼마나 인상되어야 하는지 예측해야할 때

 부양가족이 한명이 더 늘어날때 마다 의료비가 얼마나 인상되는지 예측해야할 때

 예:  부양가족이 한명 더 늘어나면 년간 의료비가 55만원 더 늘어나게 됩니다. 


▩ 표준화 와 정규와의 차이점 

 1. 표준화 :  평균이 0 이고 표준편차 1인 데이터 분포로 데이터를 구성하는것
                예:  scale 함수(내장함수)

 2. 정규화 :  min/max 정규화인데 데이터를 0 ~ 1사이의 숫자로 변환하는 것
                 예: minmax 함수 (직접생성해야함)


예제1.  미국 대학교 입학에 가장 크게 영향을 미치는 과목이 무엇인지 알아내시오 ~
           (정규화 하지 않고 수행하세요 ~)

데이터:  sports.csv 
컬럼   :  academic(학과점수)
            sports ( 체육점수)
            music ( 음악점수)

           acceptance :  입학 기준 점수( academic , sports, music 점수를 가지고 산출한 점수)

m <-  read.csv("sports.csv",  header=T)
m

reg( y = m$acceptance,  x= m[   , c(2:4) ] )

            estimate
Intercept 11.4902799
academic   0.1557737
sports       0.5726859
music       0.1046008

정규화를 하지 않았을때는 위와 같이 체육점수가 가장 영향력이 크게 나왔습니다. 

예제2. 이번에는 정규화를 하고 기울기를 확인하세요 !

m <- read.csv("sports.csv", header=T)

normalize <-  function(x) {
                                      return  (  (x-min(x)) / (max(x) - min(x) ) )
                                  }

sports_n <- as.data.frame( lapply( m[   , c(2:5)], normalize)  )
head(sports_n)

reg( y=sports_n$acceptance,  x = sports_n[   ,c(1:3) ] )

                estimate
Intercept 0.06121748
academic  0.48963854
sports    0.30194528
music     0.11432339

이번에는 정규화를 했더니 학과점수가 더 높게 기울기가 출력되었습니다. 
위와같이 종속변수에 영향도가 가장 큰 독립변수가 무엇인지 알아내려면 정규화를 해야합니다. 

문제263. R 함수인 scale 함수를 이용해서 표준화를 하고 기울기를 확인하시오 

m <- read.csv("sports.csv", header=T)

sports_n <- as.data.frame( lapply( m[   , c(2:5)],  scale )  )
head(sports_n)

reg( y=sports_n$acceptance,  x = sports_n[   ,c(1:3) ] )

문제264. 위의 결과(표준화해서 영향도 확인하는것) 를 파이썬으로 수행하시오 ~

정규화:  from  sklearn.preprocessing  import  MinMaxScaler
표준화:  from  sklearn.preprocessing  import  StandardScaler 

m = pd.read_csv("d:\\data\\sports.csv")

x = m.iloc[  : , 1:4 ]
y = m[['acceptance']]
y

from  sklearn.preprocessing  import  StandardScaler 

scaler = StandardScaler()
scaler.fit(x)
sports_n = scaler.transform(x)
sports_n

df = pd.DataFrame(sports_n)
df.columns = m.columns[1:4]
df

def reg(y,x):
    x_c = x.columns
    x['Intercept'] = 1
    x = x.to_numpy()
    b = np.dot(np.dot(np.linalg.inv(np.dot(x.T,x)),x.T),y)
    for i in range(len(x_c)):
        print(x_c[i],b[i])

reg(y ,df)
                                                             
academic [13.34035819]
sports [8.48127578]
music [2.91307106]




문제265. (오늘의 마지막 문제)  파이썬으로 만든 다중회귀 기울기 구하는 reg 함수에
           다음과 같이 insurance 를 입력했을때 수치형 데이터만 알아서 선택해서 
           다음과 같이 기울기가 출력되게하시오 

reg( insurance )

age            ?
bmi            ?
children       ?
expenses     ?

  5시 신호 보냈습니다. 나머지 시간은 자유롭게 자습 또는 스터디 하시면 됩니다. 
  6시 신호 보냈습니다. 

insurance = pd.read_csv("c:\\data\\insurance.csv")

def reg(x):
    label = input('라벨 컬럼명을 입력해주세요')
    a = []
    for j in x.columns[x.dtypes.isin(['int64','float64'])]:   # 수치형 컬럼들 가져오기
        a.append(j)
    x = x.loc[ : , a]  # 독립변수들 선택
    y = x[[label]]    # 종속변수 선택
    x_c = x.columns   # 독리변수들 컬럼들을 선택
    x['Intercept'] = 1  # 절편 만들고 
    x = x.to_numpy()  # 넘파이 어레이로 독립변수들을 변환 
    b = np.dot(np.dot(np.linalg.inv(np.dot(x.T,x)),x.T),y)  # 다중 회귀 수학식 구현
    for i in range(len(x_c)):   #  출력합니다. 
        print(x_c[i],b[i])

reg(insurance)

def reg(x,y):
    from statsmodels.formula.api import ols  # 회귀분석 패키지 임폴트
    y2 =[]
    for i in y.columns[(y.dtypes=='float64') | (y.dtypes=='int64')]:   # 수치형 컬럼들을 y2 리스트에 담는다
        y2.append(i)
    #print(y[y2])
    y3 = y[y2]     #  수치형 컬럼들만 선택
    res= ols('x~y3', data=ins).fit()    # 문법 ols('종속변수 ~ 독립변수', data=데이터프레임명)
    #return(res.summary())
    print(res.params)
    #print(ins.columns)

reg(ins['expenses'], ins)






▩ 다중회귀분석 실습 (미국 국민의 의료비 데이터를 예측)  p 268

분석목표:  보험회사의 보험비 산정을 위해 의료비 데이터를 가지고 다중 회귀 모델을 생성

# 1. 데이터를 로드합니다.
# 2. 결측치를 확인합니다.
# 3. 종속변수가 정규성을 띠는지 확인
# 4. 독립변수들과 종속변수간의 상관관계가 있는지 확인
# 5. 다중 회귀 분석 모델을 생성합니다.
# 6. 회귀분석 결과해석을 합니다.
# 7. 회귀분석 모형의 설명력을 확인합니다. (결정계수)
# 8. 결정계수를 높이기 위한 파생변수를 추가해서 성능을 높입니다.


목표: 보험회사에서 보험료 산정에 도움이 될 수 있도록 미국민의 의료비를 예측하는
       회귀모델을 생성하기 

# 1. 데이터를 로드합니다. 
insurance <- read.csv("insurance.csv")
head(insurance)

데이터 소개: 미국 환자의 가상 의료비가 들어있는 모의 데이터셋입니다. 
                 이 데이터는 미국 통계국의 인구 통계를 이용해 생성되었으며 대개 실제
                 질병을 반영합니다. 의료보험에 등록된 1,338명의 수익자 예시가 들어있으며
                 각 예시는 환자의 특성과 해당 연도에 의료보험에 청구된 전체의료비를 나타내는
                 특징으로 구성되어 있습니다.

age : 나이
sex  : 성별
bmi : 체질량 비만 지수
children : 부양가족수
smoker : 흡연여부
region : 사는 지역(북동, 남동, 북서, 남서)
expenses : 의료비 (종속변수)

# 2. 결측치가 있는지 확인합니다.

colSums( is.na(insurance) )

# 3. 종속변수가 정규성을 띄는지 확인합니다.

hist( insurance$expenses) 

그래프 해석:  오른쪽으로 꼬리가 긴 분포를 보여줍니다. 대다수의 사람들의 의료비는 
                  0 ~ $15,000 달러 사이에 있습니다.  이 분포는 선형회귀에서는 이상적이지 않지만
                  미리 약점을 알고 있으며 나중에 모델을 설계할 때 도움이 됩니다. 

# 4. 독립변수들과 종속변수간의 상관관계를 확인합니다. 

cor ( insurance[     , c("age","bmi","children","expenses") ]  )

                age        bmi        children    expenses
age      1.0000000 0.10934101 0.04246900 0.29900819
bmi      0.1093410 1.00000000 0.01264471 0.19857626
children 0.0424690 0.01264471 1.00000000 0.06799823
expenses 0.2990082 0.19857626 0.06799823 1.00000000

눈에 띄게 아주 강한 상관관계를 보이는것은 없지만 일부 눈에 띄는 연관성이 있습니다.
예를 들어 age 와 bmi 는 약한 양의 상관관계가 있어서 나이가 들수록 몸무게가 증가하는
경향이 있다. age 와 expenses 를 보면 양의 상관관계를 보이고 있어서 나이가 들수록
의료비가 증가하는 경향이 있다. 

독립변수들끼리 강한 상관관계를 보이고 있지는 않지만 다중공선성을 보이는지 확인합니다.

model <- lm( expenses ~ age + bmi + children,  data=insurance)
library(car)
vif(model) > 10  # 다중공선성을 보이는 변수들 확인 

     age      bmi   children 
   FALSE    FALSE    FALSE 

▩ 시각화해서 상관관계 확인하기 

library(psych)

pairs.panels( insurance[     ,  c('age', 'bmi', 'children', 'expenses') ] )

산포도에 있는 달걀모양의 객체는 상관관계 타원형으로 상관관계 강도를 시각화 한것입니다.

 타원이 늘어질수록 -------> 강한 상관관계
 타원이 거의 완벽한 둥근 달걀 모양 -----------> 약한 상관관계

* 독립변수들의 상관관계를 통해서 알 수 있었던 점을 정리하면 ?

1. 나이가 많을 수록 의료비 더 많이 든다
2. 나이가 많을 수록 비만지수 더 높았다
3. 중년 무렵부터 부양가족수가 최고점이 된다. 

45분까지 쉬세요 

※ 왜 회귀분석을 하기전에 상관관계를 확인해야하는가?

  독립변수들간의 강한 상관관계를 보이게 되는 다중 공선성 여부를 확인해야해야
  회귀분석 결과에 가장 중요한 결정계수(설명력)에 대한 신임을 할 수 있기 때문입니다.

# 5. 다중 회귀 분석 모델을 생성합니다.

model <- lm( expenses ~  age + children + bmi + smoker + region,  data=insurance )

또는

attach(insurance)

model <- lm( expenses ~ .  ,  data = insurance )

# 6. 회귀분석 결과해석을 합니다.

model 

Coefficients:
    (Intercept)              age          sexmale              bmi  
       -11941.6            256.8           -131.4            339.3  

       children        smokeryes  regionnorthwest  regionsoutheast  
          475.7          23847.5           -352.8          -1035.6  

   regionsouthwest  
         -959.3  

설명:   1. 나이가 일년씩 더해질 때 마다 평균적으로 의료비가 256.8 달러 증가될 것으로 예상됩니다
          2. 자녀가 한명씩 추가 될 때 마다 475.7 달러 추가될것으로 예상됩니다. 
          3. 비만지수(bmi) 의 단위가 증가할 때 마다 연간 의료비가 339.3 달러 증가될것으로 
              예상됩니다. 

          4. 더미변수를 자동으로 추가해서 변수 값의 상대적 추정은 다음과 같습니다. 
                
                 sexmale  -131.4 --> 남성은 여성에 비해서 매년 의료비가 131.4 달러 적게 든다고
                                             예상하고 있습니다. 
                 smokeryes  23847.5 --> 흡연자는 비흡연자보다 매년 평균 의료비가 23,847.5 달러
                                                  의 비용이 더 듭니다. 
                                 
                northeast 에 비해  northwest  는 의료비가 연간 평균 352.8 달러 덜 들고
                                         southeast   는 의료비가 연간 평균 1035.6 달러 덜 들고
                                         southwest  는 의료비가 연간 평균 959.3 달러 덜 든다.

# 7. 회귀분석 모형의 설명력을 확인합니다. (결정계수)

앞에서 분류를 할 때는 모델의 성능평가를 "정확도" 로 했었습니다.

회귀분석일 때는 회귀모델의 성능 평가를 무엇으로 하나요 ?  "결정계수"

결정계수가  1에 가까운 값이 나와서 이 회귀모델의 설명력이 높다고 말할 수 있습니다.

summary(model)

Multiple R-squared:  0.7509

※ 결정계수 ?  데이터에 대한 회귀모델의 설명력을 나타내는 척도

# 8. 결정계수를 높이기 위한 파생변수를 추가해서 성능을 높입니다. 

성능높이기 질문1:  나이가 들면 의료비가 많이 든다는것을 상관관계 분석을 통해서도 확인했는데
                          나이 데이터를 더 크게 만들어서 파생변수를 생성하면 결정계수가 더 올라갈까?

insurance$age2 <-  insurance$age^2
head(insurance)

model2 <- lm(expenses ~  .  , data=insurance)
summary(model2)   # 결정계수 :  0.7509 ---->  0.7537  로 결정계수가 상승했습니다. 

성능높이기 질문2 :   비만인 사람(bmi 가 30이상) 이 의료비가 더 많이 들거라 예상하고
                            insurance 데이터 프레임에 비만인 사람과 비만이 아닌 사람들을 구분하는
                            파생변수를 추가하면 결정계수가 더 올라가는지 확인해 봅니다.

insurance$bmi30 <-  ifelse( insurance$bmi >= 30, 1,  0 )
head(insurance)

model3 <-  lm(expenses ~  .  , data=insurance)
summary(model3) #  0.7582

결정계수 :  0.7509 ---->  0.7537 ----> 0.7582  로 결정계수가 상승했습니다. 
                           ↑                 ↑
                        age2 추가     bmi30 추가 

성능높이기 질문3:   비만인 사람이 담배까지 피게 되면 의료비가 더 증가할 것으로 예측되는지
                           파생변수를 추가해서 결정계수를 확인하세요 ~

파생변수명:  smokeryes_bmi30 

insurance$smokeryes_bmi30 <- ifelse( insurance$smoker =='yes' & insurance$bmi >= 30, 1, 0)
head(insurance)

model4 <- lm(expenses ~  .  , data=insurance)
summary(model4)  # 0.8664

설명: 0.7509 ---->  0.7537 ----> 0.7582 ----> 0.8664 로 결정계수가 증가 했습니다.
                   ↑               ↑                 ↑
                age2 추가     bmi30 추가    smokeryes_bmi30 

기존에 없는 새로운 컬럼을 파생변수라고 합니다. 

맨끝에 나온 model4 의 summary 결과를 보면은  smokeryes_bmi30 의 기울기가  19810 으로 나오고
있습니다.  smokeryes 의 기울기는    13405 로 나오고 있습니다.
이는  원래 흡연만 했을때는 연간 의료비가 13,405 달러가 드는데 비만인 사람이 흡연까지 하게되면
연간 의료비가 19,810 달러가 지출됨을 확인 할 수 있습니다. 

▩ 회귀분석 모델 summary 결과 해석

summary(model4) 

Residuals: 잔차? 표본에서 나온 관측값이 회귀선과 비교할 때 나타나는 차이 

     Min       1Q    Median       3Q      Max 
-17297.1  -1656.0  -1262.7   -727.8  24161.6 

예측에서 잔차에 대한 요약 통계를 위와 같이 확인할 수 있는데

1. 모델이 최소 하나의 관측치에 대해 거의 24161 달러의 비용을 낮게 예측했다

2. 잔차의 50% 는 -1656 달러 ~ -727 달러 사이에 있다. 

                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)       139.0053  1363.1359   0.102 0.918792    
age               -32.6181    59.8250  -0.545 0.585690    
sexmale          -496.7690   244.3713  -2.033 0.042267 *  
bmi               119.7715    34.2796   3.494 0.000492 ***
children          678.6017   105.8855   6.409 2.03e-10 ***
smokeryes       13404.5952   439.9591  30.468  < 2e-16 ***
regionnorthwest  -279.1661   349.2826  -0.799 0.424285    
regionsoutheast  -828.0345   351.6484  -2.355 0.018682 *  
regionsouthwest -1222.1619   350.5314  -3.487 0.000505 ***
age2                3.7307     0.7463   4.999 6.54e-07 ***
bmi30            -997.9355   422.9607  -2.359 0.018449 *  
smokeryes_bmi30 19810.1534   604.6769  32.762  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 4445 on 1326 degrees of freedom
Multiple R-squared:  0.8664,	Adjusted R-squared:  0.8653 
F-statistic: 781.7 on 11 and 1326 DF,  p-value: < 2.2e-16

설명: 추정된 회귀 계수별로 표시된 p값은 추정된 계수가 실제 0일 확률 추정치 입니다.
       p 값이 작은 경우 실제계수가 0 이 아닐 가능성이 높다는것을 말하며
       특징이 종속변수와 관계가 없을 가능성이 아주 낮다는것을 의미합니다.

       유의수준보다 낮은 p 값은 통계적으로 유의한것으로 간주됩니다.
       p 값이 0.05 미만으로 나온 독립변수가 유의한 변수 들입니다.


* p-value 값?  귀무가설에서 얻은 검정통계량의 값 이상으로 대립가설에서 유리한 
                   데이터를 얻을 수 있는 확률 

 p값 > 유의수준  ----------------> 귀무가설을 기각할 수 없다.
 p값 < 유의수준 -----------------> 대립가설을 채택할 충분한 근거가 있다.

옆에 나온 별(*) 이 추정치로 충족되는 유의수준을 나타내는 각주에 해당됩니다. 

smokeryes_bmi30 에 대한 귀무가설과 대립가설이 뭔지 정의해보면 ?

귀무가설:  비만인 사람이 흡연까지 하는것은 의료비 상승과 관련이 없다.
대립가설:  비만인 사람이 흡연까지 하는것은 의료비 상승과 관련이 있다.

▩ Adjust R-square  ?
 
 좋은 회귀 모형에는 두가지 조건이 있다.

 1. 데이터 잘 설명한다.
 2. 간단하다.

  독립변수가 많은 회귀 모형의 경우에는 위의 첫번째 조건을 만족합니다. 
  그러나 두번째 조건에서는 탈락이라고 볼 수 있습니다. 
  아무리 설명력이 좋아도 복잡하다면 그다지 좋은 모형은 아닙니다.
  독립변수가 많으면 결정계수가 높아지는데 결정계수가 모형의 설명력을 측정하기에
   좋은 척도라는것은 사실이지만 위의 단점을 보안하기 위해 보안된 척도가 있는데
  그렉 바로 "조정된 결정계수(Adjusted R-squared)" 입니다.

R-squared: 0.8664,	Adjusted R-squared:  0.8653

독립변수가 종속변수를 86% 설명하고 있다는 뜻입니다.
만약 위의 둘이 차이가 크면 불필요한 변수가 있을것이라고 예상할 수 있습니다. 

회귀모델을 만들고 데이터 분석 결과 보고서를 만들때 데이터 분석가들이 해야할 일?

 1. 성능을 높이기 위한 작업에 대한 설명 :   

     분류일 때는 모델의 성능을 개선하려고 했던일 ?     하이퍼 파라미터 조절
                                                                          1. knn : k 값
                                                                          2. naivebayes :  laplace 값
                                                                          3. decision tree: trials 값

    회귀분석일 때 모델의 성능을 개선하기 위해 해야할 일 ?   파생변수 추가 입니다.
 

▩ 책에 나온 방법으로 파생변수 생성하여 결정계수 올리기 (p 285)

 책에 나온데로 bmi30*smoker 추가하기 실습

# 1. 데이터 로드하기
insurance <- read.csv("insurance.csv")

# 2. age2 추가하기
insurance$age2 <- insurance$age^2

# 3. bmi30 파생변수 추가하기
insurance$bmi30 <- ifelse( insurance$bmi >= 30, 1, 0 )

# 4. 모델 생성하기 
model7 <- lm( expenses ~ age + age2 + children + bmi + sex + bmi30*smoker + region,
                      data=insurance )

summary(model7)

문제266.  bmi30 과 성별을 이용해서 위와 같이 파생변수가 모델생성할 때 만들어지게 하고
             혹시 결정계수가 올라가는지 아니면 유의한 독립변수가 만들어지는지 확인하시오 !


# 1. 데이터 로드하기
insurance <- read.csv("insurance.csv")

# 2. age2 추가하기
insurance$age2 <- insurance$age^2

# 3. bmi30 파생변수 추가하기
insurance$bmi30 <- ifelse( insurance$bmi >= 30, 1, 0 )

# 4. 모델 생성하기 
model8 <- lm( expenses ~ age + age2 + children + bmi + bmi30* sex + smoker + region,
                      data=insurance )

summary(model8)

성별과 smoker 를 가지고 파생변수를 만들었더니 남자이면 흡연을 하면 연간 의료비가
2385 달러 더 들었음을 확인할 수 있습니다. 

# 4. 모델 생성하기 
model8 <- lm( expenses ~ age + age2 + children + bmi + bmi30 + sex + sex*smoker + region,
              data=insurance )

문제267. 남자이면서 흡연하는 경우와 여자이면서 흡연을 하는 경우에 어느쪽이 더 의료비가 
            많이 드는지 두개의 파생변수를 추가해서 모델을 생성하시오 !

# 1. 데이터 로드하기
insurance <- read.csv("insurance.csv")

# 2. age2 추가하기
insurance$age2 <- insurance$age^2

# 3. bmi30 파생변수 추가하기
insurance$bmi30 <- ifelse( insurance$bmi >= 30, 1, 0 )

# 4. 성별별 흡연여부 2개의 파생변수 추가
man_smokeryes <- ifelse( insurance$smoker =='yes' & insurance$sex == 'male', 1, 0)
woman_smokeryes <- ifelse( insurance$smoker =='yes' & insurance$sex =='female', 1, 0)

# 5. 모델 생성하기 
model8 <- lm( expenses ~ age + age2 + children + bmi + bmi30 + sex  + region +man_smokeryes+woman_smokeryes ,
              data=insurance )

man_smokeryes    24903.644    542.120  45.938  < 2e-16 ***
woman_smokeryes 22518.468    613.068  36.731  < 2e-16 ***


점심시간 문제: A 반은 라인검사
                    B 반은 카페에 올려주세요 ~ A반인데 개인사정으로 못온 학생들 카페에 올려주세요 ~

▩ 다중공선성 

  다중 회귀 분석을 하고 결과를 봤더니 유의한 변수들을 발견할 수 없었다고 한다면
  다중 공선성을 의심해 봐야합니다.

  다중회귀분석을 했는데 결과에서 유의한 변수들이 보이지 않는다면 독립변수들끼리의
  상관관계가 아주 높은지 의심을 해봐야합니다.

 만약에 독립변수들끼리의 상관관계가 아주 강하여 절대값 1에 가까워지면
 최소제곱법 적용자체가 매우 심각한 국면을 맞이하게 됩니다.
 이때 나타나는 현상을 다중 공선성이라고 합니다.

▩ 다중 공선성 실험 

 1.  다중공선성의 vif(팽창계수) 를 확인할 수 있는 패키지를 설치합니다.
  install.packages("car")
  library(car)

 2.  데이터를 로드합니다.
  test <- read.csv("test_vif1.csv")
  test

종속변수 :시험점수
독립변수: 아이큐, 공부시간 

 3. 독립변수들 끼리의 상관관계를 확인합니다.
 cor(  test[    ,  c("아이큐", "공부시간") ] )

            아이큐  공부시간
아이큐   1.0000000 0.7710712
공부시간 0.7710712 1.0000000

설명: 두 독립변수의 상관관계가 강한 양의 상관관계를 보이고 있습니다. 

 3. 회귀모델을 생성합니다.

model <-  lm( test$시험점수 ~  아이큐 + 공부시간 , data=test )
summary(model)

결정계수가 0.90 으로 1에 가까운 설명력을 보이고 있고 아이큐, 공부시간 둘 다
유의미한 독립변수임이 확인되고 있습니다. 

 4. 다중 공선성을 보이는지 확인합니다. 

library(car)
vif(model)  > 10       

현업기준:  팽창계수(vif) 가 10보다 큰것으로 골라내는게 일반적이고 
              엄격하게 하려면 5보다 큰것을 골라냅니다. 느슨하게 하려면 15~20으로 골라냅니다.

결론:  공부시간과 아이큐는 서로 상관관계가 높았으나 팽창계수가 높지 않아 
         이 회귀모델은 적절한 모형임이 확인이 됩니다.

문제268.  test_vif2.csv 를 로드하면 등급평균이 추가되어있는데 이 데이터를 로드해서
             다중회귀 분석을 하고 결정계수를 확인하고 다중공선성을 보이는 독립변수들이
             있는지 실험하시오 !

독립변수: 공부시간, 아이큐, 등급평균
종속변수: 시험점수 

※ 중요하게 확인해야할 내용 

 다중 공선성을 보이는 독립변수들의 p-value 값이 어떻게 나타나고 있는지 확인해야합니다. !!!
 그 독립변수의 p-value 가 0.05 미만인지를  확인해야 합니다.
 그래야 유의한 변수이기 때문입니다.

# 1. 데이터를 로드합니다.
test2 <- read.csv("test_vif2.csv")
test2

# 2. 독립변수들끼리의 상관관계를 확인합니다.

cor( test2[   , c("아이큐","공부시간","등급평균") ] )

              아이큐  공부시간  등급평균
아이큐   1.0000000 0.7710712 0.9736894
공부시간 0.7710712 1.0000000 0.7300546
등급평균 0.9736894 0.7300546 1.0000000

아이큐와 등급평균이 서로 1에 가까운 높은 상관관계를 보입니다.

#3. 회귀모델을 생성합니다.

model2 <-  lm( test2$시험점수 ~ 아이큐 + 공부시간 + 등급평균, data=test2) 
summary(model2)

아이큐와 등급평균 둘다 유의미한 독립변수가 아님을 나타내고 있습니다.
둘다 p-value 가 크게 나왔습니다. 즉 각각의 독립변수들은 종속변수에 유의한 영향을
미치지 못하고 있습니다.  그렇기 때문에 아이큐와 등급평균 둘다 시험점수에 미치는 
영향이 없거나 또는 두 변수의 다중공선성을 의심해 봐야합니다. 

#4. 다중 공선성 여부를 확인합니다.

vif(model2) > 10

아이큐 공부시간 등급평균 
    TRUE    FALSE     TRUE 

둘다 현업기준으로 봤을때 높은 다중공선성여부를 보이므로 회귀분석 결과에
좋지 않은 영향을 미쳤습니다. 이럴때는 둘중에 하나를 제외하고 회귀분석을 해야합니다. 
그러면 둘중에 어떤것을 제외 시켜야할지는 각각 테스트를 해보고 결정계수가 높은것을
선택하면 됩니다. 

1. 아이큐, 공부시간
2. 등급평균, 공부시간 

model5 <-  lm( test2$시험점수 ~ 아이큐 + 공부시간 , data=test2) 
summary(model5) # 0.9053

model6 <-  lm( test2$시험점수 ~ 등급평균 + 공부시간 , data=test2) 
summary(model6) # 0.9154

▩ 파이썬으로 다중 공선성 문제를 실험하는 방법

# 1. 데이터를 로드합니다.

import  pandas  as  pd
test2 = pd.read_csv("d:\\data\\test_vif2.csv", encoding="euckr")
test2

# 2. 다중회귀 모델을 만듭니다.
from  statsmodels.formula.api   import   ols

model = ols('시험점수 ~ 아이큐+공부시간+등급평균',  test2)

result = model.fit()  # 모델 훈련

#3. 회귀분석 결과를 확인합니다. 
result.summary()

print('결정계수: ', result.rsquared)

#4.  팽창계수를 확인합니다. 

from   statsmodels.stats.outliers_influence  import  variance_inflation_factor

model.exog_names  # 모델에서 보이는 컬럼명과 순서확인

['Intercept', '아이큐', '공부시간', '등급평균']

variance_inflation_factor( model.exog,  1 )  # 아이큐의 팽창계수 확인 
variance_inflation_factor( model.exog,  2 )  # 공부시간의 팽창계수 확인 
variance_inflation_factor( model.exog,  3 )  # 등급평균의 팽창계수 확인 

22.64355276424414
2.5177862499212305
19.658263836145316

아이큐와 등급평균의 팽창계수가 10 이상으로 나오고 있어서 이 두개의 독립변수들이
유의미한 독립변수로 확인되고 있지 않는 것입니다. 

문제269.  등급평균과 공부시간을 독립변수로 하고 시험점수를 종속변수로 해서 모델을 
              만들고 다중공선성 여부를 확인해보세요 ~

# 1. 데이터를 로드합니다.

import  pandas  as  pd
test2 = pd.read_csv("d:\\data\\test_vif2.csv", encoding="euckr")
test2

# 2. 다중회귀 모델을 만듭니다.
from  statsmodels.formula.api   import   ols

model = ols('시험점수 ~ 공부시간+등급평균',  test2)

result = model.fit()  # 모델 훈련

#3. 회귀분석 결과를 확인합니다. 
result.summary()

print('결정계수: ', result.rsquared)

#4.  팽창계수를 확인합니다. 

from   statsmodels.stats.outliers_influence  import  variance_inflation_factor

model.exog_names  # 모델에서 보이는 컬럼명과 순서확인

['Intercept',  '공부시간', '등급평균']

variance_inflation_factor( model.exog,  1 )  # 아이큐의 팽창계수 확인 
variance_inflation_factor( model.exog,  2 )  # 공부시간의 팽창계수 확인 
 
▩ 미국민의 의료비를 예측하는 회귀모델을 가지고 새로운 데이터를 예측해보시오 (p288)

# 1.  bmi30 과  bmi30 이면서 smoker 가 yes 인 파생변수를 추가한 회귀모델을 생성한다.

#1
insurance<-read.csv("d:\\data\\insurance.csv")
#2
insurance$age2<-insurance$age^2
#3
insurance$bmi30<-ifelse(insurance$bmi>=30,1,0)
#4
model267<-lm(expenses ~ ., data=insurance)
summary(model267)
head(insurance)

# 2.  위에서 만든 회귀모델로 아래의 사람의 의료비를 예측한다. 

  age: 30,  
  children : 2,  
  bmi : 30, 
  sex : male , 
  bmi30 : 1, 
  smoker : no, 
  region : northwest

a <- data.frame( age=30, age2=30^2, children=2, bmi=30, sex='male', bmi30=1, 
                      smoker='no', region='northwest' )

predict( model267 , a  )  # 7481.282 


▩ 파이썬으로 다중회귀 분석 구현하기 

▣ 예제1. 미국 우주 왕복선 폭파원인 분석

  o 형링의 손상이 온도, 압력, 비행기 번호 이 3가지 중에 어떤게 더 영향이 큰지 ?
 
# 1. 데이터를 로드합니다.
import  pandas  as  pd
cha = pd.read_csv("d:\\data\\challenger.csv")
cha 

# 2. 다중회귀 모델을 생성합니다.
import  statsmodels.formula.api   as  smf

model = smf.ols( formula = 'distress_ct ~ temperature + field_check_pressure + flight_num',
                       data=cha )

# 3. 다중회귀 모델을 훈련 시킵니다.
result = model.fit()

# 4. 모델의 결과를 분석합니다 
result.summary()

설명: 온도 , 비행기 번호, 압력순으로  o 형링 파손에 영향을 주고 있습니다.  
        온도가 가장 영향력이 크므로 온도와 o형링 파손수와의 단순 선형 회귀분석을
        해서 결과분석을 해보겠습니다. 

    " 온도가 몇도일때 우주 왕복선을 발사하는게 좋은지 확인해봅니다."

model2 = smf.ols(formula = 'distress_ct ~ temperature', data=cha)
result2 = model2.fit()
result2.summary()

Intercept   	3.6984
temperature	-0.0475	

  o형링 파손수 = 3.6984 - 0.0475 x 온도 	

    2.21 개                                   31도  F(화씨)
    0.82 개                                   60도  F(화씨)
    0.34 개                                   70도  F(화씨)

분석결과:  30도에서 발사하는게 화씨 60도에서 발사하는것보다 3배 더 위험하고
              화씨 70도에서 발사하는것보다 7~8 배 더 위험합니다. 

 48분까지 쉬세요 ~~~


문제270.  미국대학교 입학점수에 가장 영향을 크게 미치는 과목이 무엇인지
             다중회귀분석 모델을 만들어서 확인하시오 !

데이터셋 :  sport.csv

종속변수 :  acceptance

독립변수:  academic
              sports
              music 

# 1. 데이터를 로드합니다.
df = pd.read_csv("d:\\data\\sports.csv")

# 2. 모델을 생성합니다.
import  statsmodels.formula.api  as  smf

model = smf.ols( formula ='acceptance~ academic + sports + music', data=df )

# 3. 모델을 훈련 시킵니다.
result = model.fit()

# 4. 분석결과를 확인합니다.
print( result.summary() )

문제271. 위에서는 정규화를 하지 않고 확인했더니 체육점수가 가장 기울기 컸습니다.
            이번에는 정규화하고 수행하세요 ~

정규화:  from  sklearn.preprocessing  import  MinMaxScaler
표준화:  from  sklearn.preprocessing  import  StandardScaler 

# 1. 데이터를 로드합니다.
df = pd.read_csv("d:\\data\\sports.csv")

# 2. 정규화 합니다. 
from  sklearn.preprocessing  import  MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(df)                                # 정규화를 위해 계산합니다. 
df_scaled = scaler.transform(df)      # 계산된 내용으로 정규화 합니다. 

# 판다스 데이터 프레임으로 변환
df_scaled2 = pd.DataFrame(df_scaled)   # 판다스 데이터 프레임으로 변환합니다.
df_scaled2.columns = df.columns         # df_scaled2 의 컬럼명을 df 의 컬럼명으로 지정합니다. 

# 3. 모델을 생성합니다.
import  statsmodels.formula.api  as  smf

model = smf.ols( formula ='acceptance~ academic + sports + music', data=df_scaled2 )

# 4. 모델을 훈련 시킵니다.
result = model.fit()

# 5. 분석결과를 확인합니다.
print( result.summary() )

정규화를 했더니 학과점수가 입학점수에 더 영향력이 크게 나오고 있습니다. 

종속변수까지 정규화 안했을때의 결과 

 아카데믹    체육       음악 
46.0623	/ 28.4052 / 10.7549

종속변수까지 정규화 안했을때의 결과

0.4896 / 0.3019 / 0.1143

▩ 미국민 의료비 데이터를 파이썬으로 회귀모델 생성하기

 데이터셋 : insurance.csv
 종속변수:  expenses
 독립변수:  age, sex, bmi, children, smoker, region

# 1. 데이터를 로드합니다.
insurance = pd.read_csv("d:\\data\\insurance.csv")
insurance.head()

# 2. 결측치를 확인합니다.
insurance.isnull().sum()

# 3. 종속변수의 정규성을 확인합니다.

insurance.expenses.plot(kind='hist')

# 4. 회귀모델을 생성합니다.
import  statsmodels.formula.api  as  smf

model = smf.ols( formula = 'expenses ~ age + sex + bmi + children + smoker + region',
                        data= insurance )

# 5. 모델을 훈련시킵니다.
result = model.fit()

# 6. 분석결과를 확인합니다.
result.summary()

result.params   # 기울기 쪽만 출력 

sex[T.male]	-131.3520      --->  남성은 여성에 비해 매년 의료비가 131 달러 적게 들거라 예상

smoker[T.yes]	2.385e+04  = 2.386 x 10^4 = 23,860 달러
                                              ----> 흡연자는 비 흡연자보다 매년 의료비가 23,860 달러 비용이
                                                      더 적게든다.

age	              256.8392     ---> 나이가 일년씩 더 해질 때마다 평균적으로 의료비가 256 달러 더 든다
bmi                      339.289863  ---> 비만지수가 1증가할 때 마다 의료비가 339달러 더 든다.
children                 475.688916  --->  부양가족이 1명 늘어날때 마다 연간의료비가 475달러 더 든다. 

                                              지역별로는 북동지역이 북서, 남동, 넘서에 비해 의료비가 더 든다.

문제272. 비만인 사람은 의료비가 더 지출이 되는지 bmi30이라는 파생변수를 추가하고 
             다시 모델을 만들어서 결정계수가 올라가는지 확인하시오 !
            ( bmi 가 30 이상이면 1 , 아니면 0  )

def  func_1(x):
    if  x >= 30:
        return  1
    else:
        return  0

insurance['bmi30'] = insurance['bmi'].apply(func_1)

model2 = smf.ols( formula = 'expenses ~ age + sex + bmi + children + smoker + region + bmi30',
                        data= insurance )

# 5. 모델을 훈련시킵니다.
result2 = model2.fit()

# 6. 분석결과를 확인합니다.
result2.summary()  # 0.751 ----> 0.756  으로 올라갔습니다.                 45분까지 쉬세요 ~~


문제273. (오늘의 마지막 문제) 비만이면서 흡연까지 하게 되면 의료비가 더 증가하는지
             bmi30_yes 라는 파생변수를 추가해서 결정계수가 더 올라가는지 확인하시오 !


■ R 과 파이썬을 활용한 머신러닝 

        파이썬 문법
1장.   R 문법 
2장.   이 책을 보기 위한 기본문법
3장.   knn
4장.   naivebayes
5장.   decision  tree  +   oneR,  Jriper 
6장.   regression (단순회귀,다중회귀) + regression tree,  model  tree

▩ 회귀트리(regression  tree)  p289

1. 회귀트리란 ?      수치를 예측하는 트리(tree)

  머신러닝으로 구현하고 하는 목표 2가지 ?   1. 분류 : knn, naivebayes, decesion tree
                                                           2. 예측 (수치) : regression
 
수치예측                                                        미국민의 의료비 예측
  ↓
집값 <----   평수, 학군, ......

의료비 <----  부양가족수, 성별, 흡연여부, 나이, 비만지수, 사는지역


수치예측 작업을 할 때에는 일반적으로 전통적인 회귀분석 방법을 가장 먼저
선택하지만 경우에 따라서는 수치 의사결정트리가 분명한 이점을 제공합니다.

의사결정트리의 장점을 수치예측에 활용할 수 있는데 그 장점이 무엇인가?

  1. 작업의 특징이 많거나 특징과 결과간의 매우 복잡하고 비선형직인 관계를
     가질때 의사결정트리가 잘 맞습니다. 

    예: 독일은행 데이터의 경우에 컬럼이 복잡하게 많았습니다.
         정기적금, 예금통장의 금액, 적금을 부은 개월수, ......

 회귀트리 ----> 회귀 + 의사결정트리의 장점 

 회귀트리에서 사용하는 수학식?  SDR(표준편차축소) 를 사용합니다.

  의사결정트리 그림            vs           회귀트리 그림 


예제1.  책 292 페이지의 중간에 나오는 그림인 원본 데이터를 A 속성으로 
          나누는게 더 나은지 B 속성으로 나누는게 더 나은지 SDR 을 구해서
         알아내시오. 어떤게 더 잘 균일하게 나눈건지 확인하시오 !

#1. 원본 데이터를 만든다
tee <- c( 1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7 )

#2. 원본 데이터를 A 속성으로 나누었을때의 데이터
at1 <- c( 1, 1, 1, 2, 2, 3, 4, 5, 5 )
at2 <- c(6, 6, 7, 7, 7, 7 )

#3. 원본 데이터를 B 속성으로 나누었을때의 데이터
bt1 <- c(1, 1, 1, 2, 2, 3, 4 )
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7 )

#4. A 속성으로 나누었을때의 SDR (표준편차축소값) 을 구한다. 

sdr_a <- sd(tee) - ( length(at1) / length(tee) * sd(at1) +
                         length(at2) / length(tee) * sd(at2)  )

sdr_a  # 1.202815

#5. B 속성으로 나누었을때의 SDR(표준편차축소값) 을 구한다.

sdr_b <- sd(tee) - ( length(bt1) / length(tee) * sd(bt1) +
                         length(bt2) / length(tee) * sd(bt2)  )

sdr_b  # 1.392751
 
#6. 둘중에 SDR 이 높은것으로 분류한다. 

 둘중에 B 속성으로 나눈 SDR 이 더 높았습니다. 

#7. B 속성으로 분류한 원본 데이터의 두 영역의 평균값을 각각 구해서 등급을 예측한다

bt1 <- c(1, 1, 1, 2, 2, 3, 4 )
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7 )

mean(bt1)   #  2
mean(bt2)   #  6.25

정리:  x 값(속성, 명목형 데이터) 을 고려하지 않고 y값(수치)만 가지고 데이터를 분할하는데
       표준편차축고값이 가장 높은 값을 기준으로 y 값을 분할하고 각각의 영역의 평균값을
       예측값으로 지정합니다. 



▩ 책 292 페이지에 나온 내용을 파이썬으로 구현하기 

#1. 원본 데이터를 만든다
tee = [ 1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7 ]

#2. 원본 데이터를 A 속성으로 나누었을때의 데이터
at1 = [ 1, 1, 1, 2, 2, 3, 4, 5, 5 ]
at2 = [ 6, 6, 7, 7, 7, 7 ]

#3. 원본 데이터를 B 속성으로 나누었을때의 데이터
bt1 = [ 1, 1, 1, 2, 2, 3, 4 ]
bt2 = [ 5, 5, 6, 6, 7, 7, 7, 7 ]

#4. A 속성으로 나누었을때의 SDR (표준편차축소값) 을 구한다. 

import  numpy  as  np

np.std(tee,ddof=1) - (len(at1)/len(tee) * np.std(at1,ddof=1) + len(at2)/len(tee) * np.std(at2,ddof=1)) 

# 1.2028145680979165

설명:  ddof 는 델타 자유도 입니다.  이 값은 기본적으로 0 입니다.  표본으로 보고 자유도를
        1 을 주고 계산해야 R 과 동일한 결과가 출력됩니다. 

#5. B 속성으로 나누었을때의 SDR(표준편차축소값) 을 구한다.

import  numpy  as  np

np.std(tee,ddof=1) - (len(bt1)/len(tee) * np.std(bt1,ddof=1) + len(bt2)/len(tee) * np.std(bt2,ddof=1)) 

# 1.3927513935303917
 
#6. 둘중에 SDR 이 높은것으로 분류한다. 

 둘중에 B 속성으로 나눈 SDR 이 더 높았습니다. 

#7. B 속성으로 분류한 원본 데이터의 두 영역의 평균값을 각각 구해서 등급을 예측한다

bt1 = [ 1, 1, 1, 2, 2, 3, 4 ]
bt2 = [ 5, 5, 6, 6, 7, 7, 7, 7 ]

np.mean(bt1)   #  2
np.mean(bt2)   #  6.25

▩  다음의 데이터 프레임을 만들고 아래의 3개의 속성중 pattern, outline, dot 중에
     어느 속성으로 area 를 나눈게 더 좋은지 실험하기 

# 1. 아래의 데이터 프레임을 판다스로 구현한다

import pandas as pd

a = {'pattern' : [ '수직', '수직', '대각선', '수평', '수평', '수평', '수직', '수직', '대각선', '수평', '수직', '대각선', '대각선', '수평' ],
'outline' :[ '점선', '점선', '점선', '점선', '실선', '실선', '실선', '점선', '실선', '실선', '실선', '점선', '실선', '점선' ],
'dot' : [ '무', '유', '무', '무', '무', '유', '무' , '무', '유', '무', '유', '유', '무', '무' ],
'area' :[25, 30, 46, 45, 52, 23, 43, 35, 38, 46, 48, 52, 44, 30 ] }

df = pd.DataFrame(a)
df

# 2. pattern 속성으로 area를 나누었을 때의 SDR 을 구하시오 !

pat1 = list ( df.loc[ df['Pattern'] == '수직', 'area' ] )
pat2 = list ( df.loc[ df['Pattern'] == '수평', 'area' ] )
pat3 = list ( df.loc[ df['Pattern'] == '대각선', 'area' ] )
list_df = list(df['area'] )

sdr_pat = np.std( list_df, ddof = 1) - ( len(pat1) / len( list_df ) * np.std(pat1, ddof = 1)
+ len(pat2) / len( list_df ) * np.std(pat2, ddof = 1)
+ len(pat3) / len( list_df ) * np.std(pat3, ddof = 1) )
print( sdr_pat )         #  0.338370410662689

# 3. outline 속성으로 area를 나누었을 때의 SDR 을 구하시오 !
out1 = list ( df.loc[ df['Outline'] == '점선', 'area' ] )
out2 = list ( df.loc[ df['Outline'] == '실선', 'area' ] )
list_df = list(df['area'] )

sdr_out = np.std( list_df, ddof = 1) - ( len(out1) / len( list_df ) * np.std(out1, ddof = 1)
+ len(out2) / len( list_df ) * np.std(out2, ddof = 1) )

print( sdr_out )         #  -0.10086200041392779

# 4. dof 속성으로 area를 나누었을 때의 SDR 을 구하시오 !

dot1 = list ( df.loc[ df['Dot'] == '유', 'area' ] )
dot2 = list ( df.loc[ df['Dot'] == '무', 'area' ] )
list_df = list(df['area'] )

sdr_dot = np.std( list_df, ddof = 1) - ( len(dot1) / len( list_df ) * np.std(dot1, ddof = 1)
+ len(dot2) / len( list_df ) * np.std(dot2, ddof = 1) )
print( sdr_dot )         #  0.06950728283727692


▩ 와인 데이터의 등급(수치)을 예측하는 회귀트리 모델을 생성하는 실습 (p294)
   ( 데이터셋:  whitewines.csv )  

# 1. 데이터를 로드합니다.
wine <- read.csv("whitewines.csv")
head(wine)

#fixed.acidity       : 고정 산도
#volatile.acidity    : 휘발성 산도
#citric.acid         : 시트르산
#residual.sugar      : 잔류 설탕
#chlorides           : 염화물
#free.sulfur.dioxide : 자유 이산화황
#total.sulfur.dioxide: 총 이산화황
#density             : 밀도
#pH                  : pH
#sulphates           : 황산염
#alcohol             : 알코올
#quality             : 품질 <------------ 종속변수 입니다.

# 2. 종속변수인 quality 가 정규분포에 속하는 안정적인 데이터인지 확인합니다.
hist( wine$quality)

설명:  어느 한쪽으로 데이터가 치우치지 않은 안정적인 모양을 보이고 있습니다. 

# 3. 결측치가 있는지 확인합니다.
colSums(is.na(wine))

# 4. 훈련 데이터와 테스트 데이터로 데이터를 분리 합니다. ( 9대 1 )
library(caret)
set.seed(1)
train_num <-  createDataPartition( wine$quality, p=0.9, list=F)

train_data<- wine[ train_num,   ]
test_data <- wine[ -train_num,  ]

nrow(train_data)  # 4409
nrow(test_data)   # 489

# 5. 훈련 데이터로 모델을 생성합니다.
install.packages("rpart")
library(rpart)

model <-  rpart( quality ~  . ,  data= train_data) 
model 

 1) root 4409 3430.35200 5.877977                                  
   2) alcohol< 10.85 2769 1642.60700 5.603467      
     4) volatile.acidity>=0.2525 1449  697.05730 5.363009 *         
     5) volatile.acidity< 0.2525 1320  769.79920 5.867424         
      10) volatile.acidity>=0.2075 656  325.38870 5.708841 *
      11) volatile.acidity< 0.2075 664  411.61450 6.024096  
        22) residual.sugar< 12.65 530  294.09250 5.903774 *
        23) residual.sugar>=12.65 134   79.50000 6.500000 *
   3) alcohol>=10.85 1640 1226.78000 6.341463  
     6) free.sulfur.dioxide< 10.5 89   98.76404 5.370787 *
     7) free.sulfur.dioxide>=10.5 1551 1039.34800 6.397163  
      14) alcohol< 11.74167 752  482.56250 6.187500 *
      15) alcohol>=11.74167 799  492.61580 6.594493 *

설명:  * 표시가 있는 노드는 잎노드로 노드에서 예측이 이루어진다는 것을 의미합니다.
        와인 데이터의 예측 등급 입니다.

 quality 5.9 잎노드로 예를 들면 alcohol< 10.85 작고  volatile.acidity< 0.2525 으면서
  volatile.acidity< 0.2075 고  residual.sugar< 12.65 으면 이 와인의 quality 는 5.9 로 예상됩니다
  quality 가 3~9등급 사이로 구성되어져 있다. 

# 6. 생성된 모델을 시각화 합니다.
install.packages("rpart.plot")
library(rpart.plot)

rpart.plot( model, digits=3)
설명: digits= 3 은 소수점 세번째까지 허용하겠다.

# 7. 훈련된 모델로 테스트 데이터를 예측합니다. 

result <- predict( model,  test_data[  , -12])

# 8. 예측값과 실제값의 상관계수를 구하여 모델의 성능을 평가합니다. 

cor( result,  test_data[  , 12] )  # 0.51

# 9. 예측값과 실제값의 오차율을 확인하여 모델의 성능을 평가합니다. 

mae <-  function( actual, predicted) {  mean( abs( actual - predicted) )   } 
 # 실제값에서 예측값을 뺀 절대값들의 평균
                       
mae( result, test_data[  , 12] )  # 0.64

  상관계수는 1에 가까워야하고 오차는 0 에 가까워야 좋은 모델입니다. 

설명:  이 모델의 경우 다른 모델인 서포트 백터 머신에서는 오차가 0.45 인데
        0.64이면 상대적으로 큰 오차이므로 개선의 여지가 필요해 보입니다.

개선 방법 :  회귀 트리 -------------->  모델트리 

▩ 모델트리 p306

 ppt 그림

 기존회귀 트리 모델  + 다중회귀를 추가한 모델 

 회귀트리는 무조건 분할한 y(종속변수의 값들) 값들의 평균값으로만 예측을 했는데
 모델트리는 분할한 x 값과 y 값들에 대한 회귀식을 통해서 y 값을 예측합니다. 

# 1. 데이터를 로드합니다.
wine <- read.csv("whitewines.csv")

# 2. 와인 데이터 종속변수의 분포를 확인합니다.
hist(wine$quality)

# 3. 와인 데이터를 훈련과 테스트로 분리합니다.
library(caret)
set.seed(1)
train_num <-  createDataPartition( wine$quality, p=0.9, list=F)

train_data<- wine[ train_num,   ]
test_data <- wine[ -train_num,  ]

nrow(train_data)  # 4409
nrow(test_data)   # 489

# 4. 모델트리를 구현하기 위한 패키지를 설치 
install.packages("Cubist")
library(Cubist)

# 5. 와인의 품질을 예측하는 모델을 생성합니다.
model2 <-  cubist( x=train_data[  ,  -12],  y=train_data[  , 12] )
model2

# 6. 만든 모델로 테스트 데이터를 예측합니다.
result2 <- predict( model2, test_data[  , -12] ) 
result2

# 7. 실제값과 예측값간의 상관계수와 오차를 확인합니다. 

cor(  result2,  test_data[  , 12] )  # 0.5954519

mae( result2,  test_data[  , 12] )  # 0.5738158

설명: 회귀트리일 때는 오차가 0.64였는데 0.57이면 많이 개선되었습니다. 

▩ 위의 회귀트리를 파이썬으로 구현하기 

수치예측하는 의사결정트리를 이용하면 됩니다. 

# 1. 데이터를 로드합니다.
import  pandas  as  pd
wine = pd.read_csv("d:\\data\\whitewines.csv")
wine

# 2. 결측치가 있는지 확인합니다.
wine.isnull().sum()

# 3. 종속변수의 정규성을 확인합니다.
wine.quality.plot(kind='hist')

# 4. 훈련 데이터와 테스트 데이터를 분리합니다.  
from  sklearn.model_selection  import   train_test_split 

x = wine.iloc[ :  , :-1].to_numpy()
y = wine.iloc[ :  ,  -1].to_numpy()

x_train, x_test, y_train, y_test =  train_test_split( x, y , test_size=0.1, random_state=1)

print (x_train.shape )  # (4408, 11)
print (x_test.shape )   # (490, 11)
print (y_train.shape )  # (4408,)
print (y_test.shape )   # (490,)

# 5. 모델 생성
from  sklearn.tree  import  DecisionTreeRegressor

model = DecisionTreeRegressor()

# 6. 모델 훈련
model.fit(x_train, y_train)

# 7. 모델 예측
result = model.predict(x_test) 
result

# 8. 실제값과 예측값간의 상관계수와 오차를 확인합니다. 
import  numpy  as  np

np.corrcoef( result, y_test) 

array([[1.        , 0.57386954],
       [0.57386954, 1.        ]])

def  mae( x, y):
   return  np.mean( abs( x - y) )

print( mae(result, y_test) )  # 0.48775510204081635

설명:  의사결정트리 회귀모델로 수치예측결과 상관계수는 0.57, 오차는 0.48 로 출력됨
        위의 수치예측 모델의 성능을 올리시오 !

from  sklearn.tree  import  DecisionTreeRegressor        
                     ↓
from  sklearn.ensemble   import  RandomForestRegressor 

문제274.  RandomForestRegressor  로 변경해서 수치예측하고 상관계수와 오차를
             확인하시오 

# 5. 모델 생성
from  sklearn.ensemble   import  RandomForestRegressor 

model = RandomForestRegressor()

# 6. 모델 훈련
model.fit(x_train, y_train)

# 7. 모델 예측
result = model.predict(x_test) 
result

# 8. 실제값과 예측값간의 상관계수와 오차를 확인합니다. 

import  numpy  as  np

np.corrcoef( result, y_test) 

array([[1.        , 0.70963597],
       [0.70963597, 1.        ]])

def  mae( x, y):
   return  np.mean( abs( x - y) )

print( mae(result, y_test) )  # 0.436265306122449


6장에서 배운 내용 ?   1. 단순회귀분석
                             2. 다중회귀분석
                             3. 회귀트리 
                             4. 모델트리 

▩ 보스톤 집값(수치) 데이터를 예측하는 회귀 모델을 만드시오 !

1.  다중회귀 모델       

from  sklearn.linear_model  import  LinearRegression

2.  의사결정트리 회귀모델

from  sklearn.tree  import   DecisionTreeRegressor        

3.  랜덤포레스트 회귀모델 

from  sklearn.ensemble  import  RandomForestRegressor       

▩ 보스톤 집값 예측  회귀모델 만들기

#1. 데이터를 로드합니다.
boston = pd.read_csv("d:\\data\\boston.csv")
boston.head()

#2. 결측치를 확인합니다.
boston.isnull().sum()

#3. 종속변수가 정규성을 띄는지 확인합니다.
boston.price.plot(kind='hist')

#4. 훈련 데이터와 테스트 데이터로 분리합니다.
from  sklearn.model_selection  import  train_test_split

x = boston.iloc[ :  ,  :-1 ].to_numpy()
y = boston['price'].to_numpy()

x_train, x_test, y_train, y_test = train_test_split( x, y , test_size=0.1, random_state=1)

print(x_train.shape)  # (455, 14)
print(x_test.shape)   # (51, 14)
print(y_train.shape)  # (455,  )
print(y_test.shape)   # (51, )

#5. 회귀 모델을 생성합니다.
from  sklearn.linear_model  import  LinearRegression

model = LinearRegression()

#6. 모델을 훈련 시킵니다.
model.fit(x_train, y_train) 

#7. 테스트 데이터를 예측합니다.
result = model.predict(x_test)

#8. 실제값과 예측값의 상관계수와 오차를 확인합니다. 
import  numpy   as  np

print (np.corrcoef( result, y_test) )   # 0.888
print ( mae( result, y_test) )      # 3.7468

문제275.    LinearRegression 을  DecisionTreeRegressor  로 변경해서 실험하시오 !

LinearRegression         는 상관계수가 0.888, 오차가 3.7468 입니다. 
DecisionTreeRegressor 는 상관계수가 0.923 , 오차가 2.7176 입니다. 

문제276. 이번에는 RandomForestRegressor 로 실험하시오 !

from  sklearn.ensemble  import  RandomForestRegressor  

RandomForestRegressor 는 0.969 , 오차는 1.921 입니다.


▩ 오라클에서 SQL 로 바로 회귀분석을 구현하는 방법

  1. 회사의 중요한 데이터는 다 오라클과 같은 RDBMS 에 저장되어있습니다.

  2. 오라클과 파이썬 또는 R 과 연동해서 회귀분석을 하는 경우가 많은데
     연동하지 않고 바로 오라클의 회귀분석 패키지를 이용해서 분석을 하게 되면
     장점이 오라클의 자동 SQL 튜닝 기능을 이용할 수 있습니다.

 3. 대용량 데이터를 파이썬이나 R 로 회귀분석을 하다보면 아주 큰 대용량 데이터인 
    경우에는 모래시계가 뜨면서 분석시간이 상당히 오래걸리거나 메모리 부족 오류가
     나면서 분석을 못할 수 있습니다.

--■ 예제_193 SQL로 머신러닝 구현하기15(REGRESSION)

sqldeveloper 를 켜서 오라클에 접속합니다. 

-- 1.  학생점수 테이블을 생성합니다.

DROP TABLE STUDENT_SCORE;

CREATE TABLE STUDENT_SCORE
(  ST_ID        NUMBER(10),
  ACADEMIC   NUMBER(20,8),
  SPORTS      NUMBER(30,10),
  MUSIC       NUMBER(30,10),
 ACCEPTANCE  NUMBER(30,10) );

-- 데이터 입력: SQL Developer를 이용해서 student_score.csv 를 STUDENT_SCORE 
-- 테이블에 입력합니다.

select count(*) from STUDENT_SCORE;
-- 200 

-- 2. 훈련 데이터와 테스트 데이터로 분리합니다. 

--180건은 훈련 테이블로 구성
DROP TABLE STUDENT_SCORE_TRAINING; 

CREATE TABLE STUDENT_SCORE_TRAINING
AS
   SELECT *
     FROM STUDENT_SCORE
     WHERE ST_ID < 181;

DROP TABLE STUDENT_SCORE_TEST;

-- 20건은 테스트 테이블로 구성 
CREATE TABLE STUDENT_SCORE_TEST
AS
   SELECT *
     FROM STUDENT_SCORE
     WHERE ST_ID >= 181;

-- 3. 회귀 분석을 위한 머신러닝 모델 구성 테이블을 생성합니다. 

아래의 테이블에 회귀분석 환경에 대한 설정이름과 설정내용이 저장이 된다. 


DROP TABLE SETTINGS_REG1;

CREATE TABLE SETTINGS_REG1
AS
SELECT *
     FROM TABLE (DBMS_DATA_MINING.GET_DEFAULT_SETTINGS)
     WHERE SETTING_NAME LIKE '%GLM%';



---  위에서 만들었던 머신러닝 환경 셋팅 테이블에 지금부터 회귀분석하겠다는 내용을 저장한다. 
----  PREP_SCALE_RANGE  데이터를 알아서 표준화해서 회귀분석 해라 

BEGIN

INSERT INTO SETTINGS_REG1
  VALUES (DBMS_DATA_MINING.ALGO_NAME, 'ALGO_GENERALIZED_LINEAR_MODEL');

INSERT INTO SETTINGS_REG1
  VALUES (DBMS_DATA_MINING.PREP_SCALE_2DNUM, 'PREP_SCALE_RANGE');

COMMIT;

END;
/


-- 4.  회귀 모델을 생성합니다. 

-- 혹시 기존에 회귀모델 MD_REG_MODEL1 이 있으면 drop해라 ~

BEGIN
 DBMS_DATA_MINING.DROP_MODEL('MD_REG_MODEL1');
END;
/


BEGIN 
   DBMS_DATA_MINING.CREATE_MODEL(    
      MODEL_NAME            => 'MD_REG_MODEL1',  
      MINING_FUNCTION       => DBMS_DATA_MINING.REGRESSION,
      DATA_TABLE_NAME       => 'STUDENT_SCORE_TRAINING',
      CASE_ID_COLUMN_NAME   => 'ST_ID',
      TARGET_COLUMN_NAME    => 'ACCEPTANCE',
      SETTINGS_TABLE_NAME   => 'SETTINGS_REG1');
END;
/


-- 5. 모델 생성 여부를 확인합니다.

SELECT MODEL_NAME,
          ALGORITHM,
          MINING_FUNCTION
  FROM ALL_MINING_MODELS
  WHERE MODEL_NAME = 'MD_REG_MODEL1';


-- 6. 모델 구성 정보를 확인합니다.

SELECT SETTING_NAME, SETTING_VALUE
  FROM ALL_MINING_MODEL_SETTINGS
  WHERE MODEL_NAME = 'MD_REG_MODEL1';


-- 7. 테스트 데이터에 대해 회귀분석 모델이 예측한 예측점수를 확인합니다. 

SELECT ST_ID 학생번호, ACADEMIC 학과점수, ROUND(MUSIC,2) 음악점수 , 
          SPORTS 체육점수, ROUND(ACCEPTANCE,2) AS 실제점수, ROUND(MODEL_PREDICT_RESPONSE,2) AS 예측점수
 FROM ( 
           SELECT T.*, PREDICTION (MD_REG_MODEL1 USING *) MODEL_PREDICT_RESPONSE
             FROM STUDENT_SCORE_TEST T
      );


-- 8. 회귀 모델의 결정계수 R 스퀘어 값을 확인합니다. 


SELECT *
  FROM TABLE(DBMS_DATA_MINING.GET_MODEL_DETAILS_GLOBAL(MODEL_NAME =>  'MD_REG_MODEL1'))
  WHERE GLOBAL_DETAIL_NAME IN ('R_SQ','ADJUSTED_R_SQUARE');


-- 9. 입학점수에 영향력 있는 변수가 무엇인지 확인합니다. 

SELECT ATTRIBUTE_NAME, COEFFICIENT
  FROM TABLE (DBMS_DATA_MINING.GET_MODEL_DETAILS_GLM ('MD_REG_MODEL1'));

문제277.  위의 미국 입학점수 예측 모델이 예측한 결과값과 실제값과의 상관계수를
             출력하시오 !
      
select  corr(sal,comm)
  from emp;
      
SELECT  corr( ROUND(ACCEPTANCE,2) , ROUND(MODEL_PREDICT_RESPONSE,2) )
 FROM ( 
           SELECT T.*, PREDICTION (MD_REG_MODEL1 USING *) MODEL_PREDICT_RESPONSE
             FROM STUDENT_SCORE_TEST T
      );

0.9771

문제278. (오늘의 마지막 문제)  지금 방금 SQL 로 회귀분석한 미국 입학데이터
            student_score.csv 를 가지고 파이썬으로 위와 같이 회귀분석해서
            오라클의 상관계수값인 0.9771 을 능가할 수 있는지 실험하시오 !

from  sklearn.ensemble  import  RandomForestRegressor 

■ 7장. 신경망 

 머신러닝 종류 3가지 ?  1. 지도학습 : 분류: knn, naivebayes, decision tree, oneR, jriper, 신경망
                                                 회귀: 단순회귀, 다중회귀, 회귀트리, 모델트리 , 신경망
                                2. 비지도학습
                                3. 강화학습 

* 인공신경망을 만들어서 분류작업을 수행

 인공신경망을 만들면서 부터 육체노동을 대신했던 기계(컴퓨터)에게 지능을 부여하기 시작함
       ↓
 인공지능 신경망의 시초가 된것은 '퍼셉트론'  입니다.

 퍼셉트론은 1958년에 로젠플래트라는 분에의해 제안된 알고리즘 

  사람의 뇌 -------------------------------------> 컴퓨터를 이용한 지능처리

 2014년 딥마인드라는 회사를 구글이 4000 억원을 주고 인수하면서 인공신경망이 더 크게
 알려졌고 딥마인드가 2015년 2월에 딥러닝 2.0 버젼을 소개하면서 DQN(깊음 보상학습)
 을 알렸다.---------------->  알파고 

▩ 단층 퍼셉트론과 이후 딥러닝의 역사

 17세기의 라이프니츠 수학자 -------> 19세기 조지블      ------------------> 엘런튜링 수학자
   ( 이진법 )                                   (0과 1을 가지고 논리 연산자)           논리연산자를
                                                                                                   컴퓨터로 구현

---------------->  로젠블라트 1957 퍼셉트론(단층) ----->  20년후 -----> 오차역전파(다층)


------> 딥러닝 ------> 딥understanding


▩ 퍼셉트론(perceptron)   --> "뇌세포 하나를 수학적으로 재현함"
                 ----------
                     ↓
           지각하다, 인지하다        목표: 기계에게 지능을 부여하는게 가능할까?

   AND 게이트          OR 게이트               Nand  게이트             Xor 게이트

 x1   x2   y            x1   x2    y                x1    x2     y              x1   x2   y
 0     0    0            0     0    0                0     0      1               0    0    0
 1     0    0            1     0    1                1     0      1               1    0    1
 0     1    0            0     1    1                0     1      1               0    1    1
 1     1    1            1     1     1               1      1     0               1    1    0

▩ and 게이트를 R 로 구현하기

예제1. 아래의 행렬을 만들고  inputs 라는 변수에 넣으시오

 0   0
 1   0
 0   1
 1   1

답:  inputs = matrix( c(0, 0, 1, 0, 0, 1, 1, 1), nrow=4, ncol=2, byrow=T)
     inputs

예제2. 아래의 행렬을 만들고 targets 라는 변수에 담으시오 
  
 0
 0
 0 
 1

targets <-  matrix ( c( 0, 0, 0, 1), nrow=4, ncol=1, byrow=T)
targets

예제3.  아래의 가중치 행렬을 w 라는 이름으로 생성하시오 

 0.3
 0.4
 0.1

답:  w <- matrix( c( 0.3, 0.4, 0.1) , nrow=3)

예제4.  아래의 행렬을 만들고 x0 이라는 변수에 담으시오 !

 -1
 -1
 -1
 -1

답:  x0 <- matrix( c(-1, -1, -1, -1), nrow=4 )

예제5. 예제1 번에서 만들었던 inputs 와 x0 행렬을 cbind 를 이용해서 서로 붙여서
         new_inputs 라는 변수에 담으시오 

new_inputs <- cbind( x0, inputs)
new_inputs

예제6. new_inputs 행렬과 w 행렬과의 곱을 구하시오 !
   
     new_inputs                     w  

  -1    0    0                     0.3, 0.4, 0.1 
  -1    1    0
  -1    0    1
  -1    1    1

 -1 * 0.3 + 0 * 0.4 + 0 * 0.1  
 -1 * 0.3 + 1 * 0.4 + 0 * 0.1  
 -1 * 0.3 + 0 * 0.4 + 1 * 0.1  
 -1 * 0.3 + 1 * 0.4 + 1 * 0.1                  45분까지 쉬세요

 답:   k <- new_inputs %*% w
       k

공지사항 :   4단계 격상으로 인해서 다음주 월요일부터 2주동안 전면 비대면 수업
                으로 진행됩니다.   

               개인적 사정으로 학원에 나와야한다면 오늘중으로 저에게 애기해주세요


예제7.  아래의 함수를 만들고  입력값과 타겟을 입력매개변수로 받게하고 
          입력값과 가중치의 곱이 출력되게 하시오 !

inputs<-matrix(c(0,0,1,0,0,1,1,1),nrow=4,byrow=T)
target<-matrix(c(0,0,0,1),nrow=4)
x0 <- matrix( c(-1, -1, -1, -1), nrow=4 )
new_inputs <- cbind( x0, inputs)

and_pcn( new_inputs, target )

 -1 * 0.3 + 0 * 0.4 + 0 * 0.1  = -0.3
 -1 * 0.3 + 1 * 0.4 + 0 * 0.1  =  0.1
 -1 * 0.3 + 0 * 0.4 + 1 * 0.1  = -0.2
 -1 * 0.3 + 1 * 0.4 + 1 * 0.1  =  0.2

and_pcn <- function(ni, t ) {
      
   w<-matrix(c(0.3,0.4,0.1),nrow=3)
      
    for  ( i  in  ( 1:nrow(ni) )  )  {
 
           k <-  ni[ i,  ] %*%  w 
           print (k)
                                     }
      }

and_pcn( new_inputs, target )

예제8. 위의 k 값이 0 보다 크면 1, 0보다 작거나 같으면 0 이 출력되게 코드를 수정하시오

and_pcn( new_inputs, target )

   0
   1
   0
   1

and_pcn <- function(ni, t ) {
      
   w<-matrix(c(0.3,0.4,0.1),nrow=3)
      
    for  ( i  in  ( 1:nrow(ni) )  )  {
 
           k <-  ni[ i,  ] %*%  w 
           k_prime <- ifelse( k>0, 1, 0)
           print( k_prime )
                                     }
      }

and_pcn( new_inputs, target )

예제9.   위에서 출력된 k_prime 과 target 값과의 차이를 출력되게하시오 !


and_pcn( new_inputs, target )

  0  -  0   = 0
  0  -  1   =  -1
  0  -  0  =  0
  1  -  1  =  0

and_pcn <- function(ni, t ) {
      
   w<-matrix(c(0.3,0.4,0.1),nrow=3)
      
    for  ( i  in  ( 1:nrow(ni) )  )  {                         
 
           k <-  ni[ i,  ] %*%  w                                
           k_prime <- ifelse( k>0, 1, 0)                      
           cost <-  t[ i,  ] - k_prime
           print(cost)
                                     }
      }

and_pcn( new_inputs, target )

예제10.  비용(cost) 또는 오차가 0 이 아니면 가중치 w0, w1, w2 의 변화가 일어나겠금
            코드를 작성하시오 !

and_pcn <- function(ni, t ) {
      
   w <- matrix(c(0.3,0.4,0.1),nrow=3)
      
    for  ( i  in  ( 1:nrow(ni) )  )  {                         
 
           k <-  ni[ i,  ] %*%  w                                
           k_prime <- ifelse( k>0, 1, 0)                      
           cost <-  t[ i,  ] - k_prime
           if  ( cost != 0 ) {
                for  (  j  in  1:nrow(w) ) {
                         w[j] <- w[j] + 0.05 * ni[i,  j] * cost 
                                               }
                                }
                                     }
      }

and_pcn( new_inputs, target )


전체코드:

inputs<-matrix(c(0,0,1,0,0,1,1,1),nrow=4,byrow=T)
target<-matrix(c(0,0,0,1),nrow=4)
x0 <- matrix( c(-1, -1, -1, -1), nrow=4 )
new_inputs <- cbind( x0, inputs)

and_pcn <- function(ni, t ) {
  
  w <- matrix(c(0.3,0.4,0.1),nrow=3)
  
  for  ( i  in  ( 1:nrow(ni) )  )  {                         
    
    k <-  ni[ i,  ] %*%  w                                
    k_prime <- ifelse( k>0, 1, 0)                      
    cost <-  t[ i,  ] - k_prime
    if  ( cost != 0 ) {
      for  (  j  in  1:nrow(w) ) {
        w[j] <- w[j] + 0.05 * ni[i,  j] * cost 
      }
    }
  }
  print(w)
}

and_pcn( new_inputs, target )

예제11. 가중치의 변화가 없을때 위의 for 루프문으로 반복하는 작업이 중단되게하시오!

inputs<-matrix(c(0,0,1,0,0,1,1,1),nrow=4,byrow=T)
target<-matrix(c(0,0,0,1),nrow=4)
x0 <- matrix( c(-1, -1, -1, -1), nrow=4 )
new_inputs <- cbind( x0, inputs)

and_pcn <- function(ni, t ) {
  
  w <- matrix(c(0.3,0.4,0.1),nrow=3)

  cost = 1                # 아래의 while loop 문을 수행하기 위해 cost 에 1을 할당
  while ( cost != 0  )  {      #  cost 가 0 이 아닐동안에만 while loop 문을 돌려라 
    
    for  ( i  in  ( 1:nrow(ni) )  )  {        #  입력값의 갯수(4) 만큼 loop 문을 돌려서 
      k <-  ni[ i,  ] %*%  w                                
      k_prime <- ifelse( k>0, 1, 0)                      
      cost <-  t[ i,  ] - k_prime          # 비용(오차)이 발생하는지 확인합니다. 
    
    if  ( cost != 0 ) {                       # 만약에 비용(오차) 가 발생한다면 
      for  (  j  in  1:nrow(w) ) {          # 가중치의 갯수만 루프문을 돌려서 
        w[j] <- w[j] + 0.05 * ni[i,  j] * cost  # 가중치를 갱신합니다.
        
         }
      }
    }
  }
  print(w)
}

and_pcn( new_inputs, target )


문제279.  위의 코드를 수정해서 or_pcn 함수를 생성하고 변경된 최종 가중치가 
              출력되게하시오 !

or_pcn( new_inputs, target )
 

inputs<-matrix(c(0,0,1,0,0,1,1,1),nrow=4,byrow=T)
target<-matrix(c(0,1,1,1),nrow=4)
x0 <- matrix( c(-1, -1, -1, -1), nrow=4 )
new_inputs <- cbind( x0, inputs)

or_pcn <- function(ni, t ) {
  
  w <- matrix(c(0.3,0.4,0.1),nrow=3)

  cost = 1                # 아래의 while loop 문을 수행하기 위해 cost 에 1을 할당
  while ( cost != 0  )  {      #  cost 가 0 이 아닐동안에만 while loop 문을 돌려라 
    
    for  ( i  in  ( 1:nrow(ni) )  )  {        #  입력값의 갯수(4) 만큼 loop 문을 돌려서 
      k <-  ni[ i,  ] %*%  w                                
      k_prime <- ifelse( k>0, 1, 0)                      
      cost <-  t[ i,  ] - k_prime          # 비용(오차)이 발생하는지 확인합니다. 
    
    if  ( cost != 0 ) {                       # 만약에 비용(오차) 가 발생한다면 
      for  (  j  in  1:nrow(w) ) {          # 가중치의 갯수만 루프문을 돌려서 
        w[j] <- w[j] + 0.05 * ni[i,  j] * cost  # 가중치를 갱신합니다.
        
         }
      }
    }
  }
  print(w)
}

or_pcn( new_inputs, target )

     [,1]
[1,] 0.25
[2,] 0.40
[3,] 0.15

문제280.  단층 퍼셉트론의 not and  gate 함수를 아래와 같이 생성하시오 !

nand_pcn(new_inputs, target )            45분까지 쉬세요 ~~


inputs<-matrix(c(0,0,1,0,0,1,1,1),nrow=4,byrow=T)
target<-matrix(c(1,1,1,0),nrow=4)
x0 <- matrix( c(-1, -1, -1, -1), nrow=4 )
new_inputs <- cbind( x0, inputs)

nand_pcn <- function(ni, t ) {
  
  w <- matrix(c(0.3,0.4,0.1),nrow=3)

  cost = 1                # 아래의 while loop 문을 수행하기 위해 cost 에 1을 할당
  while ( cost != 0  )  {      #  cost 가 0 이 아닐동안에만 while loop 문을 돌려라 
    
    for  ( i  in  ( 1:nrow(ni) )  )  {        #  입력값의 갯수(4) 만큼 loop 문을 돌려서 
      k <-  ni[ i,  ] %*%  w                                
      k_prime <- ifelse( k>0, 1, 0)                      
      cost <-  t[ i,  ] - k_prime          # 비용(오차)이 발생하는지 확인합니다. 
    
    if  ( cost != 0 ) {                       # 만약에 비용(오차) 가 발생한다면 
      for  (  j  in  1:nrow(w) ) {          # 가중치의 갯수만 루프문을 돌려서 
        w[j] <- w[j] + 0.05 * ni[i,  j] * cost  # 가중치를 갱신합니다.
        
         }
      }
    }
  }
  print(w)
}

nand_pcn( new_inputs, target )
[1,] -0.15
[2,] -0.10
[3,] -0.10

▩ 파이썬으로 and 퍼셉트론 구현하기 

예제1.  아래의 행렬을 만들고 inputs 라는 변수에 넣으시오 !

 0  0
 1  0
 0  1
 1  1 

import  numpy  as  np 

a = [ 0, 0, 1, 0, 0, 1, 1, 1 ]
inputs = np.array(a).reshape(4,2)
inputs

예제2. 아래의 행렬을 만들고 targets 라는 변수에 담으시오 

 0
 0
 0 
 1

b = [ 0, 0, 0, 1 ]
targets = np.array(b).reshape(4,1)
targets

예제3. 가중치 행렬도 w 라는 이름으로 아래와 같이 생성하시오 

  0.3
  0.4
  0.1 

c = [ 0.3, 0.4, 0.1 ]
w = np.array(c).reshape(3,1)
w

예제4. 아래의 행렬을 만들고 x0 이라는 변수에 담으시오

 -1
 -1
 -1
 -1 

d = [ -1, -1, -1, -1 ]
x0 = np.array(d).reshape(4,1)
x0

예제5. 예제1번에 만들었던 inputs 와 x0 행렬을 서로 붙여서 new_inputs 라는 변수에
          담으시오 !

new_inputs = np.hstack((x0, inputs))
new_inputs

또는 

new_inputs = np.column_stack((x0, inputs))
new_inputs

예제6. new_inputs 와 w 행렬과의 곱을 구하시오 !

     new_inputs                     w  

  -1    0    0                     0.3, 0.4, 0.1 
  -1    1    0
  -1    0    1
  -1    1    1

 -1 * 0.3 + 0 * 0.4 + 0 * 0.1  = -0.3
 -1 * 0.3 + 1 * 0.4 + 0 * 0.1  = 0.1
 -1 * 0.3 + 0 * 0.4 + 1 * 0.1  = -0.2
 -1 * 0.3 + 1 * 0.4 + 1 * 0.1  = 0.2
 
np.dot( new_inputs, w )

예제7.  아래의 함수를 만들고 입력값과 가중치를  받아서 예제6번의 결과가
          출력되게하시오  (targets 는 함수안에 넣으세요)

and_pcn( new_inputs, w )

 -1 * 0.3 + 0 * 0.4 + 0 * 0.1  = -0.3
 -1 * 0.3 + 1 * 0.4 + 0 * 0.1  = 0.1
 -1 * 0.3 + 0 * 0.4 + 1 * 0.1  = -0.2
 -1 * 0.3 + 1 * 0.4 + 1 * 0.1  = 0.2

def and_pcn(ni, w):
    b = [ 0, 0, 0, 1 ]
    targets = np.array(b).reshape(4,1)
    return np.dot(ni,w)

and_pcn(new_inputs, w)

예제8.  위에서 출력된값이 0 보다 크면 1, 0보다 작거나 같으면 0 이 출력되게 코드를
          수정하시오 !

and_pcn(new_inputs, w)

 0
 1
 0
 1

밑바닥 부터 시작하는 딥러닝 책 69 페이지 (계단함수)

def  step_function(x):
    y = x > 0
    return  y.astype(np.int)  # True 이면 1 , False 이면 0 으로 출력

def and_pcn(ni, w):
    b = [ 0, 0, 0, 1 ]
    targets = np.array(b).reshape(4,1)
    c = np.dot(ni , w )
    d = step_function(c)
    return  d 

and_pcn( new_inputs,  w )

예제9. 위에서 출력된 결과(k_prime) 와 target 과의 차이를 출력되게 함수를 수정하시오

and_pcn( new_inputs,  w )

 0 - 0 = 0
 0 - 1 = -1
 0 - 0 =  0
 1 - 1 = 0

def and_pcn(ni, w):
    b = [ 0, 0, 0, 1 ]
    targets = np.array(b).reshape(4,1)
    c = np.dot(ni , w )
    d = step_function(c)
    return  targets - d 

and_pcn( new_inputs,  w )

예제10. 비용(cost) 또는 오차가 0 이 아니면 가중치  w0, w1, w2 의 변화가 일어나겠금
           코드를 작성하시오 !   가중치 3개가 출력되게하세요


 48분까지 쉬세요 ~

def and_pcn(ni, w):
    b = [ 0, 0, 0, 1 ]
    targets = np.array(b).reshape(4,1)
    for i in  range(len(ni)):  # 입력값의 갯수 만큼 루프문을 돌려서 
        c = np.dot(ni[i,:],w)
        d = step_function(c)
        cost = targets[i] - d  # 비용을 출력하고 
        if cost !=0 :             # 만약 그 비용이 0 이 아니라면 
            for j in range(len(w)):         # 가중치의 갯수만큼 루프문을 돌려서 
                w[i] = w[i] + 0.05 * ni[i, j] *cost  # 가중치를 갱신합니다.
    print(w)
    
and_pcn(new_inputs, w)

예제11. 더이상 가중치가 갱신되지 않으면 루프문을 멈추고 가중치가 출력되게하시오

def step_function(x):
    y = x > 0
    return y.astype(np.int)  # True 이면 1, False 이면 0 으로 출력

def and_pcn(ni, w):
    b = [ 0, 0, 0, 1 ]            # and perceptron 의 target 을 셋팅한다.
    targets = np.array(b).reshape(4,1)
    cost = 1                      # 아래의 while loop 문을 수행하기 위해 cost 에 1 할당 
    while cost != 0:             # cost 가 0 이 아닌경우에 실행문을 반복 실행하는데  
        for i in range(len(ni)):  # 입력값의 갯수 만큼 루프문을 돌려서 
            c = np.dot(ni[i,:],w)
            d = step_function(c)
            cost = targets[i] - d  # 비용을 출력하고 
            if cost !=0 :             # 만약 그 비용이 0 이 아니라면 
                for j in range(len(w)):         # 가중치의 갯수만큼 루프문을 돌려서 
                    w[j] = w[j] + 0.05 * ni[i, j] *cost  # 가중치를 갱신합니다.
    print(w)
    
and_pcn(new_inputs, w)
[[0.35]
 [0.35]
 [0.1 ]]

예제12. 위에서 구한 가중치를 이용해서 직선의 방정식을 그래프로 시각화 하시오!

x = np.linspace(-2, 2, 50) #  직선의 방정식에 제공된 x 값을 -2 에서 2까지를 x 라는 변수에 담습ㄴ다.

# x0*w0 + x1*w1 + x2*w2 = 0

y = -(x * w[1] / w[2]) + (w[0] / w[2]) # 최종 가중치의 직선의 방정식

fig = plt.figure() # 그림그리는 바탕을 fig로 만들겠다.
ax = fig.add_subplot(1, 1, 1)  #  1열 2행의 첫번째 그래프 그리겠다

# 그래프 모양과 색깔 설정부분
ax.spines['left'].set_position('zero')
ax.spines['bottom'].set_position('zero')
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

# 직선의 방정식 그래프 그리는 코드 
plt.plot(x, y, '-b', label='w1')

# x축과 y 축 눈금과 라벨을 지정하는 코드 
plt.xlim(-5, 5) # x 축의 눈금숫자의 시작과 마지막
plt.ylim(-5, 5) # y 축의 눈금숫자의 시작과 마지막
plt.title('Graph of w')
plt.xlabel('x', color='#1C2833') # x의 라벨 색깔
plt.ylabel('y', color='#1C2833') # y의 라벨 색깔

plt.legend(loc='upper left') # 레전드가 왼쪽에 나오게
plt.grid() # 격자모양출력
plt.show()

45분까지 쉬세요 ~~

완우 코드:
import numpy as np

a=[0,0,1,0,0,1,1,1]
inputs=np.array(a).reshape(4,2)

inputs

b=[0,0,0,1]

targets=np.array(b).reshape(4,1)
targets

c=[0.3,0.4,0.1]

w=np.array(c).reshape(3,1)
w

d=[-1]*4

x0=np.array(d).reshape(4,1)
x0

new_inputs=np.hstack((x0,inputs))
new_inputs

np.dot(new_inputs,w)

def step_function(x):
    y = x > 0
    return y.astype(np.int) # True 이면 1. False면 0으로 출력



def and_pcn(ni,w):
    b=[0,0,0,1]
    targets=np.array(b).reshape(4,1)
    cost=1
    cnt=0
    while cost != 0 :
        cnt+=1
        for i in range(len(ni)): #입력값의 갯수만큼 루프문을 돌려서
            c=np.dot(ni[i,:],w)
            d=step_function(c)
            cost=targets[i,:]-d
            if cost!=0:
                for j in range(len(w)):
                    w[j]=w[j]+0.05*ni[i,j]*cost
                    print(cost)
                    print(w)
    return cnt,w
    
and_pcn(new_inputs, w)

문제281.  (오늘의 마지막 문제)  처음에 주어진 가중치와 나중에 갱신된 가중치의 그래프가
            아래와 같이   같이 출력되게하시오 !

처음에 주어진 가중치 :  w = [ 0.3, 0.4, 0.1 ]
나중에 갱신된 가중치 :  w = [ 0.35, 0.35, 0.1 ]

5시 신호 보냈습니다.       앞으로 2주동안 비대면 수업인데 반에 와서 수업을 듣고 싶은 학생
                                   은 선우가 등록했고 3명 더 신청할 수 있는데 신청하고 싶은 학생은
                                  뎃글로 달아주세요(선착순입니다.)  오늘중으로 사무실에 애기해야합니다.





import numpy as np

a=[0,0,1,0,0,1,1,1]
inputs=np.array(a).reshape(4,2)

inputs

b=[0,0,0,1]

targets=np.array(b).reshape(4,1)
targets

c=[0.3,0.4,0.1]

w=np.array(c).reshape(3,1)
w

d=[-1]*4

x0=np.array(d).reshape(4,1)
x0

new_inputs=np.hstack((x0,inputs))
new_inputs

np.dot(new_inputs,w)

def step_function(x):
    y = x > 0
    return y.astype(np.int) # True 이면 1. False면 0으로 출력



def and_pcn(ni,w):
    b=[0,0,0,1]
    targets=np.array(b).reshape(4,1)
    cost=1
    cnt=0
    while cost != 0 :
        cnt+=1
        for i in range(len(ni)): #입력값의 갯수만큼 루프문을 돌려서
            c=np.dot(ni[i,:],w)
            d=step_function(c)
            cost=targets[i,:]-d
            if cost!=0:
                for j in range(len(w)):
                    w[j]=w[j]+0.05*ni[i,j]*cost
                    print(cost)
                    print(w)
    return cnt,w
    
and_pcn(new_inputs, w)


▩ R 과 파이썬을 활용한 머신러닝 수업 

머신러닝의 종류 3가지 ?  1. 지도학습  : 정답이 있는 데이터로 기계를 학습 시키는것

                                    - 분류:    knn --> 유클리드 거리
                                                naivebayes --> 나이브 베이즈 확률
                                               decision tree --> 정보획득량
                                               oneR, jiper  ---> 정보획득량 
                                               random forest --> 정보획득량
                                                신경망  ---> 퍼셉트론

                                    - 회귀:   단순회귀 ,다중회귀 ---> 최소제곱법
                                               회귀트리, 모델트리 ---> 표준편차축소
                                               신경망  ---> 퍼셉트론
                                 
                                  2. 비지도학습 : 정답이 없는 데이터로 기계를 학습 시키는것
                                  3. 강화학습  : 환경에 대한 정답을 스스로 알아내는 학습 방법 

▩ 활성화 함수의 종류  p318

1. 계단함수  :  입력신호의 총합이 임계치를 넘느냐 안넘느냐를 숫자 1과 0 으로 리턴하는 함수
                   입니다. 

 예:  f(0.3) = 1,  f(-0.2) = 0

2. 시그모이드 함수:  계단함수는 무조건 0 아니면 1을 리턴하지만 시그모이드 함수는
                           0~1 사이의 연속적인 실수값을 리턴합니다. 

단층 신경망(계단함수) :  입력층 -----------> 출력층
다층 신경망(시그모이드) :  입력층 -------> 은닉층들 -------> 출력층 

신경망을 단층이 아니라 다층 신경망을 사용하려면 활성화 함수를 sigmoid 함수를 
사용해야 합니다. 
                                                       1
시그모이드 함수 공식:  f(k) = -------------------------------- (비선형함수)
                                                 1 + exp(-k)
다층 신경망:                                                         딥러닝: 입력층 --> 은닉층들--> 출력층





단층 신경망:








3. 렐루함수  : 시그모이드 함수의 문제점 ?  기울기 소실 문제가 발생해서 신경망 학습이 
                                                        제대로 안되는 문제가 생김
                                                                     ↓
                                                        가중치가 제대로 갱신이 안되서 분류를 못한다.
                     
                                                        그래서 시그모이드 함수의 단점을 개선하고자
                                                        렐루 함수가 나왔습니다. 
   
                                                         Relu  함수 ( Rectified  Linear  unit )
                                                                             ↓
                                                                         정류된 ----> 전기회로 용어

                                                     시그모이드함수의 단점이 전파가 역전파 될때
                                                     기울기 소실로 인해서 전파가 앞층까지 안된다는 
                                                     단점이 있어서 나오게 된 함수 입니다. 

                                                     렐루 함수는 입력값이 0 보다 크면 그 값을 그대로 
                                                      출력하고 0 보다 작거나 같으면 0으로 출력하는 함수

예제1.  활성화 함수인 계단함수를 R 로 생성하고 계단 함수 그래프를 그리시오

step <- function(x) {  ifelse( x>0, 1, 0) }  
step(-0.1)  # 0
step(1.4)   # 1 

x <- seq(-5, 5, 0.01)  #  -5 에서 5까지 0.01 간격으로 숫자들을 출력
x
plot(x,  step(x), col='blue', type='o' )

문제282.  이번에는 파이썬으로 계단 함수를 만들고 계단함수 그래프를 그리시오 !
             ( 비대면 수업의 집중을 높이기 위해 카페에 올려주세요~)

딥러닝 책 (p70~71)

import  numpy  as  np
import  matplotlib.pylab  as  plt

def  step_function(x):
    return  np.array( x>0, dtype=np.int )  # 입력되는 값이 0 보다 크면 true 이므로 1이 출력됨
                                                 # 입력되는 값이 0 보다 작으면 False 이므로 0 이 출력됨

x = np.arange(-5.0, 5.0, 0.1)  # -5.0 ~ 5.0 까지 0.1 간격으로 숫자들을 출력 
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)  #  y 축의 범위 지정 
plt.show()

예제2. 이번에는 시그모이드 함수를 R 로 생성하시오 !

sigmoid  =  function(x) {  1  / ( 1 + exp(-x) ) }

sigmoid(3)  #  0.9525741
sigmoid(2)  # 0.8807971

예제3. 위의 시그모이드 함수를 이용해서 그래프를 그리시오

x <- seq( -5,  5, 0.01 )  
plot( x, sigmoid(x), col='blue') 

문제283.  이번에는 파이썬으로 시그모이드 함수를 만들고 그래프를 그리시오 !
             ( 온라인 수업의 집중을 높이기 위해서 카페에 올려주세요 ~) 

def sigmoid(x):
    import numpy as np
    return 1/(1+np.exp(-x))

x=np.arange(-5.0,5.0,0.01)
y=sigmoid(x)

plt.plot(x,y)
plt.ylim(-0.1,1.1)
plt.show()

예제4. 렐루함수를 R 로 생성하시오 !

입력되는 값이 0 이거나 음수이면 0 을 출력하고 0 보다 크면 입력되는 값을 그대로 출력하는 함수

relu <- function(x) { ifelse( x>0, x, 0) }
relu(4)
relu(-3) 

예제5. 위의 렐루 함수를 그래프로 그리시오 !

 plot( x, relu(x), col='blue')

문제284. 위의 렐루함수를 파이썬으로 생성하고 그래프로 그리시오 !
             ( 온라인 수업의 집중을 높이기 위해서 카페에 올리세요) 딥러닝 책 p77 페이지

입력되는 값이 0 이거나 음수이면 0 을 출력하고 0 보다 크면 입력되는 값을 그대로 출력하는 함수

import  numpy  as  np
import  matplotlib.pylab  as  plt

def  relu(x):
    return  np.maximum(0, x) #  0 과 x 중에 최대값을 출력해라 ~

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)
plt.plot(x, y)                                          
plt.ylim(-0.1, 5)
plt.show()

▩ 신경망 실습1 ( 콘크리트 강도를 예측하는 신경망 만들기 )  p328

" 어떻게 재료를 조합해야 강도가 높은 콘크리트를 만들 수 있는가 ? "

" 콘크리트의 강도를 예측하는 신경망 만드는 실습 "

자갈, 모래, 시멘트 등을 몇 대 몇 비율로 섞었을때 어느정도 강도가 나오는지
예측하는 신경망 

신경망으로는 분류도 할 수 있고 수치예측도 할 수 있는데 이번 실습은 수치예측입니다.

* 콘크리트 데이터 소개(concreat.csv)

 1. mount of  cement  :  콘크리트의 총량
 2. slag  :   시멘트
 3. ash  :    분(시멘트)
 4. water :  물 
 5. superplasticizer : 고성능 감수제(콘크리트의 강도를 높이는 첨가제)
 6. coarse  aggregate : 굵은 자갈
 7. fine  aggregate  : 잔 자갈
 8. aging time   : 숙성 시간
 9. strength :  강도  ( 정답라벨)

# 1. 데이터를 로드합니다.
concrete <- read.csv("concrete.csv")
str(concrete)

nrow(concrete) # 1030
ncol(concrete)  # 9

# 2. 결측치가 있는지 확인합니다.
colSums( is.na(concrete) )

# 3. 이상치가 있는지 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("concrete.csv")

for (i in 1:length(colnames(wisc))){

  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )

}

# 4. 데이터를 정규화 합니다. ( 0 ~ 1사이로 데이터를 변환)

normalize <- function (x) {
                                   return  ( ( x - min(x) ) / ( max(x) - min(x) ) )
                                  }

concrete_norm <- as.data.frame(lapply( concrete, normalize) ) 
summary(concrete_norm)

# 5. 훈련 데이터와 테스트 데이터로 분리합니다.( 8:2 )
library(caret)
set.seed(1)
k <- createDataPartition( concrete_norm$strength, p=0.8, list=F)

train_data <- concrete_norm[ k , ]
test_data <- concrete_norm[ -k,  ]

nrow(train_data) # 826
nrow(test_data)  # 204

# 6. 모델설정
install.packages("neuralnet")
library(neuralnet)

# 7. 훈련 데이터로 모델생성
concreate_model <-  neuralnet( formula=strength ~ cement + slag + ash + water + 
                                                       superplastic + coarseagg + fineagg + age,
                                          data = train_data )

# 신경망 시각화
plot( concreate_model)

# 8. 테스트 데이터를 예측

result <- compute( concreate_model,  test_data[  ,1:8] )
result$net.result

# 9. 모델 성능 평가
cor( result$net.result,  test_data[  , 9] )  # 0.8245659

설명: 분류이면 이원교차표를 통해서 정확도를 확인할텐데 수치예측이므로 상관계수로
       모델의 성능을 체크해야 합니다. 

# 10. 모델 성능 개선 
위의 신경망은 2층 신경망 이었습니다.  그래서 개선방법은 3층으로 신경망을 늘리고
은닉층의 뉴런의 갯수도 1개에서  7개로 늘려봅니다.

concreate_model2 <-  neuralnet( formula=strength ~ cement + slag + ash + water + 
                                                        superplastic + coarseagg + fineagg + age,
                                           data = train_data,  hidden=c(5,2) )

※ 설명:  hidden = c( 5,     2) 
                           ↓     ↓
          은닉1층의 뉴런수  은닉2층의 뉴런수 

plot( concreate_model2) 

result2 <- compute( concreate_model2,  test_data[  , 1:8] )
cor ( result2$net.result,  test_data[  , 9] )  # 0.927863

문제285.  위의 신경망의 성능을 더 올리시오 ~

set.seed(1) # 항상 일정한 상관계수를 보기 위해서 모델 생성전에 설정합니다.
concreate_model3 <-  neuralnet( formula=strength ~ cement + slag + ash + water + 
                                                        superplastic + coarseagg + fineagg + age,
                                           data = train_data,  hidden=c(5,5,2) )

#※ 설명:  hidden = c( 5,   5,   2) 
     
plot( concreate_model3) 

result3 <- compute( concreate_model3,  test_data[  , 1:8] )
cor ( result3$net.result,  test_data[  , 9] )  # 0.9551868

문제286. 보스톤 집값 데이터를 R 로 로드하고 찰스강의 경계에 위치한 집값의 평균값과 찰스강의 경계에
           위치하지 않은 집값의 평균값 출력하시오 !  ( boston.csv )
           chas 를 출력하고 chas 별 집값(medv)의 토탈값을 출력하시오 !

힌트:  x2 <- aggregate( sal~job, emp, sum ) # 직업별 토탈월급을 세로로 출력 

boston <- read.csv("boston.csv")

aggregate(price~CHAS, boston, mean)

 CHAS    price
1    0    22.09384
2    1    28.44000

※ 데이터 설명

 [01]  CRIM	자치시(town) 별 1인당 범죄율
 [02]  ZN	             25,000 평방피트를 초과하는 거주지역의 비율
 [03]  INDUS	비소매상업지역이 점유하고 있는 토지의 비율
 [04]  CHAS	찰스강에 대한 더미변수(강의 경계에 위치한 경우는 1, 아니면 0)
 [05]  NOX	10ppm 당 농축 일산화질소
 [06]  RM	             주택 1가구당 평균 방의 개수
 [07]  AGE	1940년 이전에 건축된 소유주택의 비율
 [08]  DIS	             5개의 보스턴 직업센터까지의 접근성 지수
 [09]  RAD	방사형 도로까지의 접근성 지수
 [10]  TAX	10,000 달러 당 재산세율
 [11]  PTRATIO	자치시(town)별 학생/교사 비율
 [12]  B	1000(Bk-0.63)^2, 여기서 Bk는 자치시별 흑인의 비율을 말함.
 [13]  LSTAT	모집단의 하위계층의 비율(%)
 [14]  MEDV	본인 소유의 주택가격(중앙값) (단위: $1,000)

문제287. 오라클의 ntile 과 같은 함수와 같이 등급을 나누는 함수가 R 에도 있는지
             확인하고 방의 갯수의 비율(RM)을 4등급으로 나누시오

boston <- read.csv("boston.csv")
library(dplyr) 
boston$rm_grade <-  ntile(boston$RM, 4)
head(boston)
aggregate( price ~ rm_grade, boston, mean)

  rm_grade    price
1        4 32.36825
2        3 21.41508
3        2 19.26378
4        1 17.15276

방의 갯수가 많을수록 집값이 비싼것을 확인할 수 있습니다 

문제288. 하위계층의 비율이 높을 수록 집값이 저렴한지 확인해보기 위해서 하위계층의 비율을
            4개의 grade 로 나눈 파생변수를 lstat_grade 로 추가하시오 !

library(dplyr) 
boston$lstat_grade  <-  ntile(boston$LSTAT, 4)
head(boston)

문제289.  하위계층의 비율의 등급, 그 등급별 집값의 평균값을 출력하시오 !

aggregate( price ~ lstat_grade , boston, mean)

하위계층의 비율이 높을 수록 집값이 떨어지고 있습니다. 

문제290.  오래된 집(age) 일 수록 직업센터의 접근성 지수(dis)가 떨어지는지 확인하기 위하여
             age 에 대한 등급을 4등급으로 나눠서  age_grade 라는 파생변수를 추가하시오 !

library(dplyr) 
boston$age_grade  <-  ntile(boston$AGE, 4)
head(boston)

문제291.  age_grade 를 출력하고 age_grade 별 접근성 지수(dis) 값의 평균값을 출력하시오 !

aggregate( DIS ~ age_grade , boston, mean)

  age_grade      DIS
1         1     6.049286
2         2     4.270391
3         3     2.743161
4         4     2.095669

등급이 높을 수록 접근성 지수가 떨어지고 있습니다.  오래된 집일 수록 직장과의 거리가 멀다.

문제292.  공장지대(INDUS) 일수록 일산화질소(NOX) 가 높은지 확인하기 위해 
             공장지대 비율을 4등급으로 나누는 indus_grade 라는 파생변수를 추가하시오 !

library(dplyr) 
boston$indus_grade  <-  ntile(boston$INDUS, 4)
head(boston)

문제293. indus_grade 를 출력하고 indus_grade 별 일산화질소(NOX) 의 평균값을 출력하시오 !

aggregate( NOX ~ indus_grade, boston, mean)

 indus_grade       NOX
1           1 0.4608969
2           2 0.4914866
3           3 0.5974127
4           4 0.6702302

문제294.  일산화질소(NOX) 가 높을 수록 집값이 떨어지는지 확인해보기위해서 
             일산화질소(NOX) 에 대한 등급을 4등급으로 나누는 파생변수를 nox_grade 라는
             이름으로 추가하시오 !

library(dplyr) 
boston$nox_grade  <-  ntile(boston$NOX, 4)
head(boston)

문제295. nox_grade 를 출력하고 nox_grade 별 집값의 평균값을 출력하시오 !

aggregate( price ~ nox_grade, boston, mean )

  nox_grade    price
1         1   27.10866
2         2   25.32992
3         3   20.25476
4         4   17.37937

공해가 많을 수록 집값이 떨어지고 있습니다. 

지금까지 만든 파생변수 생성 스크립트 5개를 한군데 모으시오 !

boston$rm_grade <-  ntile(boston$RM, 4)
boston$lstat_grade  <-  ntile(boston$LSTAT, 4)
boston$age_grade  <-  ntile(boston$AGE, 4)
boston$indus_grade  <-  ntile(boston$INDUS, 4)
boston$nox_grade  <-  ntile(boston$NOX, 4)

다시 새롭게 boston 데이터를 로드하시오 !

boston <- read.csv("boston.csv")
head(boston)

▩ boston 집값을 예측하는 인공신경망 모델 만들기 

#1. 데이터를 로드합니다.
boston <- read.csv("boston.csv")

#2. 결측치를 확인합니다.
colSums(is.na(boston))

#3. 이상치를 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("boston.csv")

for (i in 1:length(colnames(wisc))){

  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )

}

#4. 데이터를 정규화 시킵니다.

normalize <- function(x) {
                                     return ( (x-min(x) ) /  ( max(x) - min(x)) )
                                }

boston_normalize <- as.data.frame( lapply( boston, normalize) ) 
summary(boston_normalize)

#5. 훈련데이터와 테스트 데이터로 데이터를 분리합니다. (9:1)
library(caret)
set.seed(1)
train_num <-  createDataPartition( boston_normalize$price, p=0.9, list=F)
train_data <-  boston_normalize[ train_num,  ] #훈련 데이터 90%
test_data <-  boston_normalize[ -train_num,  ] #테스트 데이터 10%
nrow(train_data) #  458
nrow(test_data)  #  48 

#6. 인공신경망 모델을 설정합니다.

library(neuralnet)

#7. 훈련 데이터로 인공신경망 모델을 학습 시킵니다.  
boston_model <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                                                 + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT ,
                                        data= train_data)
plot(boston_model) 

#8. 테스트 데이터를 예측합니다.

library(neuralnet) 
boston_result <- neuralnet::compute( boston_model , test_data[  , 2:14] )

# 설명: compute 가 dplyr 의 패키지에도 compute 가 있어서 neuralnet 과 충돌이 되어서
# 그냥 compute 만 했을때 자꾸 실행안되면서 오류가 나서 neuralnet::compute 해줘야
# neuralnet 의 compute 를 써라 ~ 라고 명령을 하는것입니다. 

#9. 모델의 성능을 확인합니다.                                     

cor( boston_result$net.result, test_data[  , 15] )  #  0.8305972

#10. 모델의 성능을 더 올립니다. (하이퍼 파라미터 조절로 올리는 시도)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                                                 + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT ,
                                        data= train_data, hidden=c(5,2) )
plot(boston_model2) 

library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[  , 2:14] )

#11. 모델의 성능을 더 올립니다. ( 파생변수 추가로 올리는 시도 )

cor( boston_result2$net.result, test_data[  , 15] )  #  0.8775291

                              set.seed(1)  , hidden(5, 5, 2)    # 0.9180599   -- 선우 제공
                              set.seed(1),  hidden=c(11,6,2)  # 0.94           -- 완우 제공 

문제296. (오늘의 마지막 문제) 아까 위에서 만들었던 파생변수 5개를 하나씩 사용해서
            파생변수 추가로 모델의 성능을 더 올릴수 있는지 확인하시오 !
            파생변수는 언제 추가하냐면 #1번.데이터 로드하고 나서 바로 하세요 ~
            한번에 5개를 다 추가하지 마시고 한번에 하나씩만 추가해서  성능이 더 올라가는지
            확인하세요 ~

           파생변수 추가                                                                 성능 
boston$rm_grade <-  ntile(boston$RM, 4)                                          ?
boston$lstat_grade  <-  ntile(boston$LSTAT, 4)                                   ?
boston$age_grade  <-  ntile(boston$AGE, 4)                                       ?
boston$indus_grade  <-  ntile(boston$INDUS, 4)                                 ?
boston$nox_grade  <-  ntile(boston$NOX, 4)                                      ?

▩ 신경망 과적합 여부 확인 테스트 

과적합(overfitting) ?  훈련 데이터에 대해서는 정확도가 좋은데 상대적으로 테스트 데이터에 
                            대해서는 정확도가 떨어지는 현상

훈련 데이터 ---> 모델 ---> 훈련

훈련 데이터 ---> 모델 ---->  정확도
테스트 데이터 ---> 모델 ---> 정확도 

실습:

# 1. 데이터 로드
boston <- read.csv("boston.csv")

# 2. 파생변수 추가 
boston$nox_grade <- ntile( boston$NOX , 4 ) 

# 3. 정규화 진행 
normalize <- function (x) { return ( ( x - min(x) ) / ( max(x) - min(x) ) ) }
boston_norm <- as.data.frame( lapply(boston, normalize) )

# 4. 훈련 데이터와 테스트 데이터를 분리 
library(caret)
set.seed(1)
k <- createDataPartition( boston_norm$price, p = 0.9, list = F )
train_data <- boston_norm[ k , ]
test_data <- boston_norm[ -k , ]

#5. 모델 생성 
library(neuralnet)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                            + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT +nox_grade,
                            data= train_data, hidden=c(5,4) )

# 6. 훈련 데이터의 정확도 확인 
boston_result1 <- neuralnet::compute( boston_model2 , train_data[ ,c(2:14,16) ])

cor( boston_result1$net.result, train_data[  , 15] )  #  0.9771174

# 7. 테스트 데이터의 정확도 확인 
library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[ ,c(2:14,16) ])

cor( boston_result2$net.result, test_data[  , 15] )  # 0.8612561

설명:  훈련 데이터의 정확도는 0.97 인데 테스트 데이터의 정확도는 0.86 이어서 
         오버피팅이 발생하고 있습니다. 

 오버피팅을 해결하는 방법?    1. 드럽아웃
                                         2. 아주 큰 가중치의 영향력을 감소 시키는 방법 

R 신경망 패키는 우리가 만든게 아니라 남이 만는거라서 오버피팅을 해결하는 코드를 구현할 수 가
없고 딥러닝 수업때 파이썬으로 신경망을 만들면서 드롭아웃, 큰 가중치의 영향렬을 감소 시키는
방법을 구현해 볼것 입니다. ( 딥러닝책 6장 p189 페이지)


신경망 모형을 만들때 크게 신경써야하는 2가지 ?  1. 언더피팅 문제 해결:
                                                                                    - 하이퍼 파라미터를 조정
                                                                                    - 파생변수를 추가

                                                                   2. 오버피팅 문제 해결 
                                                                                    - 드룹아웃
                                                                                    - 배치정규화
                                                                                    - 가중치감소(L2정규화,L4정규화)
                                                        


▩ 신경망 모델 성능 개선 (P336) 

  1. 하이퍼 파라미터 :   

                 -  hidden :   신경망의 층수와 노드(뉴런)수를 조정하는 파라미터 

                   ※ 뉴런 ? 뇌세포 하나 

                       1. 사람 :  850억개 
                       2. 고양이: 10억개
                       3. 쥐 : 7천 5백만개 
                       4. 바퀴벌래: 몇백만개
                       5. 하루살이:  지금 현재까지 나온 최점단 인공지능의 뉴런수보다 많다.

  p338       - 활성함수의 선택 :  계단함수, 시그모이드, 렐루 

예제1.  책 339 페이지 나온데로 softplus 함수를 만들고 어제 마지막 문제의 코드에
          신경망 생성 부분에 적용해서 적확도가 더 올라가는지 확인한다. 

softplus <- function(x) { log( 1 + exp(x))  }


# 1. 데이터 로드
boston <- read.csv("boston.csv")

# 2. 파생변수 추가 
boston$nox_grade <- ntile( boston$NOX , 4 ) 

# 3. 정규화 진행 
normalize <- function (x) { return ( ( x - min(x) ) / ( max(x) - min(x) ) ) }
boston_norm <- as.data.frame( lapply(boston, normalize) )

# 4. 훈련 데이터와 테스트 데이터를 분리 
library(caret)
set.seed(1)
k <- createDataPartition( boston_norm$price, p = 0.9, list = F )
train_data <- boston_norm[ k , ]
test_data <- boston_norm[ -k , ]

#5. 모델 생성 
library(neuralnet)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                            + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT +nox_grade,
                            data= train_data, hidden=c(5,4), act.fct=softplus )

# 6. 훈련 데이터의 정확도 확인 
boston_result1 <- neuralnet::compute( boston_model2 , train_data[ ,c(2:14,16) ])

cor( boston_result1$net.result, train_data[  , 15] )  #  0.9771174

# 7. 테스트 데이터의 정확도 확인 
library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[ ,c(2:14,16) ])

cor( boston_result2$net.result, test_data[  , 15] )  # 0.8612561


* 언더피팅을 해결하는 방법  2가지 ?  1.  하이퍼 파라미터 조절
                                                  - hidden
                                                   - act.fct 
                                                   - learningrate.factor = list(minus = 0.5, plus = 1.2)
                              
  러닝레이트(learning  rate) : 학습률    Wi = Wi + 0.05 * x1 * 오차                                                  

                                                 2.  파생변수 추가 

예제2.  예제1번의 신경망에서 러닝 레이트를 0.01 ~ 1.0 까지를 주고 학습하고 정확도를 확인
          하세요!


# 1. 데이터 로드
boston <- read.csv("boston.csv")

# 2. 파생변수 추가 
boston$nox_grade <- ntile( boston$NOX , 4 ) 

# 3. 정규화 진행 
normalize <- function (x) { return ( ( x - min(x) ) / ( max(x) - min(x) ) ) }
boston_norm <- as.data.frame( lapply(boston, normalize) )

# 4. 훈련 데이터와 테스트 데이터를 분리 
library(caret)
set.seed(1)
k <- createDataPartition( boston_norm$price, p = 0.9, list = F )
train_data <- boston_norm[ k , ]
test_data <- boston_norm[ -k , ]

#5. 모델 생성 
library(neuralnet)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                            + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT +nox_grade,
                            data= train_data, hidden=c(5,4), act.fct=softplus,
                            learningrate.factor = list(minus = 0.01, plus = 1.0) )

# 6. 훈련 데이터의 정확도 확인 
boston_result1 <- neuralnet::compute( boston_model2 , train_data[ ,c(2:14,16) ])

cor( boston_result1$net.result, train_data[  , 15] )  #  0.9771174

# 7. 테스트 데이터의 정확도 확인 
library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[ ,c(2:14,16) ])

cor( boston_result2$net.result, test_data[  , 15] )  # 0.8612561

하이퍼 파라미터는 위의 3가지를 조정해서 모델의 성능을 올리면 됩니다. 
사람이 직접 입력하고 실험을 해줘야하므로 번거로워지므로 이를 개선하기 위해서 
나온게  바로  아래의 2가지 입니다.

R 에서는 caret  패키지가 자동으로 최적의 하이퍼 파라미터를 알아내겠금 해줍니다. 
파이썬에서는 grid search 가 있어서 자동으로 최적의 하이퍼 파라미터의 조합을 발견해 줍니다. 

* 신경망 모델의 정확도를 높이기 위한 방법 2가지 ?
                                                                     1. 하이퍼 파라미터 조절
                                                                     2. 파생변수 추가 

▩ 보스톤 집값을 예측하는 신경망의 정확도를 높이기 위해서 파생변수 5가지를 추가 

1. 방의 갯수가 많으면 집값이 오를까?
2. 하위층의 비율이 높으면 집값이 떨어질까?
3. 집의 노후화 정도가 높으면 집값이 떨어질까?
4. 직장과의 거리가 가까우면 집값이 높을까?
5. 공해가 많으면 집값이 떨어질까 ?

위의 질문은 어떻게 생각했는가?   이러한 질문을 생각해내기위해 도움을 줄 수 있는 방법은 ?

상관관계 그래프를 보면 위의 질문을 생각해는데 도움을 받을 수 있습니다. 

예제1.  책 275 페이지에서 사용했던 pairs.panel()  함수를 이용해서 보스톤 집값의 상관관계를
          확인합니다. 

boston <- read.csv("boston.csv")

library(psych)
pairs.panels( boston[   , -1] )

예제2.  corrplot 패키지를 이용해서 상관관계를 확인합니다.

install.packages("corrplot")
library(corrplot)
boston_cor <-  cor( boston[  , -1] )
boston_cor


질문1: 미국도 학군이 높으면 집값이 비쌀까?
질문2: 세금이 높은 지역일수록 집값이 낮아질까 ?
질문3. 하위계층의 비율이 높을 수록 집값이 떨어질까?
질문4. 방의 갯수가 많을수록 집값이 높아질까?
질문5. 직장과의 거리가 가까울 수록 집값이 높아질까?
질문6. 공해가 심할 수록 집값이 떨어질까?
질문7. 집의 노후화가 심할수록 집값이 떨어질까?
질문8. 범죄율이 높을 수록 집값이 떨어질까?

위의 질문들은 상관계수를 보고서 알아낸것이고 위의 질문들에 대한 파생변수를 생성해서
신경망 모델의 성능을 높이는 사용하면 됩니다. 

corrplot( boston_cor,  method="circle")

설명: 남색과 빨간색으로 시각화를 하는데 남색에 가까우면 양의 상관관계가 높고 
       빨간색으로 진해지면 음의 상관관계가 높습니다. 원의 크기가 클수록 상관계수가 크다는
       의미입니다.

예제3. 위의 그래프를 보고 종속변수(price) 와 독립변수들과의 상관관계가 아닌
         독립변수들 끼리의 상관관계를 확인해서 상관계수가 높은것을 확인하고
         새로운 질문을 생성해보시오 ~ (카페에 답글로 올려주세요 ~)

예제:  공장지대(indus) 일 수록 일산화질소(nox) 가 높다

김영균: 방사형 도로에서 가까울수록(RAD) 범죄율이 높다(CRIM)
양남휘: 방사형 도로까지의 접근성지수(RAD)가 높을수록 세금(TAX)이 높다.
박건우: 일산화질소의 농도가 높을수록 직업센터까지의 접근성이 떨어지는가?
박건우: 비소매상업지역이 적을수록 재산세를 적게 내는가 ?
박건우: 비소매상업지역이 적을수록 일산화질소의 농도가 낮은가?
이의석: 방사형 도로와의 접근성이 좋으면 TAX 가 높은가?
이의석: 공장지대에 가까울수록 NOX 가 심한가?
이의석: 보스톤직업센터와의 거리가 멀수록 NOX 는 줄어드는가?
편석영: 범죄율(rad)이 높으면 방사형 도로(crim)와 접근성이 높다
편석영: 세금율(tax)이 높으면 범죄율(crim)이 높다
편석영: 직장 접근성(dis) 이 높으면 거주지역 25000 비율(zn) 높다
임진영: 크기가 큰 거주지역(zn)은 직업센터까지의 접근성(dis)이 좋다.
임진영: 방사형 도로까지의 접근성(rad)이 좋으면 범죄율(crim)이 높다.
임진영:세금(tax)이 클수록 범죄율(crim)이 높다.
임진영:공장지대(indus)에는 오래전에 건축된 주택(age)이 많다.

점심시간 문제:  카페에 올리시고 즐거운 점심시간 되세요 ~~~

▩ 아래의 2개의 질문이 신경망 모델의 성능을 올리는데 도움이 되는 질문인지 확인하기 

질문1: 미국도 학군이 높으면 집값이 비쌀까?
질문2: 세금이 높은 지역일수록 집값이 낮아질까 ?

▩ 아래의 첫번째 질문으로 파생변수를 생성하기 

질문1: 미국도 학군이 높으면 집값이 비쌀까?

예제1. PTRATIO(자치시별 학생/교사 비율) 이 낮을 수록 집값이 높은지 확인하기 위해서
        이 비율을 ntile 을 이용해서 4개의 등급으로 나누시오 ! (파생변수명: ptr_grade )

library(dplyr)
ptr_grade <- ntile( boston$PTRATIO, 4)
ptr_grade

예제2. 위의 파생변수가 없었을 때의 신경망 성능 확인 

▩ boston 집값을 예측하는 인공신경망 모델 만들기 (게시글 1044번)

#1. 데이터를 로드합니다.
boston <- read.csv("boston.csv")

#2. 결측치를 확인합니다.
colSums(is.na(boston))

#3. 이상치를 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("boston.csv")

for (i in 1:length(colnames(wisc))){

  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )

}

#4. 데이터를 정규화 시킵니다.

normalize <- function(x) {
                                     return ( (x-min(x) ) /  ( max(x) - min(x)) )
                                }

boston_normalize <- as.data.frame( lapply( boston, normalize) ) 
summary(boston_normalize)

#5. 훈련데이터와 테스트 데이터로 데이터를 분리합니다. (9:1)
library(caret)
set.seed(1)
train_num <-  createDataPartition( boston_normalize$price, p=0.9, list=F)
train_data <-  boston_normalize[ train_num,  ] #훈련 데이터 90%
test_data <-  boston_normalize[ -train_num,  ] #테스트 데이터 10%
nrow(train_data) #  458
nrow(test_data)  #  48 

#6. 인공신경망 모델을 설정합니다.

library(neuralnet)

#7. 훈련 데이터로 인공신경망 모델을 학습 시킵니다.  
boston_model <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                                                 + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT ,
                                        data= train_data)
plot(boston_model) 

#8. 테스트 데이터를 예측합니다.

library(neuralnet) 
boston_result <- neuralnet::compute( boston_model , test_data[  , 2:14] )

# 설명: compute 가 dplyr 의 패키지에도 compute 가 있어서 neuralnet 과 충돌이 되어서
# 그냥 compute 만 했을때 자꾸 실행안되면서 오류가 나서 neuralnet::compute 해줘야
# neuralnet 의 compute 를 써라 ~ 라고 명령을 하는것입니다. 

#9. 모델의 성능을 확인합니다.                                     

cor( boston_result$net.result, test_data[  , 15] )  #  0.8305972

#10. 모델의 성능을 더 올립니다. (하이퍼 파라미터 조절로 올리는 시도)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~CRIM + ZN + INDUS + CHAS  +  NOX  +  RM
                                                 + AGE  + DIS + RAD + TAX + PTRATIO+B +LSTAT ,
                                        data= train_data, hidden=c(5,2) )
plot(boston_model2) 

library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[  , 2:14] )

#11. 모델의 성능을 더 올립니다. ( 파생변수 추가로 올리는 시도 )

cor( boston_result2$net.result, test_data[  , 15] )  #  0.8775291

예제3.  학군과 관련된 파생변수인 ptr_grade 컬럼을 추가하고서 다시 성능을 확인해보시오

#1. 데이터를 로드합니다.

boston <- read.csv("boston.csv")

library(dplyr)
ptr_grade <- ntile( boston$PTRATIO, 4)
ptr_grade

설명:  학군과 집값을 서로 상관관계는 높았으나 집값을 예측하는 머신러닝 모델의 
        성능을 향상시키는데는 크게 기여하지 못했습니다. 

#▩ boston 집값을 예측하는 인공신경망 모델 만들기 

#1. 데이터를 로드합니다.
boston <- read.csv("boston.csv")

library(dplyr)
boston$ptr_grade <- ntile( boston$PTRATIO, 4)



#2. 결측치를 확인합니다.
colSums(is.na(boston))

#3. 이상치를 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("boston.csv")

for (i in 1:length(colnames(wisc))){
  
  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )
  
}

#4. 데이터를 정규화 시킵니다.

normalize <- function(x) {
  return ( (x-min(x) ) /  ( max(x) - min(x)) )
}

boston_normalize <- as.data.frame( lapply( boston, normalize) ) 
summary(boston_normalize)

#5. 훈련데이터와 테스트 데이터로 데이터를 분리합니다. (9:1)
library(caret)
set.seed(1)
train_num <-  createDataPartition( boston_normalize$price, p=0.9, list=F)
train_data <-  boston_normalize[ train_num,  ] #훈련 데이터 90%
test_data <-  boston_normalize[ -train_num,  ] #테스트 데이터 10%
nrow(train_data) #  458
nrow(test_data)  #  48 

test_data[  , c(2:14,16)]

#6. 인공신경망 모델을 설정합니다.

library(neuralnet)

#7. 훈련 데이터로 인공신경망 모델을 학습 시킵니다.  
boston_model <- neuralnet( formula = price ~ . , data= train_data[ , -1])
#plot(boston_model) 

#8. 테스트 데이터를 예측합니다.

library(neuralnet) 
boston_result <- neuralnet::compute( boston_model , test_data[  , c(2:14,16)] )

# 설명: compute 가 dplyr 의 패키지에도 compute 가 있어서 neuralnet 과 충돌이 되어서
# 그냥 compute 만 했을때 자꾸 실행안되면서 오류가 나서 neuralnet::compute 해줘야
# neuralnet 의 compute 를 써라 ~ 라고 명령을 하는것입니다. 

#9. 모델의 성능을 확인합니다.                                     

cor( boston_result$net.result, test_data[  , 15] )  #  0.8305972

#10. 모델의 성능을 더 올립니다. (하이퍼 파라미터 조절로 올리는 시도)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~ . , data= train_data[ , -1], hidden=c(5,2) )
#plot(boston_model2) 

library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[  , c(2:14,16)] )

#11. 모델의 성능을 더 올립니다. ( 파생변수 추가로 올리는 시도 )

cor( boston_result2$net.result, test_data[  , 15] )  # 0.9140615

질문2: 세금이 높은 지역일수록 집값이 낮아질까 ?

문제297.  세금에 대한 파생변수를 추가하고 인공신경망 모델을 학습 시킨후 
              더 성능이 올라가는지 확인하시오 !

#▩ boston 집값을 예측하는 인공신경망 모델 만들기 

#1. 데이터를 로드합니다.
boston <- read.csv("boston.csv")

library(dplyr)
boston$tax_grade <- ntile( boston$TAX, 4)

#2. 결측치를 확인합니다.
colSums(is.na(boston))

#3. 이상치를 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("boston.csv")

for (i in 1:length(colnames(wisc))){
  
  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )
  
}

#4. 데이터를 정규화 시킵니다.

normalize <- function(x) {
  return ( (x-min(x) ) /  ( max(x) - min(x)) )
}

boston_normalize <- as.data.frame( lapply( boston, normalize) ) 
summary(boston_normalize)

#5. 훈련데이터와 테스트 데이터로 데이터를 분리합니다. (9:1)
library(caret)
set.seed(1)
train_num <-  createDataPartition( boston_normalize$price, p=0.9, list=F)
train_data <-  boston_normalize[ train_num,  ] #훈련 데이터 90%
test_data <-  boston_normalize[ -train_num,  ] #테스트 데이터 10%
nrow(train_data) #  458
nrow(test_data)  #  48 

test_data[  , c(2:14,16)]

#6. 인공신경망 모델을 설정합니다.

library(neuralnet)

#7. 훈련 데이터로 인공신경망 모델을 학습 시킵니다.  
boston_model <- neuralnet( formula = price ~ . , data= train_data[ , -1])
#plot(boston_model) 

#8. 테스트 데이터를 예측합니다.

library(neuralnet) 
boston_result <- neuralnet::compute( boston_model , test_data[  , c(2:14,16)] )

# 설명: compute 가 dplyr 의 패키지에도 compute 가 있어서 neuralnet 과 충돌이 되어서
# 그냥 compute 만 했을때 자꾸 실행안되면서 오류가 나서 neuralnet::compute 해줘야
# neuralnet 의 compute 를 써라 ~ 라고 명령을 하는것입니다. 

#9. 모델의 성능을 확인합니다.                                     

cor( boston_result$net.result, test_data[  , 15] )  #  0.8305972

#10. 모델의 성능을 더 올립니다. (하이퍼 파라미터 조절로 올리는 시도)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~ . ,
                            data= train_data[ , -1], hidden=c(5,2) )
#plot(boston_model2) 

library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[  , c(2:14,16)] )

#11. 모델의 성능을 더 올립니다. ( 파생변수 추가로 올리는 시도 )

cor( boston_result2$net.result, test_data[  , 15] )  #  0.9142466

문제298.  ptr_grade 와 tax_grade 두개를 다 추가했을때 상관계수가 어떻게 달라지는지
            확인하시오 !

0.8890252  로 파생변수를 하나만 추가했을때 보다 성능이 더 나아지지 않았습니다.

답:
#▩ boston 집값을 예측하는 인공신경망 모델 만들기 

#1. 데이터를 로드합니다.
boston <- read.csv("boston.csv")

library(dplyr)
boston$tax_grade <- ntile( boston$TAX, 4)
boston$ptr_grade <- ntile( boston$PTRATIO, 4)
head(boston)
ncol(boston)

#2. 결측치를 확인합니다.
colSums(is.na(boston))

#3. 이상치를 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("boston.csv")

for (i in 1:length(colnames(wisc))){
  
  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )
  
}

#4. 데이터를 정규화 시킵니다.

normalize <- function(x) {
  return ( (x-min(x) ) /  ( max(x) - min(x)) )
}

boston_normalize <- as.data.frame( lapply( boston, normalize) ) 
summary(boston_normalize)

#5. 훈련데이터와 테스트 데이터로 데이터를 분리합니다. (9:1)
library(caret)
set.seed(1)
train_num <-  createDataPartition( boston_normalize$price, p=0.9, list=F)
train_data <-  boston_normalize[ train_num,  ] #훈련 데이터 90%
test_data <-  boston_normalize[ -train_num,  ] #테스트 데이터 10%
nrow(train_data) #  458
nrow(test_data)  #  48 

test_data[  , -c(1,15)]

#6. 인공신경망 모델을 설정합니다.

library(neuralnet)

#7. 훈련 데이터로 인공신경망 모델을 학습 시킵니다.  
boston_model <- neuralnet( formula = price ~ . , data= train_data[ , -1])
#plot(boston_model) 

#8. 테스트 데이터를 예측합니다.

library(neuralnet) 
boston_result <- neuralnet::compute( boston_model , test_data[  , -c(1,15)] )

# 설명: compute 가 dplyr 의 패키지에도 compute 가 있어서 neuralnet 과 충돌이 되어서
# 그냥 compute 만 했을때 자꾸 실행안되면서 오류가 나서 neuralnet::compute 해줘야
# neuralnet 의 compute 를 써라 ~ 라고 명령을 하는것입니다. 

#9. 모델의 성능을 확인합니다.                                     

cor( boston_result$net.result, test_data[  , 15] )  #  0.8305972

#10. 모델의 성능을 더 올립니다. (하이퍼 파라미터 조절로 올리는 시도)
set.seed(2)
boston_model2 <- neuralnet( formula = price ~ . ,
                            data= train_data[ , -1], hidden=c(5,2) )
#plot(boston_model2) 

library(neuralnet) 
boston_result2 <- neuralnet::compute( boston_model2 , test_data[  , -c(1,15)] )

#11. 모델의 성능을 더 올립니다. ( 파생변수 추가로 올리는 시도 )

cor( boston_result2$net.result, test_data[  , 15] )  #  0.8775291


정리: 

 1.  R 로 머신러닝 모델을 만들었을때 제일 성능이 좋았던 파생변수? indus_grade 

 2.  제일 성능이 좋았던 hidden 값은 ?     hidden=c(5,4)

▩ 파이썬으로 콘크리트 강도를 예측하는 인공신경망 구현하기 

#1.  데이터를 로드합니다.
import  pandas  as  pd

df = pd.read_csv("d:\\data\\concrete.csv")
df.shape  # (1030, 9)

#2.  결측치를 확인합니다.
df.isnull().sum()

#3.  이상치를 확인합니다.
def outlier_value(x):
    for i in x.columns[(x.dtypes == 'float64') | (x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( df )

#4.  정규화를 진행합니다.
x = df.iloc[ :  , 0:-1]
y = df.iloc[ :  , -1 ] 

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
x2 

y2 = y.to_numpy()

#5.  훈련 데이터와 테스트 데이터를 분리합니다.
from  sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test  =  train_test_split( x2, y2, test_size=0.2, random_state=10)

print( x_train.shape)  # (824, 8)
print( x_test.shape)   # (206, 8)
print( y_train.shape)  # (824,)
print( y_test.shape)   # (206,)

#6.  모델 생성
from  sklearn.neural_network  import  MLPRegressor 

model = MLPRegressor(random_state=0)

#7.  모델 훈련
model.fit( x_train, y_train)

#8.  모델 예측
result = model.predict(x_test)
result

#9.  모델 평가  (상관계수 확인)
import  scipy.stats  as  stats

stats.pearsonr( y_test, result )

(0.6512809632529383, 3.021786871431153e-26 )

                
#10. 성능 개선 

#모델 생성 부분부터 다시 합니다.
from  sklearn.neural_network  import  MLPRegressor 

model2 = MLPRegressor(random_state=0, hidden_layer_sizes=(200, 50 ) )

#설명:  은닉1층의 뉴런의 갯수를 200개로 늘립니다. (기본값이 100)
          은닉2층의 뉴런의 갯수를 50개로 늘립니다.
          입력층(0층) ---> 은닉1층 ---> 은닉2층 ---> 출력층(3층)
model2.fit( x_train, y_train)

result2 = model2.predict(x_test)
result2

import  scipy.stats  as  stats
stats.pearsonr( y_test, result2 )  # (0.7898151873162131, 3.4639857935253367e-45)

문제299. hidden_layer_sizes 하이퍼  파라미터를 더 조절해서 모델의 성능을 더 올리시오 

model2 = MLPRegressor(random_state=0, hidden_layer_sizes=(300, 200 ) )

MLPRegressor(random_state=0,                  #  seed 값
                    hidden_layer_sizes=(200,50),  # 은닉1층 200개, 은닉2층 50개
                    activation='relu',                 # 신경망에 들어가는 활성화 함수 relu
                    solver='adam',                    # 경사하강법의 최적화 방법중 하나인 adam
                    max_iter=1000)                   # 책 몇번 볼것인가 

활성화 함수의 종류:  relu, logstic, identity, tanh 
경사하강법의 최적화 종류:  adam, sgd, lbfgs 


model3 = MLPRegressor(random_state=0,                  #  seed 값
                                hidden_layer_sizes=(200,50),  # 은닉1층 200개, 은닉2층 50개
                                activation='relu',                 # 신경망에 들어가는 활성화 함수 relu
                                solver='adam',                # 경사하강법의 최적화 방법중 하나인 adam
                               max_iter=1000)     

model3.fit( x_train, y_train)

result3 = model3.predict(x_test)
result3

import  scipy.stats  as  stats
stats.pearsonr( y_test, result3 )   # 0.9298979119599521


▩  grid search 기능을 이용해서 가장 최적화된 하이퍼 파라미터의 조합을 알아내기

# 모델 생성 부분에 추가하세요 ~
from  sklearn.neural_network  import  MLPRegressor
from  sklearn.model_selection  import  GridSearchCV 

param_grid = [ 
                      { 'hidden_layer_sizes' : [ (100, 50), (200, 50), (100, 100), (200, 200) ],
                        'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                        'solver' : ['lbfgs', 'sgd', 'adam'] 
                       }
                    ]

model7 = GridSearchCV( MLPRegressor( random_state=0) , param_grid, cv=3, n_jobs=-1,
                                 verbose=2) 

# 설명: verbose=2 는  학습하면서 최적의 하이퍼 파라미터를 찾는 과정을 화면에 출력한다. 
#           n_jobs = -1 로 하면 진행과정을 전부다 보여주지 않고 요약해서 보여줍니다.
#           cv=3  n-fold 교차검증을 사용하겠다( 11장에 다룹니다)

model7.fit( x_train, y_train)

# 최적의 하이퍼 파라미터의 조합을 확인하는 코드

print ( model7.best_params_ )

#  {'activation': 'relu', 'hidden_layer_sizes': (100, 100), 'solver': 'lbfgs'}

result7 = model7.predict(x_test)
result7

import  scipy.stats  as  stats
stats.pearsonr( y_test, result7 )   # 0.9451701315406708

설명:  활성화 함수를 relu 로 하고 은닉1층 100개, 은닉2층 100개의 뉴런으로 구성하고
         경사하강법의 방법 중이 하나인 lbfgs 라는 경사하강법을 사용하면 상관계수가
         0.945 에 해당하는 값이 출력됩니다.  R 에서는 caret 패키지가 이런 같은 역활을 합니다.
         caret 패키지 사용은 11장에서 배웁니다. 

문제300.  boston 하우징 데이터의 인공신경망 모델을 생성하는데 일단 지금은 파생변수 추가없이
             인공신경망 모델을 생성하시오 !  (grid search 도 하지 말고 파생변수도 추가하지 말고
             진행하세요)

#1.  데이터를 로드합니다.
import  pandas  as  pd

df = pd.read_csv("d:\\data\\boston.csv")
df.shape  # (506, 15)

#2.  결측치를 확인합니다.
df.isnull().sum()

#3.  이상치를 확인합니다.
def outlier_value(x):
    for i in x.columns[(x.dtypes == 'float64') | (x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( df )

#4.  정규화를 진행합니다.
x = df.iloc[ :  , 1:-1]
y = df.iloc[ :  , -1 ] 

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
x2 

y2 = y.to_numpy()

#5.  훈련 데이터와 테스트 데이터를 분리합니다.
from  sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test  =  train_test_split( x2, y2, test_size=0.1, random_state=10)

print( x_train.shape)  # (455, 14)
print( x_test.shape)   # ( 51, 14)
print( y_train.shape)  # (455,)
print( y_test.shape)   # (51,)

#6.  모델 생성
from  sklearn.neural_network  import  MLPRegressor 

model = MLPRegressor(random_state=0)

#7.  모델 훈련
model.fit( x_train, y_train)

#8.  모델 예측
result = model.predict(x_test)
result

#9.  모델 평가  (상관계수 확인)
import  scipy.stats  as  stats

stats.pearsonr( y_test, result )  # 0.5730694056418174

문제301. (오늘의 마지막 문제 ) 
            보스톤 집값 예측을 하는 위의 신경망에 grid search 기능을 적용해서 
             최적의 하이퍼 파라미터가 무엇인지 알아내는 실험을 하시오 !

from  sklearn.model_selection  import  GridSearchCV 

param_grid = [ 
                      { 'hidden_layer_sizes' : [ (100, 50), (200, 50), (100, 100), (200, 200) ],
                        'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                        'solver' : ['lbfgs', 'sgd', 'adam'] 
                       }
                    ]


▩ R 와 파이썬을 활용한 머신러닝 수업 

신경망 :   1. 수치예측 :  콘크리트
                               보스톤 집값 

             2. 분류 

어제 마지막 문제 코드 전체를 가져오세요 ~

세현이 코드:

import pandas as pd
df = pd.read_csv("d:\\data\\boston.csv")
# [506 rows x 15 columns]

# 결측치 확인
# df.isnull().sum() # 결측치 없음

# 이상치 확인
def outlier_value(x):
  for i in x.columns[(x.dtypes == 'float64')|(x.dtypes == 'int64')]:
    Q1 = x[i].quantile(0.25)
    Q3 = x[i].quantile(0.75)
    iqr = Q3-Q1
    print(i, x[i][(x[i]>Q3+iqr*1.5)|(x[i]<Q1-iqr*1.5)].count())
outlier_value(df)

# 정규화 진행
x=df.iloc[:,0:-1]
y=df.iloc[:,-1]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x)
x2=scaler.transform(x)
y2= y.to_numpy()

# 훈련데이터와 테스트데이터를 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x2,y2, test_size=0.1, random_state=10)

# 모델 생성 부분에 추가하세요 ~
from  sklearn.neural_network  import  MLPRegressor
from  sklearn.model_selection  import  GridSearchCV 

param_grid = [ 
                      { 'hidden_layer_sizes' : [ (100, 50), (200, 50), (100, 100), (200, 200) ],
                        'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                        'solver' : ['lbfgs', 'sgd', 'adam'] 
                       }
                    ]

model = GridSearchCV( MLPRegressor( random_state=0) , param_grid, cv=3, n_jobs=-1,
                                 verbose=2) 


model.fit( x_train, y_train)

# 최적의 하이퍼 파라미터의 조합을 확인하는 코드

print ( model.best_params_ )

#  {'activation': 'relu', 'hidden_layer_sizes': (200, 200), 'solver': 'lbfgs'}

result = model.predict(x_test)
result

import  scipy.stats  as  stats
stats.pearsonr( y_test, result )   # 0.957950269983475

▩ R의 ntile 과 같은 등급으로 나누는 파이썬의 함수  ( pandas 의 cut 함수 )

import   pandas   as  pd

ages = [ 0, 10, 15, 13, 21, 23, 37, 43, 80, 61, 20, 41, 32, 100 ]
bins = [ 0, 15, 25, 35, 60, 100 ]
labels =['어린이', '청년', '장년', '중년', '노년' ]

cuts = pd.cut( ages, bins, right=False, labels=labels)
cuts 

설명:  0 <= age < 15  :  어린이
        15 <= age < 25  :  청년
        25 <= age < 35  :  장년
        35 <= age < 60  :  중년
        60 <= age < 100 :  노년  

예제1.  mtcars 데이터의 mpg 를 0~25% 는  A, 26~50% 는 B, 51~75% 는 C , 
          76~100%는 D 로 범주화하는 파생변수를 mpg_grade 라는 이름으로 추가하시오 !

import   pandas  as   pd

mtcars = pd.read_csv("d:\\data\\mtcars.csv")
mtcars 

q0 = mtcars['mpg'].quantile(0.0)
q1 = mtcars['mpg'].quantile(0.25)
q2 = mtcars['mpg'].quantile(0.50)
q3 = mtcars['mpg'].quantile(0.75)
q4 = mtcars['mpg'].quantile(1.0)
print ( q0, q1, q2, q3, q4)  # 10.4 15.425 19.2 22.8 33.9

bins =[ 10.4, 15.425, 19.2, 22.8, 34 ]
cuts = pd.cut( mtcars['mpg'], bins, right=False, labels=['A', 'B', 'C', 'D' ])
mtcars['mpg_grade']=cuts
mtcars.head()

예제2.  위의 코드를 조금더 간단하게 작성하는 방법 

mtcars = pd.read_csv("d:\\data\\mtcars.csv")

mtcars['mgp_grade'] = pd.qcut( mtcars['mpg'], q=4, labels=['A','B','C','D'] )

mtcars

▩ 보스톤 하우징 인공신경망의 성능을 높이기 위해 파생변수를 생성하기 위한 예제들

예제1. 보스톤 데이터의 방의 갯수의 비율(RM) 을 4개의 등급으로 나누고 rm_grade 라는
         파생변수로 생성하시오 ( 1,2,3,4 ) 

※  설명:  cut 은 right false 값으로 A<= x < B 
            qcut 은 A< x <= B 들어간 형태입니다. 

참고자료: https://pandas.pydata.org/docs/reference/api/pandas.qcut.html

(1) pd.cut()으로 동일 길이로 나누어서 범주를 만든 후 GroupBy()로 그룹별 통계량 구하기

(2) pd.qcut()으로 동일 개수로 나누어서 범주를 만든 후 GroupBy()로 그룹별 통계량 구하기

출처: https://rfriend.tistory.com/404 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]

boston['rm_grade'] = pd.qcut( boston['RM'], q=4, labels=[1,2,3,4] )
boston['rm_grade']

Categories (4, int64): [1 < 2 < 3 < 4]

예제2.  qcut 으로 방의 비율에 대한 파생변수를 생성한 이후에 보스턴 집값 예측 모델의 
          성능이 더 올라가는지 확인하시오 


import pandas as pd
df = pd.read_csv("d:\\data\\boston.csv")


df['rm_grade'] = pd.qcut( df['RM'], q=4, labels=[1,2,3,4] )
df['rm_grade']

# 결측치 확인
# df.isnull().sum() # 결측치 없음

# 이상치 확인
def outlier_value(x):
  for i in x.columns[(x.dtypes == 'float64')|(x.dtypes == 'int64')]:
    Q1 = x[i].quantile(0.25)
    Q3 = x[i].quantile(0.75)
    iqr = Q3-Q1
    print(i, x[i][(x[i]>Q3+iqr*1.5)|(x[i]<Q1-iqr*1.5)].count())
outlier_value(df)

# 정규화 진행
x=df.drop(['id','price'], axis = 1)
y=df.price
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x)
x2=scaler.transform(x)
y2= y.to_numpy()

# 훈련데이터와 테스트데이터를 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x2,y2, test_size=0.1, random_state=10)

# 모델 생성 부분에 추가하세요 ~
from  sklearn.neural_network  import  MLPRegressor
from  sklearn.model_selection  import  GridSearchCV 

param_grid = [ 
                      { 'hidden_layer_sizes' : [ (100, 50), (200, 50), (100, 100), (200, 200) ],
                        'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                        'solver' : ['lbfgs', 'sgd', 'adam'] 
                       }
                    ]

model = GridSearchCV( MLPRegressor( random_state=0) , param_grid, cv=3, n_jobs=-1,
                                 verbose=2) 


model.fit( x_train, y_train)

# 최적의 하이퍼 파라미터의 조합을 확인하는 코드

print ( model.best_params_ )

#  {'activation': 'relu', 'hidden_layer_sizes': (200, 200), 'solver': 'lbfgs'}

result = model.predict(x_test)
result

import  scipy.stats  as  stats
stats.pearsonr( y_test, result )   # 0.957950269983475
                                         # 0.9583032119122499  rm_grade 컬럼을 추가했을때 

예제3.  예제2번은 qcut 으로 방의 비율에 대한 파생변수를 추가했는데 이번에는
          cut 으로 방의 비율에 대한 파생변수를 추가하고 모델의 성능을 확인하시오 !

import pandas as pd
df = pd.read_csv("d:\\data\\boston.csv")

df['rm_grade'] = pd.cut( df['RM'], bins=4, labels=[1,2,3,4] )  # 동일한 길이로 나눈다. 
df['rm_grade']


# 결측치 확인
# df.isnull().sum() # 결측치 없음

# 이상치 확인
def outlier_value(x):
  for i in x.columns[(x.dtypes == 'float64')|(x.dtypes == 'int64')]:
    Q1 = x[i].quantile(0.25)
    Q3 = x[i].quantile(0.75)
    iqr = Q3-Q1
    print(i, x[i][(x[i]>Q3+iqr*1.5)|(x[i]<Q1-iqr*1.5)].count())
outlier_value(df)

# 정규화 진행
x=df.drop(['id','price'], axis = 1)
y=df.price
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x)
x2=scaler.transform(x)
y2= y.to_numpy()

# 훈련데이터와 테스트데이터를 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x2,y2, test_size=0.1, random_state=10)

# 모델 생성 부분에 추가하세요 ~
from  sklearn.neural_network  import  MLPRegressor
from  sklearn.model_selection  import  GridSearchCV 

param_grid = [ 
                      { 'hidden_layer_sizes' : [ (100, 50), (200, 50), (100, 100), (200, 200) ],
                        'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                        'solver' : ['lbfgs', 'sgd', 'adam'] 
                       }
                    ]

model = GridSearchCV( MLPRegressor( random_state=0) , param_grid, cv=3, n_jobs=-1,
                                 verbose=2) 


model.fit( x_train, y_train)

# 최적의 하이퍼 파라미터의 조합을 확인하는 코드

print ( model.best_params_ )

#  {'activation': 'relu', 'hidden_layer_sizes': (200, 200), 'solver': 'lbfgs'}

result = model.predict(x_test)
result

import  scipy.stats  as  stats
stats.pearsonr( y_test, result )   #  0.9583032119122499  qcuts 으로 했을때 
                                         #  0.9533686742926833  cuts 로 했을때 

예제4. qcuts 을 이용해서 하위계층의 비율을 4개의 등급으로 나눈 파생변수를 
         lstat_grade 로 추가하시오 !

df['lstat_grade'] = pd.qcut( df['LSTAT'], q=4, labels=[1,2,3,4] )
df['lstat_grade']

예제5.  lstat_grade 라는 파생변수가 추가되었을때 보스톤 하우징 신경망의
          성능이 더 올라가는지 확인하시오 

0.9481854092254369

예제6. 집의 노후화(AGE) 비율을 4개의 등급으로 나누는 age_grade 라는 파생변수를
         추가하시오 !  (qcuts 를 이용하세요)

df['age_grade'] = pd.qcut( df['AGE'], q=4, labels=[1,2,3,4] )
df['age_grade']

예제7. age_grade 라는 파생변수가 추가 되었을때 보스톤 집값을 예측하는 신경망의
         성능이 더 올라가는지 확인하시오

0.9456553466041837

예제8.  집의 노후화와 집값은 집의 노후화의 비율이 클수록 집값이 떨어지는 
          음의 상관관계를 보이므로  labels 를 1,2,3,4 순이 아닌 4,3,2,1 순으로 변경하고
          다시 실험하시오 !

df['age_grade'] = pd.qcut( df['AGE'], q=4, labels=[4,3,2,1] )
df['age_grade']

0.9421920629123643

지금까지의 실험으로는 방의 비율에 대한 파생변수가 가장 영향력이 있었습니다. 

예제9. 공해가 심할 수록 집값이 떨어지는지 확인해보기 위해 일산화질소(NOX)를
         4등급으로 나눈 nox_grade 라는 파생변수를 추가하시오 !

df['nox_grade'] = pd.qcut( df['NOX'], q=4, labels=[1,2,3,4] )
df['nox_grade']

예제10.  nox_grade 라는 파생변수가 추가되었을때 집값예측 신경망의 성능이 
           더 올라가는지 확인하시오 !

0.9597566607822805

예제11.  직장과의 거리가 가까울 수록 집값이 올라가는지를 확인하기 위해서
            직장과의 거리에 대한 비율 컬럼인 DIS 컬럼을 4개의 등급을 나눈
            dis_grade 라는 파생변수를 추가하시오 !

df['dis_grade'] = pd.qcut( df['DIS'], q=4, labels=[1,2,3,4] )
df['dis_grade']

예제12.  dis_grade 라는 파생변수를 추가했을때 집값 예측 신경망의 성능이
           더 올라가는지 실험하시오 !

0.947994947536834

▩ 파이썬으로 상관관계 그래프 그리기 

import  seaborn  as   sns 
import matplotlib.pyplot as plt

boston = pd.read_csv("d:\\data\\boston.csv")
f, ax = plt.subplots(figsize=(12, 10))   # 사이즈를 키웁니다. 
sns.heatmap( data = boston.corr(),  annot=True, fmt='.2f', linewidths=.5, cmap='Blues')

설명:  annot=True  는 상관계수 값 출력,
         fmt='.2f 는 글씨 크기 
         linewidths=.5 는 라인의 넓이
         cmap='Blues' 는 색상입니다. 

공해(NOX)에 비율을 가지고 만든 파생변수가 가장 성능이 좋았습니다.


인공신경망 --> 1. 수치예측
                     2. 분류

▩ 인공신경망을 이용해서 분류하기 (R)

    "와인의 등급을 분류하는 신경망 "

#1. 데이터를 로드합니다.
wine <- read.csv("wine.csv", stringsAsFactors=T )
head(wine)
nrow(wine)  # 178
ncol(wine)   # 14

맨앞에 있는 컬럼이 정답 라벨 컬럼입니다.

unique(wine$Type)
str(wine)

#2. 결측치가 있는지 확인합니다.
colSums(is.na(wine)) 


#3. 이상치가 있는지 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("wine.csv")

for (i in 2:length(colnames(wisc))){
  
  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )
  
}

#4. 정규화를 진행합니다.

normalize <- function(x) {
                                     return ( (x-min(x) ) /  ( max(x) - min(x)) )
                                }

wine_normalize <- as.data.frame( lapply( wine[ , -1] , normalize) ) 
summary(wine_normalize)

wine2 <- cbind( Type=wine$Type , wine_normalize)
head(wine2)

#5. 훈련 데이터와 테스트 데이터를 분리합니다.
library(caret)
set.seed(1)
train_num <-  createDataPartition( wine2$Type,  p=0.9,  list=FALSE)
train_data <- wine2[ train_num,   ]  # 훈련 데이터 구성
test_data <-  wine2[ - train_num, ]  # 테스트 데이터 구성

nrow(train_data)  # 162
nrow(test_data)   # 16

#6. 모델을 설정합니다.
install.packages("nnet")  # 분류를 위한 신경망 모델 
library(nnet)

#7. 모델을 훈련시킵니다.
set.seed(1)
model <- nnet(Type~.,  data=train_data,  size=2 )

#설명:  size=2 는 은닉층 1개로 하고 은닉층의 뉴런의 갯수를 2개로 하겠다는 뜻입니다.

#8. 테스트 데이터를 예측합니다.

result <-  predict( model,  test_data,  type="class")
result

#9. 모델을 평가 합니다.
sum( result == test_data[  , 1] ) / length( test_data[  , 1] )

#10. 모델의 성능을 높입니다. 

model2 <- nnet(Type~.,  data=train_data,  size=5  )  # 은닉층의 뉴런의 갯수 5개 

result2 <-  predict( model2,  test_data,  type="class")

sum( result2 == test_data[  , 1] ) / length( test_data[  , 1] )

문제302.  iris 의 품종을 분류하는 인공신경망 모델을 생성하시오 !

# 1. 데이터를 로드합니다.
iris <- read.csv("iris2.csv", stringsAsFactors=T )

# 2. 결측치를 확인합니다.
colSums(is.na(iris))

# 3. 이상치를 확인합니다.
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

wisc <- read.csv("iris2.csv")

for ( i in 1:4 ){
  
  a = grubbs.flag(wisc[,colnames(wisc)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(wisc)[i] , '--> ',  length(b) )  )
  
}

#4. 정규화를 진행합니다.

normalize <- function(x) {
                                     return ( (x-min(x) ) /  ( max(x) - min(x)) )
                                }

iris_normalize <- as.data.frame( lapply( iris[ , -5] , normalize) ) 
summary(iris_normalize)

iris2 <- cbind( Species=iris$Species , iris_normalize)
head(iris2)

#5. 훈련 데이터와 테스트 데이터를 분리합니다.
library(caret)
set.seed(1)
train_num <-  createDataPartition( iris$Species,  p=0.9,  list=FALSE)
train_data <- iris2[ train_num,   ]  # 훈련 데이터 구성
test_data <-  iris2[ - train_num, ]  # 테스트 데이터 구성

nrow(train_data)  # 135
nrow(test_data)   # 15

#6. 모델을 설정합니다.
#install.packages("nnet")  # 분류를 위한 신경망 모델 
library(nnet)

#7. 모델을 훈련시킵니다.
set.seed(1)
model <- nnet(Species~.,  data=train_data,  size=2 )

#설명:  size=2 는 은닉층 1개로 하고 은닉층의 뉴런의 갯수를 2개로 하겠다는 뜻입니다.

#8. 테스트 데이터를 예측합니다.

result <-  predict( model,  test_data,  type="class")
result

#9. 모델을 평가 합니다.
sum( result == test_data[  , 1] ) / length( test_data[  , 1] )

문제303.  타이타닉 생존자를 예측하는 신경망 모델을 생성하시오 !

tat <-  read.csv("tatanic.csv", stringsAsFactors=T )
tat 


▩  파이썬으로 분류하는 인공신경망 만들기 

수치예측: from sklearn.neural_network import MLPRegressor

분류:  from sklearn.neural_network import MLPClassifier

예제1.  wine 의 품질을 분류하는  인공신경망을 파이썬으로 구현하시오 !

#1. 데이터를 로드합니다.
import  pandas  as  pd
wine = pd.read_csv("d:\\data\\wine.csv")
wine.shape # (178, 14)

#2. 결측치를 확인합니다.
wine.isnull().sum()

#3. 이상치를 확인합니다.

def outlier_value(x):
  for i in x.columns[(x.dtypes == 'float64')|(x.dtypes == 'int64')]:
    Q1 = x[i].quantile(0.25)
    Q3 = x[i].quantile(0.75)
    iqr = Q3-Q1
    print(i, x[i][(x[i]>Q3+iqr*1.5)|(x[i]<Q1-iqr*1.5)].count())

outlier_value(wine)

#4. 정규화를 진행합니다.
x = wine.iloc[  : , 1: ]
y = wine['Type']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()

#5. 훈련 데이터와 테스트 데이터를 분리합니다.
from sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test = train_test_split( x2, y2,  test_size=0.1, random_state=0)
print(x_train.shape) # (160, 13)
print(x_test.shape)  # (18, 13)
print(y_train.shape)  # (160, )
print(y_test.shape)   # (18,  )

#6. 모델생성
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(random_state=0)
model

#7. 모델훈련
model.fit(x_train, y_train)

#8. 모델예측
result = model.predict(x_test)
result

#9. 모델평가
sum( result == y_test) / len(y_test)

#10. 성능개선 

model = MLPClassifier( hidden_layer_sizes=(100, 100), activation='relu', solver='adam',
                               random_state=0 ) 


문제304.  iris 의 품종을 분류하는 인공신경망을 파이썬으로 생성하시오 

#1. 데이터를 로드합니다.
import  pandas  as  pd
iris= pd.read_csv("d:\\data\\iris2.csv")
iris.shape # (178, 14)

#2. 결측치를 확인합니다.
iris.isnull().sum()

#3. 이상치를 확인합니다.

def outlier_value(x):
  for i in x.columns[(x.dtypes == 'float64')|(x.dtypes == 'int64')]:
    Q1 = x[i].quantile(0.25)
    Q3 = x[i].quantile(0.75)
    iqr = Q3-Q1
    print(i, x[i][(x[i]>Q3+iqr*1.5)|(x[i]<Q1-iqr*1.5)].count())

outlier_value(iris)

#4. 정규화를 진행합니다.
x = iris.iloc[  : , 0:-1 ]
y = iris['Species']


from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()

#5. 훈련 데이터와 테스트 데이터를 분리합니다.
from sklearn.model_selection  import   train_test_split

x_train, x_test, y_train, y_test = train_test_split( x2, y2,  test_size=0.1, random_state=0)
print(x_train.shape) # (135, 4)
print(x_test.shape)  # (15, 4)
print(y_train.shape)  # (135, )
print(y_test.shape)   # (15,  )

#6. 모델생성
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(random_state=0)
model

#7. 모델훈련
model.fit(x_train, y_train)

#8. 모델예측
result = model.predict(x_test)
result

#9. 모델평가
sum( result == y_test) / len(y_test)

0.7333333333333333

문제305. 위의 iris 품종을 분류하는 인공신경망 모델의 성능을 올리시오 !

model2 = MLPClassifier( hidden_layer_sizes=(100, 100), activation='relu', solver='adam',
                                random_state=0 ) 

문제306. 유방암 데이터(wisc.csv) 의 악성종양과 양성종양을 분류하기 위해서
            유방암 데이터를 파이썬으로 로드하시오 !

wisc = pd.read_csv("d:\\data\\wisc_bc_data.csv")
wisc.shape  #(569, 32)

문제307.  유방암 데이터의 악성종양과 양성종양을 분류하기 위해 독립변수들을
             정규화 하시오 !

x = wisc.iloc[ :  , 2: ]
y = wisc['diagnosis']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()                     

문제308. (오늘의 마지막 문제)  유방암을 분류하는 신경망을 파이썬으로 완성 시키세요.
             
# 1. 데이터 로드
import pandas as pd
wisc = pd.read_csv('d:\\data\\wisc_bc_data.csv')
wisc.shape # (569, 32)

# 2. 결측치 확인
wisc.isnull().sum()

# 3. 이상치를 확인
def outlier_value(x):
    for i in x.columns[(x.dtypes=='float64')|(x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR*5)|(x[i]<Q1-IQR*5)].count())

outlier_value(wisc)

# 4. 정규화 

x = wisc.drop(['id','diagnosis'],axis=1)
y = wisc.diagnosis

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()
wisc.head()

# 5. 훈련데이터와 테스트 데이터 분리
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x2,y2,test_size=0.1,random_state=4)

# 6. 모델 생성
from sklearn.neural_network import MLPClassifier

model = MLPClassifier(random_state=0)

# 7. 모델 훈련
model.fit(x_train,y_train)

# 8. 모델 예측
result = model.predict(x_test)
result

# 9. 모델 평가
sum(result==y_test)/len(y_test) # 0.9824561403508771

# 10. 모델 성능 개선
parameter_space = {
    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,50)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam']
 #   'alpha': [0.0001, 0.05],
 #   'learning_rate': ['constant','adaptive'],
}

from sklearn.model_selection import GridSearchCV

model2 = GridSearchCV(MLPClassifier(random_state=4),parameter_space,n_jobs=-1,cv=3,verbose=1)
model2.fit(x_train,y_train)

print(model2.best_params_) 
# {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50, 50, 50), 'learning_rate': 'constant', 'solver': 'adam'}

# 11. 개선된 모델로 예측
result2 = model2.predict(x_test)

# 12. 개선된 모델 평가
sum(result2==y_test)/len(y_test) # 0.9824561403508771


# random_state = 4 <- accuracy 100%

유방암 데이터에 대해서 정확도 100% 의 신경망 코드의 하이퍼 파라미터는 다음과 같습니다.

random_state =  4

'activation': 'tanh', 
'hidden_layer_sizes': (100, 50), 
'solver': 'adam'

'alpha': [0.0001, 0.05]  --->  오버피팅이 발생하지 않도록 하는 하이퍼 파라미터인데
L1 규제, L2 규제라는 용어로 쓰이는 파라미터이고 아주 큰 가중치에 대해서 패널티를 부여하여
큰 가중치를 감소 시키는 역활을 하는 파라미터

예:  개와 고양이를 분류하기위해서 고양이 사진을 학습하는 신경망의 경우 고양이 귀가 개와 고양이를
      분류하는데 있어서 중요한 부분이라고 여기면 고양이 귀에 대해서 아주 큰 가중치를 부여하는
      신경망이 만들어지게 되는데 이런 신경망의 경우 고양이 귀가 없는 사진이 입력되면 고양이로
      분류하지 못하는 현상이 발생하게 된다. 이를 오버피팅이라고 합니다.
     그래서 이런 오버피팅이 발생하지 않도록 아주 큰 가중치에 대해서 가중치를 감소시키는
     수학식을 적용하는게 이 규제 방법입니다. 

신경망으로 할 수 있는 2가지 -->  1. 수치예측 : 콘크리트 강도, 보스톤 집값
                                             2. 분류 :  와인, 아이리스, 타이타닉 

▩ 타이타닉 생존자 예측 신경망 만들기 (R)

# 데이터 로드
tat<-read.csv("tatanic.csv", stringsAsFactors = T)
head(tat)
ncol(tat) # 16
nrow(tat) # 891


tat1=tat[  ,c(-1,-9,-10,-11,-12,-13,-15, -16)]  # 겹치는 컬럼, 필요 없는 컬럼 총 5개 제거
                                               #  ( X, embarked, who, adult_male,deck,alive 제거)
head(tat1)

tat1$sex <- as.numeric(tat1$sex)  # 명목형 데이터 숫자로 변경 (이렇게 숫자로 바꿔도 되나요,,,?)
#tat1$class <- as.numeric(tat1$class)
tat1$embark_town <- as.numeric(tat1$embark_town)
#tat1$alone<- as.numeric(tat1$alone)

nrow(tat1) #891
ncol(tat1) #10

unique(tat1$survived) # 0 1

# 결측치

colSums(is.na(tat1)) # age 결측치 전부 평균으로 변경

tat1[is.na(tat1$age),'age'] <- 29.6

colSums(is.na(tat1))

# 이상치
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}


for (i in 1:length(colnames(tat1))){
  a = grubbs.flag(tat1[,colnames(tat1)[i]])
  b = a[a$Outlier==TRUE,"Outlier"]
  print ( paste( colnames(tat1)[i] , '--> ',  length(b) )  )
}  

# "sibsp -->  46" / "parch -->  15" / "fare -->  53" 이상치 의심

#4. 정규화를 진행한다.

normalize <- function(x) {
  return ( (x-min(x) ) /  ( max(x) - min(x)) )
}

tat_normalize <- as.data.frame( lapply( tat1[,-1], normalize) ) 
summary(tat_normalize)

attach(tat1)
tat2<-cbind(survived=tat1$survived,tat_normalize)
head(tat2)

#5. 훈련 데이터와 테스트 데이터를 분리한다.
library(caret)
set.seed(2)
train_num<-createDataPartition(tat2$survived,p=0.9,list=FALSE)
train_data<-tat2[train_num,] # 훈련 데이터 구성
test_data<-tat2[-train_num,] # 테스트 데이터 구성

nrow(train_data) #802
nrow(test_data) #89

#6. 모델을 설정한다.
#install.packages("nnet") # 분류를 위한 신경망 모델
library(nnet)

#7. 모델을 훈련시킨다.
set.seed(2)
model<-nnet(as.factor(survived)~.,data=train_data,size = 2)
model


#8. 테스트 데이터를 예측한다.

result<-predict(model, test_data, type="class")
result

#9. 모델을 평가한다.
sum(result==test_data[,1])/length(test_data[,1]) # 결과 :  0.8089888

#10. 모델의 성능을 높인다. 

for (i in 1:10) {
  model2 <- nnet(as.factor(survived)~.,  data=train_data,  size= i  )  # 은닉층의 뉴런의 개수 3개 (뉴런의 개수만 늘릴 수 있다. 
  result2 <-  predict( model2,  test_data,  type="class")
  final<-sum( result2 == test_data[  , 1] ) / length( test_data[  , 1] )
  max_final<-max(final)
  print(paste(i,'---->',max_final))
} # 4일때 최대 0.820224719101124 정확도 도출

45분까지 쉬세요 ~~

[1] "7 ----> 0.842696629213483"

신경망의 정확도를 올리는 방법 2가지 ?  1. 하이퍼 파라미터 조절 : size 를 알아내는 for 문

                                                      2. 파생변수 추가 

문제309.  아이와 여자는 먼저 보트에 탔기 때문에 생존율이 높았습니다. 
             그래서 아이와 여자면 1이고 아니면 0 으로 하는 파생변수를 생성하세요 !
             (아이의 기준은 7살보다 작은것으로 하겠습니다.)

tat$women_child <- ifelse ( tat$age < 7  | tat$sex=='female', 1, 0  ) 


문제310.  위의 파생변수를 추가했을때 정확도가 더 올라가는지 확인하시오 !

[1] "7 ----> 0.842696629213483"    # 파생변수 추가전 최고 정확도 
            
                   ↓  
 
[1] "3 ----> 0.853932584269663"    # 파생변수 추가후 최고 정확도 


답:
# 데이터 로드
tat<-read.csv("tatanic.csv", stringsAsFactors = T)
head(tat)
ncol(tat)
nrow(tat) 

▩ 파이썬으로 타이타닉 생존자 예측하는 신경망 만들기 

# 1. 데이터를 로드합니다.
import  pandas  as  pd
tat = pd.read_csv("d:\\data\\tatanic.csv")
tat.head()
tat.shape  # (891, 16)

# 2.필요한 컬럼들만 선택합니다.
tat.columns

tat2 = tat[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embark_town' ]]
tat2            

# 3. 결측치를 확인합니다.
tat2.isnull().sum()

※ 결측치를 다른값으로 치환하는 방법 
 
1. 평균값으로 치환
2. 중앙값으로 치환
3. 최빈값으로 치환
4. 주변의 행의 값으로 치환
5. 회귀식의 예측값으로 치환 

# 나이를 평균값으로 치환하기 
mean_age = tat2['age'].mean()
tat2['age'].fillna( mean_age, inplace=True)
tat2.isnull().sum()

# embark_town 을 최빈값으로 변경하기 
most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
print(most_town) # Southampton
tat2['embark_town'].fillna( most_town, inplace=True)
tat2.isnull().sum()

# 4. 이상치를 확인합니다.
def outlier_value(x):
    for i in x.columns[(x.dtypes=='float64')|(x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR*5)|(x[i]<Q1-IQR*5)].count())

outlier_value(tat2)

※ 끝에서 정확도 확인하고 이상치값에 대한 조정이 필요해보입니다.

# 5. 명목형 데이터를 숫자로 변환 합니다. 
tat2.info()

tat3 = pd.get_dummies(tat2, drop_first=True)
tat3.head()

# 6. 정규화를 진행합니다.
x = tat3.iloc[ :  , 1: ]
y = tat3['survived']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()

# 7. 훈련 데이터와 테스트 데이터를 분리합니다
from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x2, y2, test_size=0.1, random_state=0)

print( x_train.shape)  # (801, 8)
print( x_test.shape)   # (90, 8)
print( y_train.shape)  # (801, )
print( y_test.shape )  # (90,  )

# 8. 모델 생성
from  sklearn.neural_network  import  MLPClassifier

model = MLPClassifier()

# 9. 모델 훈련
model.fit(x_train, y_train)

# 10. 모델 예측
result =  model.predict(x_test)

# 11. 모델 평가
sum( result == y_test ) / len(y_test)  # 0.8111111111111111

# 12. 성능 개선 

 - grid search 적용하기
 - 파생변수 추가하기 

문제311. 아래의 남휘 코드를 참고해서 위의 신경망에 grid search 를 적용해서 최적의 하이퍼 파라미터
             를 알아내시오

from  sklearn.neural_network  import  MLPClassifier
from sklearn.model_selection import GridSearchCV

param_grid = [ { 'hidden_layer_sizes' : [ (100, 50), (230, 50), (100,100), (200,200)],
                  'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                  'solver' : ['lbfgs', 'sgd', 'adam']} ]

model3= GridSearchCV(MLPClassifier( random_state=0), param_grid, cv=3, n_jobs=-1,verbose=2)

model3.fit(x_train, y_train)
print(model3.best_params_)  # {'activation': 'relu', 'hidden_layer_sizes': (100, 50), 'solver': 'adam'}

# 10. 모델 예측
result3 =  model3.predict(x_test)
result3
# 11. 모델 평가
sum( result3 == y_test ) / len(y_test)

0.8111111111111111

grid search 를 안썼을때와 차이가 없습니다. 

문제312.  신경망을 사용하지 말고 random forest 를 사용해서 결과를 출력하세요 !

from   sklearn.ensemble   import  RandomForestClassifier

model = RandomForestClassifier(random_state=0)
model.fit( x_train, y_train)

result = model.predict( x_test )

sum(result == y_test) /len(y_test)

0.8111111111111111
 
※  성능개선을 위한 방법들 

1. 파생변수 추가 

2. 이상치를 다른값으로 치환 

  - 평균값
  - 중앙값 

문제313.  여자와 아이는 1로 하고 아니면 0으로 하는 파생변수를 women_child 로 추가하시오

# 1. 데이터 로드 
tat = pd.read_csv("d:\\data\\tatanic.csv")

# 2. 필요한 컬럼만 선별
tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked','class']]

# 3. 파생변수 추가 
mask = (tat2['sex']=='female')  |  (tat2['age'] < 12 )
mask 
tat2['women_child'] = mask.apply(int)
tat2.head()

문제314. 위의 파생변수를 추가한 상태에서 다시 훈련 시키고 인공신경망의 정확도를 확인하시오 ! 

# 1. 데이터 로드 
tat = pd.read_csv("d:\\data\\tatanic.csv")

# 2. 필요한 컬럼만 선별
tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarke_town','class']]

# 3. 파생변수 추가 
mask = (tat2['sex']=='female')  |  (tat2['age'] < 12 )
mask 
tat2['women_child'] = mask.apply(int)
tat2.head()

# 4. 결측치 확인

tat2.isnull().sum()

mean_age = tat2['age'].mean()
tat2['age'].fillna( mean_age, inplace=True)
tat2.isnull().sum()

most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
print(most_town) # Southampton
tat2['embark_town'].fillna( most_town, inplace=True)

tat2.isnull().sum()

# 5. 이상치 확인 
def outlier_value(x):
    for i in x.columns[(x.dtypes=='float64')|(x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR*5)|(x[i]<Q1-IQR*5)].count())

outlier_value(tat2)

#6. 명목형 변수를 더미화 시킨다.

tat2.info()

tat3 = pd.get_dummies(tat2, drop_first=True)
tat3.head()

#7. 정규화 작업을 수행합니다.
x = tat3.iloc[ :  , 1: ]
y = tat3['survived']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()

#8. 훈련 데이터와 테스트 데이터를 분리합니다.

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x2, y2, test_size=0.1, random_state=0)

print( x_train.shape)
print( x_test.shape)
print( y_train.shape)
print( y_test.shape )

#9. 그리드 서치를 이용해서 모델을 훈련 시킵니다.

from  sklearn.neural_network  import  MLPClassifier
from sklearn.model_selection import GridSearchCV

param_grid = [ { 'hidden_layer_sizes' : [ (100, 50), (230, 50), (100,100), (200,200),(100,100,100)],
                  'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                  'solver' : ['lbfgs', 'sgd', 'adam']} ]
model3= GridSearchCV(MLPClassifier( random_state=0), param_grid, cv=3, n_jobs=-1,verbose=2)
model3.fit(x_train, y_train)
print(model3.best_params_)

#10. 예측합니다.
result3 =  model3.predict(x_test)
result3

#11. 모델을 평가합니다.
sum( result3 == y_test ) / len(y_test)


아래와 같이 random_state 를 1로 했을때 0.83 까지 올라갔습니다.

model3= GridSearchCV(MLPClassifier( random_state=1), param_grid, cv=3, n_jobs=-1,verbose=2)

문제315. random_state 도 grid search에 추가해서 최적의 random_state 를 알아내게 하시오 !


# 1. 데이터 로드 
tat = pd.read_csv("d:\\data\\tatanic.csv")

# 2. 필요한 컬럼만 선별
tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embark_town','class']]

# 3. 파생변수 추가 
mask = (tat2['sex']=='female')  |  (tat2['age'] < 12 )
mask 
tat2['women_child'] = mask.apply(int)
tat2.head()

# 4. 결측치 확인

tat2.isnull().sum()

mean_age = tat2['age'].mean()
tat2['age'].fillna( mean_age, inplace=True)
tat2.isnull().sum()

most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
print(most_town) # Southampton
tat2['embark_town'].fillna( most_town, inplace=True)

tat2.isnull().sum()

# 5. 이상치 확인 
def outlier_value(x):
    for i in x.columns[(x.dtypes=='float64')|(x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR*5)|(x[i]<Q1-IQR*5)].count())

outlier_value(tat2)

#6. 명목형 변수를 더미화 시킨다.

tat2.info()

tat3 = pd.get_dummies(tat2, drop_first=True)
tat3.head()

#7. 정규화 작업을 수행합니다.
x = tat3.iloc[ :  , 1: ]
y = tat3['survived']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()

#8. 훈련 데이터와 테스트 데이터를 분리합니다.

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x2, y2, test_size=0.1, random_state=0)

print( x_train.shape)
print( x_test.shape)
print( y_train.shape)
print( y_test.shape )

#9. 그리드 서치를 이용해서 모델을 훈련 시킵니다.

from  sklearn.neural_network  import  MLPClassifier
from sklearn.model_selection import GridSearchCV

param_grid = [ { 'hidden_layer_sizes' : [ (100, 50), (230, 50), (100,100), (200,200),(100,100,100)],
                  'activation' : ['identity', 'logistic', 'tanh', 'relu'],
                  'solver' : ['lbfgs', 'sgd', 'adam'] ,
                  'random_state' : [0,1,2,3,4,5,6,7,8,9,10] } ]

model3= GridSearchCV(MLPClassifier(), param_grid, cv=3, n_jobs=-1,verbose=2)
model3.fit(x_train, y_train)
print(model3.best_params_)

#10. 예측합니다.
result3 =  model3.predict(x_test)
result3

#11. 모델을 평가합니다.
sum( result3 == y_test ) / len(y_test)

# 12. 이원 교차표를 확인해서 FN 값을 확인합니다.
from  sklearn  import  metrics 

result_metrix = metrics.confusion_matrix( y_test, result3)
print ( result_matrix) 

문제316. 위에서 활성화함수와 경사하강법과 층은 이미 가장 좋은게 뭔지 알아냈으므로 
            random_state 만 0번 부터 100까지중에서 어떤게 가장 최적인지 알아내는 코드를 
           작성하시오 !

# 1. 데이터 로드                                                     45분까지 쉬세요 ~~
tat = pd.read_csv("d:\\data\\tatanic.csv")

# 2. 필요한 컬럼만 선별
tat2 = tat[[ 'survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embark_town','class']]

# 3. 파생변수 추가 
mask = (tat2['sex']=='female')  |  (tat2['age'] < 12 )
mask 
tat2['women_child'] = mask.apply(int)
tat2.head()

# 4. 결측치 확인

tat2.isnull().sum()

mean_age = tat2['age'].mean()
tat2['age'].fillna( mean_age, inplace=True)
tat2.isnull().sum()

most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
most_town = tat2['embark_town'].value_counts(dropna=True).idxmax()
print(most_town) # Southampton
tat2['embark_town'].fillna( most_town, inplace=True)

tat2.isnull().sum()

# 5. 이상치 확인 
def outlier_value(x):
    for i in x.columns[(x.dtypes=='float64')|(x.dtypes=='int64')]:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR*5)|(x[i]<Q1-IQR*5)].count())

outlier_value(tat2)

#6. 명목형 변수를 더미화 시킨다.

tat2.info()

tat3 = pd.get_dummies(tat2, drop_first=True)
tat3.head()

#7. 정규화 작업을 수행합니다.
x = tat3.iloc[ :  , 1: ]
y = tat3['survived']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x2 = scaler.transform(x)
y2 = y.to_numpy()

#8. 훈련 데이터와 테스트 데이터를 분리합니다.

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x2, y2, test_size=0.1, random_state=0)

print( x_train.shape)
print( x_test.shape)
print( y_train.shape)
print( y_test.shape )

#9. 그리드 서치를 이용해서 모델을 훈련 시킵니다.

from  sklearn.neural_network  import  MLPClassifier
from sklearn.model_selection import GridSearchCV

#6. 모델 생성                                                    45분까지 쉬세요 ~~
aa =[]
bb = []
from sklearn.neural_network import MLPClassifier
for i in range(100):
    model = MLPClassifier(random_state = i,activation ='tanh', hidden_layer_sizes= (230, 50), solver= 'adam')
    model.fit(x_train,y_train)
    result = model.predict(x_test)
    aa.append(sum(result == y_test)/len(y_test))
    bb.append(i)
all = pd.DataFrame({'num':aa,"i":bb})


all[all.num == all['num'].max()]

■ 8장. 연관규칙 

  머신러닝의 종류 3가지 ?    1.  지도학습 :   정답이 있는 데이터로 학습 
                                                       분류: knn, naivebayes, decision tree, oneR, jiper, 신경망
                                                  수치예측: regression,  신경망
 
                                      2.  비지도 학습 : 정답이 없는 데이터로 학습 

                                         비지도 학습을 그림으로 가장 적절하게 설명한 내용:
 
                                           구글에서 검색창에 "머신러닝 얀루쿤 케익"

                               " 이 세상의 대부분의 데이터는 정답이 없는 데이터가 많습니다 "

                                 정답이 없는 데이터를 현업에서 어떻게 잘 활용할 것인가 ?

                                    1. aprori 알고리즘(8장) :  교보문고, 쿠팡, 아마존의 연관상품 추천,
                                                                     유튜브등 ..

                                    2. k-means 알고리즘(9장) : 통신사에 기지국을 세울때 어디에 기지국을
                                                                       세우는게 가장 효과가 좋을까? 

                                      3.  강화학습 

▩ 연관규칙이 필요한 이유 ?

  연관규칙이란 데이터 내부에 존재하는 항목간의 상호관계 또는 종속관계를 찾아내는 분석기법
  입니다.  데이터간의 관계에서 조건과 반응을 연결하는 분석으로 장바구니 분석 또는 서열 분석
   이라고 합니다.

 쿠팡 사례
 SSG 광고 

▩ Apriori 알고리즘 ?

   간단한 성능 측정치를 이용해서 거대한 데이터에서 데이터간의 연관성을 찾는 알고리즘 

▩ Apriori 알고리즘은 어떤 데이터의 패턴을 찾을 때 유용한가 ?

1.  암 데이터에서 빈번히 발생하는 DNA 패턴과 단백질의 서열을 검사할 때
2.  사기성 신용카드 및 사기성 보험 청구시 패턴 발견
3. 유통업에서는 장바구니 분석을 통해 상품 추천 뿐만 아니라 상품진열, 
   홈쇼핑의 경우에는 방송순서등에 작용하고 있습니다.

▩ 연관 규칙의 관련한 중요 용어 3가지 ?

1. 지지도 :  전체 거래중 항목 A 와 B를 동시에 포함하는 거래의 비율 

           1.1   X 아이템의 지지도 ?
                                                            n(X)      <----------  아이템 X 의 거래건수
                   표기식:  support(X)  =  --------------------- 
                                                             N        <------   전체 거래 건수 

           1.2   두 아이템 X 와 Y 의 지지도 ?

                                                        n(X∩Y)     <--------- 아이템 X 와 Y를 포함하는 거래건수
                  표기식:  support(X, Y) = --------------------
                                                             N        <--------- 전체거래건수

                             A 와 B 를 동시에 포함된 거래건수
  지지도 :  P(A∩B) = -------------------------------------
                                 전체 거래건수 

2. 신뢰도 :  A 상품을 샀을때 B 상품을 살 조건부 확률에 대한 척도  (p373)

  " 두 아이템의 연관규칙이 유용한 규칙일 가능성의 척도 "  
 
                                            support(X, Y)
표기식:  confidence( X → Y) = -------------------------
                                             support(X)

               P(A∩B)      A 와 B가 동시에 포함되는 거래건수
  신뢰도:  ----------- = --------------------------------------- = P(B | A) 
                 P(A)            A 를 포함하는 거래건수 


 신뢰도가 높을 수록 유용한 규칙일 가능성이 높다고 할 수 있습니다. 

3. 향상도 :  규칙이 우연에 의해 발생한것인지를 판단하기 위한 연관성 정도 척도 

  " 두 아이템의 연관규칙이 우연인지 아닌지를 나타내는 척도"

                                  C( X → Y)
 표기식:  lift( X → Y) = ---------------- 
                                     S(Y)
                                                                        
                                       P(B | A)                P(A ∩ B)           A와 B를 모두 포함하는 거래건수
  향상도:  lift( A → B) = --------------------= -------------------- = -------------------------------
                                        P(B)                   P(A)  * P(B)    A를 포함하는 거래건수x B를 포함하는 거래건수


  아이템 x 가 주어지지 않았을 때의 아이템 Y 의 확률대비 
  아이템 x 가 주어졌을때의 아이템 Y 의 확률증가 비율을 나타냅니다. 

  향상도                 설명                      예시
  향상도=1        서로 독립적인 관계     과자와 후추
  향상도 > 1      양(+) 의 상관관계       빵과 버터
  향상도 < 1      음(-) 의 상관관계        설사약과 변비약 

 
 거래번호          거래 아이템
    1                  우유, 버터, 시리얼
    2                   맥주, 기저귀
    3                  동요패드, 쌀과자 
    :                          :
    :                          :
   99                  동요패드, 쌀과자
   :                              :
   :                              :
 10000                치즈, 빵 

예제1.  다음의 데이터의 지지도를 구하시오 !


  거래번호              거래 아이템
      1                     우유, 버터, 시리얼
      2                     우유, 시리얼
      3                     우유, 빵
      4                     버터, 맥주, 오징어 

 
  지지도(우유 → 시리얼)  ?     우유와 시리얼을 동시에 구매할 결합확률 

                            A 와 B 를 동시에 포함된 거래건수               2
  지지도 :  P(A∩B) = ------------------------------------- = --------------- = 0.5 (50%의 확률)
                                 전체 거래건수                                    4


예제2. 우유를 구매할 때 시리얼도 구매할 신뢰도를 구하시오 !(조건부 확률)

  거래번호              거래 아이템 
      1                     우유, 버터, 시리얼
      2                     우유, 시리얼
      3                     우유, 빵
      4                     버터, 맥주, 오징어 

 신뢰도(우유 → 시리얼) ?  우유를 샀을때 시리얼도 구매할 조건부 확률. 

                          P(A∩B)      A 와 B가 동시에 포함되는 거래건수           2
  신뢰도(A → B):   ----------- = --------------------------------------- = ----------- = 0.66 (66%의 확률)
                             P(A)            A 를 포함하는 거래건수                      3    


예제3. 우유를 구매할 때 시리얼도 구매할 향상도를 구하시오 !

  거래번호              거래 아이템 
      1                     우유, 버터, 시리얼
      2                     우유, 시리얼
      3                     우유, 빵
      4                     버터, 맥주, 오징어 

 향상도(우유 → 시리얼) ?  우유를 샀을때 시리얼을 산다는게 우연인지 아닌지를 알아보는 확률


                                              C( 우유 → 시리얼)          (2/3)             4
 표기식:  lift( 우유 → 시리얼) = ------------------------ = -------------- = ------- = 1.33 
                                                  S(시리얼)                  (2/4)              3
                                                                        
                                       P(B | A)                P(A ∩ B)           A와 B를 모두 포함하는 거래건수
  향상도:  lift( A → B) = --------------------= -------------------- = -------------------------------
                                        P(B)                   P(A)  * P(B)    A를 포함하는 거래건수x B를 포함하는 거래건수

문제317.  아래의 지지도와 신뢰도를 각각 구하시오 !

                              지지도                신뢰도
 우유 → 시리얼           50%                   66%
 시리얼 → 우유           50%                  100%

문제318.  다음은 쇼핑물의 거래내역이다. 연관규칙: 우유  →  빵에 대한 신뢰도는 얼마인가 ?

  항목        거래수 
   우유         10
   빵            20
  우유, 빵     50
 빵,  초콜릿   40
 전체 거래건수 : 100 

                                                       P(우유 ∩ 빵)         ( 50/100)         50
 신뢰도(우유  →  빵 ) = P(빵 | 우유 ) = -------------------= ---------------- = ------ = 0.83
                                                        P(우유)                 (60/100)        60

문제319.  아래는 A 쇼핑물의 거래내역이다. 연관규칙 '과자 → 기저귀' 에 대한 지지도(support) 
              는 얼마인가 ?

 품목           판매수량
 과자              10
 기저귀           20
과자, 기저귀    40
기저귀, 맥주    20
전체판매수량   100

답:   0.4 (40%)   

문제320.  아래는 A 쇼핑물의 거래내역이다.  연관규칙 '과자 → 기저귀' 에 대한 신뢰도  
              는 얼마인가 ?

 품목           판매수량
 과자              10
 기저귀           20
과자, 기저귀    40
기저귀, 맥주    20
전체판매수량   100


답:  0.8        

문제321.(오늘의 마지막 문제)  아래의 데이터 프레임을 R 에서 만들고 기저귀를 샀을때 맥주도
            살 지지도, 신뢰도, 향상도를 각각 구하시오 !

x <- data.frame(
  beer=c(0,1,1,1,0),
  bread=c(1,1,0,1,1),
  cola=c(0,0,1,0,1),
  diapers=c(0,1,1,1,1),
  eggs=c(0,1,0,0,0),
  milk=c(1,0,1,1,1) )

지지도(맥주→기저귀) = ? 
신뢰도(맥주→기저귀) = ? 
향상도(맥주→기저귀) = ? 

5시 신호 보냈습니다.  마지막 문제 답 올리시고 자유롭게 자습 또는 스터디 하세요 ~~
6시 신호 보냈습니다. 

■ 컴퓨터가 어떻게 연관규칙을 찾는지 샘플로 알아보기 

  거래번호           아이템 목록
      1                   A, B, D
      2                   B, C
      3                   A, B, C, E
      4                   B, C, E 

쿠팡의 물류센터에서 동요패드와 쌀과자의 조합을 알아냈듯이 위의 거래를 보고
가장 적절한 아이템 목록의 조합을 알아내는게 우리의 목적입니다. 

첫번째 단계:  위의 아이템들에 대해서 아이템 1개에 대한 지지도를 구한다.
                  원래 지지도는 구매건수/전체구매건수 인데 그냥 단순하게 아이템의 갯수로
                  처리하겠습니다.

  거래번호           아이템 목록
      1                   A, B, D
      2                   B, C
      3                   A, B, C, E
      4                   B, C, E 

 아이템         지지도 
    A                2
    B                4
    C                3
    D                1
    E                 2

위의 결과에서 지지도가 1보다 큰것만 추출해서 다시 정리하시오 ~

  아이템          지지도
     A                2
     B                4
     C                3
     E                2

두번째 단계:  이제 아이템들간의 연관규칙을 알아야하므로 다시 아이템들간의 조합으로
                  구성하고 지지도를 다시 구한다. ( 2가지 조합)

  거래번호           아이템 목록   ---------------->  A, B, C, E  (지지도 1이상)
      1                   A, B, D
      2                   B, C
      3                   A, B, C, E
      4                   B, C, E 

 아이템 (A, B, C, E 에 대한 2가지 조합)      지지도 
 A   B                                                   2       
 A   C                                                   1
 A   E                                                   1
 B   C                                                   3
 B   E                                                    2
 C   E                                                    2

위의 결과에서 지지도 1보다 큰것으로 다시 정리하면 

   아이템                         지지도 
    A   B                             2
    B   C                             3
    B   E                             2
    C   E                             2

 A, B, C, E 로 만들 수 있는 3개의 조합으로 지지도를 구하면 ?

    아이템                지지도 

 A  B  C                      1     
 A  B  E                      1
 A  C  E                      1
 B  C  E                      2


  거래번호           아이템 목록   
      1                   A, B, D
      2                   B, C
      3                   A, B, C, E
      4                   B, C, E 

지지도가 1보다 큰것으로 다시 정리하면 ?

 B  C  E 

▩ 아프리오리 알고리즘 예제1. (맥주와 기저귀)

  금요일 밤에 남자들이 기저귀를 사러 마트에가면 맥주를 같이 사는 패턴을 발견

# 1. 데이터를 로드합니다.

x <- data.frame(
  beer=c(0,1,1,1,0),
  bread=c(1,1,0,1,1),
  cola=c(0,0,1,0,1),
  diapers=c(0,1,1,1,1),
  eggs=c(0,1,0,0,0),
  milk=c(1,0,1,1,1) )

#2.  arules 패키지를 설치한다. (연관규칙을 구현하는 패키지)

install.packages("arules")
library(arules)

#3. x 데이터 프레임을 행렬로 변환합니다. (arules 패키지가 데이터를 행렬로 제공받기 때문입니다)

x2 <- as.matrix( x, "Transaction")
x2

#4. arules 패키지의 apriori 함수를 이용해서 연관관계를 분석합니다. 

rules1 <- apriori( x2,  parameter=list(supp=0.2, conf=0.6, target="rules") )

# 설명: x2 데이터에서 지지도가 0.2 이상이고 신뢰도가 0.6 이상인 rule 를 발견해라 ~

rules1

set of 49 rules  # 49개의 연관품목들을 발견했습니다. 

#5. 연관품목들 확인하는 방법

inspect( sort(rules1) )

# 결과 설명 :  맥주를 샀을때 기저귀를 살 연관성이 가장 높고 그 다음으로는 기저귀를 샀을때
                   맥주를 살 연관성이 두번째로 높습니다. 

  3개의 조합으로는 맥주, 빵, 기저귀가 가장 연관성이 높습니다.

#6. 위의 연관규칙을 시각화 하기

install.packages("sna")
install.packages("rgl")
library(sna)
library(rgl)

b2 <-  t( as.matrix(x) ) %*% as.matrix(x)  # 희소행렬
b2

         beer bread cola diapers eggs milk
beer       3     2    1       3        1    2
bread      2     4    1       3       1    3
cola        1     1    2       2       0    2
diapers    3     3    2       4       1    3
eggs       1     1    0       1       1    0
milk        2     3    2       3      0    4

library(sna)
library(rgl)

diag(b2)   # b2 행렬의  대각선을 출력
diag(diag(b2)) # b2 의 대각선 행렬만 나오고 나머지는 0으로 출력

b3 <- b2 - diag(diag(b2))  # 대각선 행렬만 0 으로 나오고 나머지 행렬은 자기값 그대로 출력
b3       

gplot(b3 , displaylabel=T , vertex.cex=sqrt(diag(b2)) , vertex.col = "green" ,
             edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , 
             label.pos = 3 , edge.lwd = b3*2) 


설명: 

displaylabel=T 는 품목명을 출력하는 옵션
vertex.cex 는 출력되는 동그라미의 크기
vertex.col  는 동그라미 색깔 
edge.col   는 연결선 색깔
boxed.labels 는 품목명에 박스를 둘러주는 옵션
arrowhead.cex 는 화살표의 크기를 나타내는 옵션
label.pos 는 품목명의 위치(1은 원 아래에 출력되고, 2는 원 왼쪽에 출력됩니다.)
edge.lwd는 연결선의 굵기를 나타내는 옵션 

▩ 아프리오리 알고리즘 예제2  (보습학원과 연관성이 높은 업종은? )

 " 보습학원이 많은 건물에는 어떤 업종으로 입점해야 장사가 잘될까? "

#1. 데이터를 로드합니다.

bd <- read.csv("building.csv", header=T)
View(bd )

#2. NA 를 0으로 변경합니다.

bd[ is.na(bd) ] <- 0
bd

#3. 건물번호를 제외 시킵니다. 

bd2 <- bd[   , -1]  
bd2  

#4. 데이터 프레임을 행렬로 변환합니다.

bd3 <- as.matrix( bd2, "Transaction")  # 행렬로 변환합니다.

#5. 위의 행렬 데이터를 가지고 업종간의 연관분석을 하시오 !
library(arules)

rules2 <-  apriori( bd3,  parameter=list(supp=0.2, conf=0.6, target="rules"))
rules2  # 46개의 rule 이 발견되었습니다. 

#6. 연관규칙을 확인합니다.

inspect( sort(rules2) )

#7. 위의 결과에서 보습학원 부분만 따로 떼어서 출력해 봅니다. 

rules3 <-  subset( rules2, subset= lhs %pin% '보습학원' & confidence > 0.7 )

inspect( sort(rules3) )

         lhs                rhs    support confidence coverage lift count
[1] {보습학원}      => {은행}   0.2     1          0.2      5    4    
[2] {보습학원}      => {카페}   0.2     1          0.2      4    4    
[3] {보습학원,은행} => {카페}  0.2     1          0.2      4    4    
[4] {카페,보습학원} => {은행}  0.2     1          0.2      5    4   

결과해석: 보습학원이 있는 건물에는 은행이 많고 카페가 많다. 

문제322. 편의점이 있는 건물에  많은 업종은 무엇인가 ? 

rules4 <-  subset( rules2, subset= lhs %pin% '편의점' & confidence > 0.7 )
inspect( sort(rules4) )

문제324.  병원이 있는 건물에 가장 연관된 업종이 무엇인가 ?

rules5 <-  subset( rules2, subset= lhs %pin% '병원' & confidence > 0.7 )
inspect( sort(rules5) )

      lhs                  rhs          support confidence coverage lift    
[1] {병원}            => {약국}       0.25    0.8333333  0.30     3.333333

문제325.  보습학원이 있는 건물에 어떤 업종이 많이 있는지 연관분석을 한 결과를 시각화 하시오

#1. 데이터를 로드합니다.

bd <- read.csv("building.csv", header=T)
View(bd )

#2. NA 를 0으로 변경합니다.

bd[ is.na(bd) ] <- 0
bd

#3. 건물번호를 제외 시킵니다. 

bd2 <- bd[   , -1]  
bd2  

#4. 데이터 프레임을 이용해서 희소행렬을 출력합니다. 

bd3 <-  t(as.matrix(bd2))  %*% as.matrix(bd2)  # 컬럼을 row 로도 출력을 해줌 
bd3

# 5. 시각화 합니다.

library(sna)
library(rgl)

diag(bd3)   # bd3 행렬의  대각선을 출력
diag(diag(bd3)) # bd3 의 대각선 행렬만 나오고 나머지는 0으로 출력

bd4 <- bd3 - diag(diag(bd3))  # 대각선 행렬만 0 으로 나오고 나머지 행렬은 자기값 그대로 출력
bd4      

gplot(bd4 , displaylabel=T , vertex.cex=sqrt(diag(bd3)) , vertex.col = "green" ,
             edge.col="blue" , boxed.labels=F , arrowhead.cex = .3 , 
             label.pos = 3 , edge.lwd = bd4*2) 


문제326. 다음의 데이터를 이용해서 사람들끼리의 사회적 연결망 데이터 분석을 하시오 

  "사회적 연결망 데이터 분석" ----------->  사회학, 통계물리학 
              ↓
  우리책 12장에서 잠깐 소개하고 있음

   영어로는 social network analysis 라고 하며 줄여서 SNA 라고 합니다.
   사회적 연결망 분석은 개인과 집단간의 관계를 노드와 링크로서 모델링해
   그것의 구조나 확산 및 진화과정을 계량적으로 분석하는 방법론 입니다. 

   이때 노드란 분석하고자 하는 객체로서, 사람이나 사물등을 말합니다.
   이 노드간의 관계를 나타내는 연결을 링크라고 합니다. 

 그림: 사회적 원자 책 표지 


#1. 요번주에 친구를 만난 횟수를 희소행렬로 구성함

paper <- read.csv("paper1.csv" , header = T)  
paper[is.na(paper)] <- 0      # NA 를 0으로 변경합니다.
paper 

rownames(paper) <- paper[,1]  # 학생이름으로 row 의 이름을 지정합니다 
paper <- paper[-1]                # 학생이름 컬럼을 지웁니다. 
paper2 <- as.matrix(paper)  # 행렬로 변환합니다. 
paper2

#2. 요번주에 개별적으로 책을 읽은 시간 데이터를 로드한다.

book <- read.csv("book_hour.csv" , header = T)
book

library(sna) 


gplot(paper2 , displaylabels = T, boxed.labels = F , vertex.cex = sqrt(book[,2]) , vertex.col = "blue" , 
        vertex.sides = 20 , edge.lwd = paper2*2 , edge.col = "green" , label.pos = 3)

설명: vertex.cex = sqrt(book[,2]) 는 원의 크기를 책 읽는 시간으로 해라 ~
       edge.lwd = paper2*2 는 학생과 학생과의 연결선의 굵기 

▩ 파이썬으로 연관 규칙 구현하기 

#0. 아나콘다 프롬프트창을 열고 mlxtend 패키지를 설치합니다. 

pip  install mlxtend

#1. 데이터를 로드합니다.

dataset = [ ['사과', '치즈', '생수'],
               ['생수', '호두', '치즈', '고등어'],
               ['수박','사과','생수'],
               ['생수','호두','치즈','옥수수'] ]


#2. 필요한 패키지를 로드합니다.
import  pandas  as  pd
from  mlxtend.preprocessing  import  TransactionEncoder   # 연관분석을 위한 데이터 전처리
from  mlxtend.frequent_patterns  import   apriori    # 연관분석 패키지

#3. 연관 규칙 데이터 분석을 위한 데이터 전처리를 합니다.

te = TransactionEncoder()  # 데이터 전처리 클래스를 객체화 시킵니다. 
te.fit(dataset)                   # 데이터를 계산시킵니다.
te_ary = te.transform(dataset)  # 계산된 내용으로 데이터를 변경합니다 
te_ary

#4. 출력된 결과를 보기좋게 출력하기 위해 데이터 프레임으로 만듭니다.
df = pd.DataFrame( te_ary, columns=te.columns_)
df

#5. 지지도가 0.5 이상인것만 출력합니다.

apriori( df,  min_support=0.5,  use_colnames =True)

▩ 보습학원의 건물 데이터를 파이썬 아프리오리 알고리즘으로 연관분석하기 

#1. 필요한 패키지를 로드합니다.
import  pandas  as  pd
from  mlxtend.preprocessing  import  TransactionEncoder   # 연관분석을 위한 데이터 전처리
from  mlxtend.frequent_patterns  import   apriori    # 연관분석 패키지

#2. 데이터를 로드합니다.
dataset = pd.read_csv("d:\\data\\building.csv", encoding='CP949', index_col='Unnamed: 0')
dataset

#3. 결측치를 0 으로 치환합니다. 
dataset.fillna(0, inplace=True)
dataset.head()

#4. 판다스 데이터 프레임을 중첩 리스트로 변환합니다.
a = []
colnames = dataset.columns
colnames

for  j  in   range(0,20):  # 건물이 20개여서 20번 루프를 돌립니다. 
    b = []
    for  i, k  in  list( enumerate(colnames)):  # 번호와 컬럼명을 하나씩 가져옵니다. 예: (0, '병원')
        if  dataset.iloc[ j, i ] == 1.0:  #  데이터가 1.0 이면
            b.append(k)  # 컬럼명을 b 리스트에 append 시켜라 ~
        else:      # 1.0 이 아니면
            pass  # 아무것도 하지 말아라 ~
    a.append(b) # 만든 리스트를 a 리스트에 append 시켜라 

dataset = a  #  a 리스트를 dataset 에 담습니다. 
dataset 

# 5. 연관규칙 분석을 위한 데이터로 전처리합니다. 

te = TransactionEncoder()  # 전처리 클래스를 객체화 시킵니다.
te.fit(dataset)       # 1.0 을 True 로 변경하고 0.0 False 로 변경하는 계산작업을 합니다
te_ary =  te.transform(dataset)  # 계산된 내용을 변경된 데이터를 te_ary 에 담습니다. 
te_ary

# 전처리된 결과를 판다스 데이터 프레임으로 만듭니다.
df = pd.DataFrame( te_ary, columns=te.columns_ )   
df

# 6. 연관분석을 이용해서  지지도 0.1 이상인 업종 리스트를 출력합니다. 

apriori( df,  min_support=0.1, use_colnames=True)

문제327. 위에서 출력된 결과가 R 처럼 보기좋게 정렬이 되어서 출력되게하시오 !

 답글로 달아주세요 ~~  지지도가 높은것부터 나오게 해주세요 ~

- 건우 코드 

apri = apriori( df, min_support = 0.1 , use_colnames = True )

temp = []

for i in range(0,len(list( apri['itemsets'] )) ):  # 아이템 리스트의 갯수만큼 루프문을 돌리는데
    temp.append( len(list( apri['itemsets'] )[i] ) ) # 아이템 리스트안의 요소의 갯수를 temp 에 append 시킵

apri['item_len'] = temp  # apri 데이터 프레임에 요소의 갯수를 출력하는 컬럼이 생성

apri.sort_values( by = [ 'item_len', 'support' ] , ascending = False ) # itime_len 을 먼저 정렬하고 
# 이것을 정렬한것을 기준으로 support 를 정렬함 

#7. 신뢰도가 0.3 이상인것만 출력하시오 !

from  mlxtend.frequent_patterns import  association_rules 

result = association_rules( apri, metric='confidence', min_threshold=0.3) 
result 

#9. 지지도가 0.2 이상인것만 출력하시오!

from  mlxtend.frequent_patterns import  association_rules 

result2 = association_rules( apri, metric='support', min_threshold=0.2) 
result2 

#10. 위에서 중간에 생략된 행들을 다 출력되게하시오!

pd.set_option('display.max_rows', None)

result2.sort_values(by=['antecedents', 'support'] , ascending= False) 

문제328. 위에서 출력된 결과를 R 처럼 예쁘게 출력되겠금 하시오 !

from  mlxtend.frequent_patterns import  association_rules 

result = association_rules( apri, metric='support', min_threshold=0.2) 

result['cnt'] = result['antecedents'].apply(lambda x:len(x)) 
# antecedents 의 요소의 건수를 담는 파생변수를 cnt 로 생성
                                                                        
result.sort_values(by=['cnt', 'support'], ascending=[True,False])
# cnt 를 오름차 순으로 출력한것을 기준으로 지지도를 내림차순으로 출력한다. 

문제329. 위의 결과에서 보습학원에 대한것만 출력하시오! 

result[ result['antecedents'].apply(str).str.contains('보습학원') ]

#설명: result 데이터 프레임에 antecedents 컬럼을 문자형으로 다 변경하고 
        문자형 함수중에 contains 를 써서 '보습학원' 이 포함된 행들만 가져와라 ~

문제330.(오늘의 마지막 문제)  건물 데이터를 연관분석하는데 편의점이 있는 건물에는
             어떤 업종이 편의점과 연관성이 높은지 문제329 번 코드를 보습학원 아니라
             편의점으로 검색해서 출력하는데 지지도를 내림차순으로 출력한 결과를 기준으로
             신뢰도를 내림차순으로 출력해서 결과를 출력하시오 


5시 신호 보냈습니다. 마지막 문제 올리시고 자유롭게 자습 또는 스터디 하시면 됩니다. 
6시 신호 보냈습니다. 

▦   5십 4만건의 온라인 주문내역을 연관분석해서 가장 연관이 높은 상품들이 뭐가 있는지 알아보기

 실제 데이터 :  OnlineRetail.csv

데이터 소개:
InvoiceNo : 주문번호	 
StockCode : 상품코드	
Description : 상품설명
Quantity	:  주문량
InvoiceDate: 주문날짜
UnitPrice	:  달러로 환원했을때의 금액
CustomerID: 고객번호	
Country : 주문한 나라 

* 온라인 주문 내역을 연관분석하기 전에 알아야할 파이썬 문법 2가지?

  1.  groupby
  2.  unstack 

* groupby 판다스 문법 연습하기 

예제1.  사원 데이터프레임에서  부서번호, 부서번호별 토탈월급을 출력하시오 !

emp = pd.read_csv("d:\\data\\emp3.csv")

emp.groupby(['deptno'])['sal'].sum()

예제2. 직업, 직업별 평균월급을 출력하시오 !

emp.groupby('job')['sal'].mean().reset_index()

예제3.  월급이 1000 이상인 사원들의 직업, 직업별 토탈월급을 출력하시오 !

sal = emp[emp.sal>=100]

sal.groupby('job')['sal'].mean().reset_index()

예제4.  직업이 SALESMAN, ANALYST 인 사원들의 부서번호, 부서번호별 토탈월급을
           출력하시오 !

emp[emp['job'].isin(['SALESMAN','ANALYST'])].groupby('deptno')[['sal']].sum()

예제5. 아래의 SQL을 판다스로 구현하시오 !

SQL> select  deptno, job, sum(sal)
          from  emp
          group  by deptno, job
          order  by deptno, job;


Pandas> emp.groupby(['deptno','job'])['sal'].sum()

예제6. 위의 결과에서 판다스의 문법중에 하나인 unstack() 을 써서 결과를 출력하고
         어떤 결과인지 분석하시오 

Pandas> emp.groupby(['deptno','job'])['sal'].sum().unstack()

※ 설명: 데이터로 있던 직업(job) 이 컬럼이 되었습니다. 

예제7. 부서번호, 관리자번호,  부서번호별 관리자 번호별  평균월급을 출력하시오

emp.groupby(['deptno', 'mgr'])['sal'].mean()

예제8.  위의 결과에서 데이터로 있는 mgr 을 컬럼으로 출력하시오 !

emp.groupby(['deptno', 'mgr'])['sal'].mean().unstack()

예제9.  온라인 주문내역 데이터를 판다스 데이터 프레임으로 만드세요 ~

df = pd.read_csv("d:\\data3\\OnlineRetail.csv",encoding = 'unicode_escape' )
df.head()

예제10. stockcode  84406B 의 stockcode 와 Description 을 출력하시오 1

df.loc[ df.StockCode=='84406B', ['StockCode', 'Description'] ]

예제11.  df 데이터 프레임에서  주문한 나라가 United Kingdom 인 곳의 주문번호와
            Description 을 출력하시오 !

df.loc[ df.Country=='United Kingdom', ['InvoiceNo', 'Description'] ]

예제12. 주문한 나라가 영국인 데이터에서 주문번호, 주분번호별 토탈 수량(Quantity)
           을 출력하시오 !

mask2 = df.Country == 'United Kingdom'

df_mask2 = df.loc[mask2, :]

df_mask2.groupby('InvoiceNo')['Quantity'].sum().reset_index()

예제13.  주문한 나라가 영국인 데이터에서 주문번호, 상품설명(Description), 
            주문번호별 상품설명별 토탈수량(Quantity) 을 출력하세요 ~

df2 = df.loc[ df.Country == 'United Kingdom', : ]

df2.groupby( [ 'InvoiceNo', 'Description' ] )['Quantity'].sum().reset_index()

설명: 마이너스(-) 값은 주문취소로 보겠습니다. 

예제14. 위의 결과에서 데이터로 출력되던 상품설명(Description) 이 컬럼으로 출력되게
          하시오 !

df_uk = df.loc[df.Country=='United Kingdom', ]

df_uk.groupby(['InvoiceNo','Description'])['Quantity'].sum().unstack()

예제15.  주문번호에 알파벳 대문자 C 를 포함하는 주문내역의 모든 컬럼을 출력하시오!

df.loc[ df.InvoiceNo.str.contains('C'), :  ] 

예제16. 주문번호에 알파벳 대문자 C 를 포함하지 않는 주문 내역의 모든 컬럼을 출력하시오

df.loc[ ~df.InvoiceNo.str.contains('C'), :  ] 

예제17. 예제14번을 다시 출력하는데 주문번호에서 대문자 C 를 포함하지 않는 주문내역
           만 가지고 출력하시오 !

df2 = df.loc[ ~df.InvoiceNo.str.contains('C'), :  ] 

df2_uk = df2.loc[df2.Country=='United Kingdom', ]

result = df2_uk.groupby(['InvoiceNo','Description'])['Quantity'].sum().unstack()
result.shape  # (18668, 4188)  4188 건의 상품들이 있다는 것이다.

예제18. 위의 result 의 결과에서 NaN(결측치) 를 0 으로 변경하세요 ~

df2 = df.loc[ ~df.InvoiceNo.str.contains('C'), :  ] 

df2_uk = df2.loc[df2.Country=='United Kingdom', ]

result = df2_uk.groupby(['InvoiceNo','Description'])['Quantity'].sum().unstack()
result.fillna(0, inplace=True)
result

예제19.  위의 result 의 결과에서 수량이 1 이상이면 1 되게하고 0보다 작거나 같으면
            0 이 되겠금 데이터를 변경하시오 !

def function(x):
    if x >= 1.0:
        return 1
    else:
        return 0
    
result2 = result.applymap(function)
result2

여기까지는 연관분석을 하기 위한 데이터 전처리 였습니다. 
컬럼이 상품명들이고 데이터는 0 또는 1인 데이터로 변경해주는 작업이
연관분석을 하기위한 데이터 전처리입니다.

예제20. mlxtend 패키지의 아프리오리 함수를 이용해서 상품별로 가장 연관이 있는 상품이
          어떤것인지 연관분석을 하시오 !

from  mlxtend.frequent_patterns  import  apriori 

itemsets = apriori( result2, min_support=0.03, use_colnames=True)

설명: 지지도 0.03 이상인 상품들만 itemsets 에 담았습니다.

예제21. 위의 itemsets 를 가지고 향상도가 0.5 이상인 상품들의 연관규칙 출력하시오

from   mlxtend.frequent_patterns  import  association_rules

rules = association_rules( itemsets, metric='lift', min_threshold=0.5) 
rules

문제331. 프랑스에서 주문한 상품들의 연관 규칙을 출력하시오 ! 
            지지도 0.03 이상, 향상도 0.5 이상으로 영국과 똑같이 하세요 ~

import pandas as pd

df = pd.read_csv("c:\\pdata\\OnlineRetail.csv", encoding='unicode_escape')
df.head()
 
df2 = df.loc[ ~df["InvoiceNo"].str.contains("C"), : ]
df2_fr = df2.loc[df2["Country"]=="France", ]
result = df2_fr.groupby(["InvoiceNo","Description"])["Quantity"].sum().unstack()

result.fillna(0, inplace=True)

def function(x):
    if x >= 1.0:
        return 1
    else : 
        return 0
    
result3 = result.applymap(function)

from mlxtend.frequent_patterns import apriori
itemsets=apriori( result3, min_support=0.03, use_colnames=True )

from mlxtend.frequent_patterns import association_rules
rules = association_rules(itemsets, metric='lift', min_threshold=0.5)
rules

■  9장. k-means 알고리즘

  머신러닝의 종류 3가지 ?   1. 지도학습: 분류:  knn, naivebayes, decision tree, oneR, jriper
                                                     회귀 : 단순회귀, 다중회귀, 신경망

                                     2. 비지도학습 :  아프리오리 알고리즘(8장)
                                                           k-means (9장) 
                                     3. 강화학습 
 
▩ k-means(k개의 평균들) 군집화 이론이란 ?

  k-means  알고리즘은 주어진 데이터를 k 개의 클러스터로 묶는 알고리즘으로
  각 클러스터와 거리차이의 분산을 최소화하는 방식으로 동작한다. 

 이 알고리즘은 자율학습의 일종으로 레이블(정답)이 달려있지 않은 입력 데이터에
  레이블(정답)을 달아주는 역활로도 활용되고 있다.

  데이터에 정답을 다는 작업(라벨링)도 사람이 손으로 일일이 하는거라 실수를 할 수 있기
  때문에 k-means 의 분류 결과와 비교해서 혹시 차이가 있는지 확인해볼때 사용합니다.

 k-means 의 현업사례:  s모회사 텔레콤에서 k-means 알고리즘을 이용해서 
                               어디에 기지국을 세우는게 가장 효율적인가를 분석한 사례

* 책에 나오는 다른 사례

 1. 병원에서 암판별 머신러닝 모델을 만드는데 라벨링이 있는 지도학습 데이터를
    훈련 시킬때 비지도 학습을 같이 사용해서 모형의 정확도를 올리는데 참고 있다.

 2. 마케팅쪽에서는 세그멘테이션(segmentation)에 활용됩니다.
     고객들을 특성에 맞는 사람들끼리 군집화 합니다.

 3. 보험회사에서도 고객들을 segmentation 을 해서 맞춤형 보험상품 개발 및 광고에
    활용합니다. 

 4. 미국의 유명한 사례중 경찰의 순찰 지역을 정하는데 활용을 해서 범죄율을 줄이는데
    사용하고 있습니다. 영화 마이러니 리포트 처럼 범죄를 미리 예방하는데 사용을 합니다.

▩ k-means 기본 실습1

#1. 기본 데이터셋을 만듭니다.

c <- c(3,4,1,5,7,9,5,4,6,8,4,5,9,8,7,8,6,7,2,1)
row <- c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')
col <- c('X', 'Y')
data <- matrix( c, nrow=10, ncol=2, byrow=TRUE, dimnames=list(row,col) ) 
data

#2. 위에서 만든 데이터셋으로 plot 그래프를 그립니다.

plot(data)

#3. k-means 모델을 생성해서 중앙지점을 찾아냅니다.

km <- kmeans( data ,2 )  # data 를 2분류로 군집화해라 ~
km

km$cluster   #  10개의 데이터에 대해서 1군집인지 2군집이를 출력

cbind( data,  km$cluster )

km$centers  # 각각의 군집의 중앙점을 출력 

#4. 중앙지점을 시각화 합니다. 

plot( round(km$center),  col=km$center, pch=22, bg="dark blue", 
       xlim= range(0:10), ylim=range(0:10) )

#5. 원래 데이터를 위의 그래프에 합쳐서 출력하시오 !

plot( round(km$center),  col=km$center, pch=22, bg="dark blue", 
       xlim= range(0:10), ylim=range(0:10) )

par(new=T)  # 두개의 그래프를 합칠때 사용 

plot( data, col=c("blue","red"), xlim=range(0:10), ylim=range(0:10) )

문제332. 위의 결과를 다시 출력하는데 이번에는 3개로 군집화해서 출력하시오 


#1. 기본 데이터셋을 만듭니다.

c <- c(3,4,1,5,7,9,5,4,6,8,4,5,9,8,7,8,6,7,2,1)
row <- c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')
col <- c('X', 'Y')
data <- matrix( c, nrow=10, ncol=2, byrow=TRUE, dimnames=list(row,col) ) 
data

#2. 위에서 만든 데이터셋으로 plot 그래프를 그립니다.

plot(data)

#3. k-means 모델을 생성해서 중앙지점을 찾아냅니다.

km <- kmeans( data ,3 )  # data 를 3분류로 군집화해라 ~
km

km$cluster   #  10개의 데이터에 대해서 1군집인지 2군집이를 출력

cbind( data,  km$cluster )

km$centers  # 각각의 군집의 중앙점을 출력 

#4. 중앙지점을 시각화 합니다. 

plot( round(km$center),  col=km$center, pch=22, bg="dark blue", 
       xlim= range(0:10), ylim=range(0:10) )

#5. 원래 데이터를 위의 그래프에 합쳐서 출력하시오 !

plot( round(km$center),  col=km$center, pch=22, bg="dark blue", 
       xlim= range(0:10), ylim=range(0:10) )

par(new=T)  # 두개의 그래프를 합칠때 사용 

plot( data, col=c("blue","red","green"), xlim=range(0:10), ylim=range(0:10) )


▩ k-means 로 야채, 과일, 단백질 3가지를 분류하기

# 1. 데이터를 생성하기 ( x 축이 단맛, y 축은 아삭한 정도)

c <- c( 10, 9, 1, 4, 10, 1, 7, 10, 3, 10, 1, 1, 6, 7)
row <- c('apple','bacon', 'banana', 'carrot', 'salary', 'cheese', 'tomato')
col <- c('X', 'Y')
data <- matrix( c, nrow=7, ncol=2, byrow=TRUE, dimnames=list(row,col) )
data

#2. data 를 시각화 합니다.

plot(data)

#3. k-means 로 분류해서 시각화 해봅니다. 

install.packages("stats")
library(stats)

km <- kmeans( data, 3) 
km
cbind( data, km$cluster )

#4. factoexra 패키지를 이용해서 시각화 하시오 !

install.packages("factoextra")
library(factoextra)

km <- kmeans( data, 3) 

fviz_cluster( km, data=data, stand=F)

▩ k-means 이론 설명 

단계        방법                 설명  
 1       k개 객체 선택       초기 군집중심으로 k개의 객체를 임의로 선택한다. 
 2            할당              자료를 가장 가까운 군집 중심에 할당한다. 
 3        중심을 갱신         각 군집내의 자료들의 평균을 계산하여 군집 중심을 갱신한다.
 4            반복              군집 중심의 변화가 거의 없을 때 까지 단계2와 단계3을 반복합니다.

위의 단계를 설명한 그림 :

https://cafe.daum.net/oracleoracle/SeFi/458

문제333.  factoextra 로 아래의 데이터를 2개로 분류하고 시각화 하시오 !

      x     y
1     4    4 
2     8    4
3    15    8
4     24   4
5     24   12 

답:
# 1. 데이터를 생성하기 ( x 축이 단맛, y 축은 아삭한 정도)

c <- c( 4, 4, 8, 4, 15, 8, 24, 4, 24, 12 )
row <- c( 1, 2, 3, 4, 5) 
col <- c('X', 'Y')
data <- matrix( c, nrow=5, ncol=2, byrow=TRUE, dimnames=list(row,col) )
data

#2. factoexra 패키지를 이용해서 시각화 하시오 !

install.packages("factoextra")
library(factoextra)

km <- kmeans( data, 2) 

fviz_cluster( km, data=data, stand=F)


▩ k-means  실습 ( 국영수 점수 데이터 )  

#1. 데이터를 로드합니다.
academy <-  read.csv("academy.csv")
academy

#2. 수학점수와 영어점수만 선택합니다.
a2 <- academy[   , c(3,4) ]
a2

#3. k 값을 4로 주고 비지도학습 시켜서 모델을 생성합니다. 

km <- kmeans( a2 , 4)
km

#4. 학생번호, 수학점수, 영어점수, 분류번호가 같이 출력되게 합니다.

result <- cbind( academy[    , c(1,3,4)], km$cluster )

result

 1. 영어, 수학 둘다 잘하는 학생
 2. 영어, 수학 둘다 못하는 학생
 3. 영어를 잘하는데 수학을 못하는 학생
 4. 수학은 잘하는데 영어를 못하는 학생

#5. 시각화를 합니다. 

library(factoextra)

fviz_cluster(  km,  data=a2,  stand=F)

▩ 유방암 데이터의 악성종양과 양성종양을 k-means 로 2개로 군집화해서 분류해보기 

 " 정답 없이 데이터만 보고 분류하는 알고리즘 "

# 1. 데이터를 로드합니다.
wisc <- read.csv("wisc_bc_data.csv", header=T)
head(wisc)
ncol(wisc)  # 32

# 2. 필요한 컬럼을 선택합니다. 
wisc2 <- wisc[   , 3:32 ]   # 환자id 와 정답을 제외한 나머지 컬럼들
head(wisc2)

# 3. k-means 모델을 생성합니다. 

km <- kmeans( wisc2, 2)
cbind( wisc$diagnosis, km$cluster)

# 4. 시각화 합니다.

library(factoextra)

fviz_cluster( km,  data=wisc2, stand=F)

# 5. 정확도를 확인하여 성능을 평가합니다. 
library(gmodels)
CrossTable( wisc$diagnosis,  km$cluster)


 486 /569  0.85의 정확도를 보입니다. 나쁘지 않은 정확도가 나왔습니다. 

문제334. 이번에는 데이터를 정규화 하고 정확도를 확인하시오 ~

힌트코드:

normalize <- function(x) {
                          return   ((x-min(x)) /  ( max(x) - min(x) ) ) 
                                }

wisc_n <-  as.data.frame( lapply( wisc2, normalize )   )

0.9279438

답:
# 1. 데이터를 로드합니다.
wisc <- read.csv("wisc_bc_data.csv", header=T)
head(wisc)
ncol(wisc)

#2. 필요한 컬럼만 선택합니다.
wisc2 <- wisc[   , 3:32 ]   # 환자id 와 정답을 제외한 나머지 컬럼들
head(wisc2)

# 3. 정규화를 진행합니다.
normalize <- function(x) {
  return   ((x-min(x)) /  ( max(x) - min(x) ) ) 
}

wisc_n <-  as.data.frame( lapply( wisc2, normalize )   )

# 4. 모델을 생성합니다.
km <- kmeans( wisc_n, 2)
cbind( wisc$diagnosis, km$cluster)

# 5. 시각화를 합니다.
library(factoextra)

fviz_cluster( km,  data=wisc2, stand=F)

# 6. 정확도를 확인합니다. 
library(gmodels)
CrossTable( wisc$diagnosis,  km$cluster)

문제335. 위에서는 지금 min max 정규화를 했는데 머신러닝 학습 시킬때는 scale 함수로
            표준화 하는것보다 정규화를 하는게 더 성능이 좋다고 하는데 진짜로 그런지 실험하세요

표준화 :  평균이 0이고 표준편차가 1인 데이터로 변경 (함수: scale)
정규화 : 데이터를 0~1 사이로 변경   ( normalize )

문제336. (오늘의 마지막 문제) iris 데이터를 k-means 를 이용해서 3개로 분류하고 factoextra 시각화 
            하고 시각화된 결과 이미지를 카페에 답글로 올리세요 ~~~
 
          데이터셋 : iris2.csv 


5시 신호 보냈습니다.  나머지 시간은 마지막 문제 올리고 자유롭게 자습 또는 스터디 하세요 ~~
6시 신호 보냈습니다. 

■ 머신러닝 수업 복습

 1.  R 기초문법
 2.  이 책을 보기 위한 기본 문법들
 3.  머신러닝 :
                     1. 지도학습  :    분류:   knn, naivebayes, decision tree, oneR, jriper, 신경망
                                           회귀:   단순회귀, 다중회귀,  신경망
                     2. 비지도학습 :  연관규칙, k-means 
                     3. 강화학습  

  k-means 알고리즘 실습을 위하여 사용한 데이터 셋 :    1.  국영수 데이터
                                                                           2.  유방암 데이터
                                                                           3.  iris 데이터  

 kmeans 함수를 가지고 정답없는 데이터를 분류했습니다.


                       knn                    vs                k-means 

 목표 :              분류                                        분류 

공통점:          유클리드 거리                           유클리드 거리    

차이점:          정답이 있다                                정답이 없다
                        ↓                                             ↓
                    지도학습                                   비지도 학습 

뜻    :   k 개의 가까운 이웃을 자기 이웃으로           k개의 평균들을 정해서 그 평균들이 바로
          보겠다                                                  분류되는 군집집단의 중심점이 된다.
                                                                     중심점이 계속 이동되다가 나중에 중심점이
                                                                     이동되지 않을때가 바로 분류가 완성되는 시점 
                                                                      이다. 

▩ knn 함수를 R 로 생성하기1 (과일 데이터)

fruits <- data.frame( '재료'=c('사과','베이컨','바나나','당근','샐러리','치즈'),
                            '단맛'=c(10,1,10,7,3,1),
                            '아삭한맛'=c(9,4,1,10,10,1),
                            '음식종류'=c('과일','단백질','과일','채소','채소','단백질')
)

tomato <- c(7,8)

코드 구현 목표 :   토마토와 가장 가까운 거리에 있는 재료의 음식종류를 자동으로 알아내자 ~

fruits 

예제1.  R 을 이용해서 아래의 두 점의 유클리드 거리를 구하시오 !

a = c( 2, 5 )
b = c(4, 7 ) 

sqrt(sum((b-a)^2))  

예제2.  위의 코드를 이용해서 distance 라는 함수를 만드시오 !

distance(a,b)  # 2.828427

distance  <-  function(x,y) {
                                      return ( sqrt(sum((x-y)^2))  )
                                   }

예제3.  위에서 만든 distance 를 이용해서 fruits 데이터 프레임에서 x, y 좌표와
          토마토의 좌표인 c(7,8) 과의 거리를 구하시오 !

fruits[ ,c(2,3)]     와   c(7,8) 의 거리를 각각 구하시오 ~

distance( fruits[ 행의 숫자 ,c(2,3)] , c(7,8)  )

답:

for (i  in 1:nrow(fruits)) {
                 
  a <- distance( fruits[ i ,c(2,3)] , c(7,8)  )
  print(a) 
                            }

예제4. 위에서 출력되고 있는 거리값들을 temp 라는 변수에 append 되게 하시오 !

temp <- c()    # 파이썬에서 비어있는 리스트 만드는 문법과 같은 문법 
for (i  in 1:nrow(fruits)) {
                 
  a <- distance( fruits[ i ,c(2,3)] , c(7,8)  )
  temp <- append(temp, a)   # temp 변수에 a 값을 하나씩 append 하겠다.
                            }

예제5.  지금 만든 temp 변수를 fruits 데이터프레임의 파생변수로 추가하시오 !
          컬럼명은 d 라고 하세요 !

fruits$d <- temp
fruits

예제6.  fruits 데이터프레임에서 d 의 값을 기준으로 순위를 출력하세요 ~

rank( fruits$d,  ties.method='min')

[1] 2 4 5 1 3 6

예제7. 위의 결과를 fruits 의 파생변수로 추가하시오 ! 파생변수명은 rnk 입니다.

fruits$rnk <-  rank( fruits$d,  ties.method='min')
fruits

예제8.  fruits 에서 순위가 1부터 3위까지에 해당하는 음식종류를 출력하시오 !

fruits[ fruits$rnk <= 3,  '음식종류']

[1] "과일" "채소" "채소"

예제9.  위의 결과에서 최빈값을 출력하시오 !

top <-  fruits[ fruits$rnk <= 3,  '음식종류']

names( which.max(table(top) )  )

[1] "채소"

예제10.  위에서 예제1번부터 예제9번까지 작성한 코드들을 모두 조합해서 
            아래의 myknn 이라는 함수를 생성하시오 !  

result1 <- knn(train=wbcd_train,  test=wbcd_test,   cl=wbcd_train_label,  k=3)
                               ↑                      ↑                ↑                   ↑
                           훈련 데이터      테스트 데이터       라벨               k값

myknn( train= fruits[  , c(2,3)],  test=c(7,8) ,  cl=fruits$음식종류, k=3)

채소

답: 
myknn <- function( train, test, cl, k )  {   # 빈 대학교에서 만든것처럼 입력 매개변수를 똑같이
              temp <- c()                        # 파이썬에 비어있는 리스트를 만드는 문법과 같다
              for ( i  in  1:nrow(train) ) {      #  테스트 데이터와 훈련 데이터의 거리를 구해서 
                        a <- distance( train[ i , ], test )
                        temp <- append( temp, a)   # temp 변수에 입력하고 
                                               }
              train$d <-  temp                   # temp 변수를 train 데이터프레임에 파생변수로 추가
              train$rnk <- rank(train$d, ties.method='min')  # 순위 컬럼을 추가하고서 
              train <- cbind( train, cl )  # 라벨컬럼을 train 데이터프레임에 추가합니다. 
              top <- train[train$rnk <= k, 'cl' ]  # 순위가 k 개까지의 라벨을 top 에 담고 
              return  ( names( which.max(table(top)) )  )  # 그중에 최빈값을 출력합니다. 
                                                    }

myknn( train= fruits[  , c(2,3)],  test=c(7,8) ,  cl=fruits$음식종류, k=3)    46분까지 쉬세요 ~


건우 코드:

fruits <- data.frame( '재료'=c('사과','베이컨','바나나','당근','샐러리','치즈'),
                      '단맛'=c(10,1,10,7,3,1),
                      '아삭한맛'=c(9,4,1,10,10,1),
                      '음식종류'=c('과일','단백질','과일','채소','채소','단백질')
)

myknn <- function ( train, test, cl, k ) {
                                                   temp <- c()
                                                   for ( i in 1:nrow(train) ) {
                                                               temp <- append( temp,  sqrt( sum ( ( train[ i ,]- test )^2 )  ) )
                                                                               }
                                                   x <- data.frame(cl)
                                                   x$rnk <- rank(temp, ties.method = 'min')
                                                   top <- x[ x$rnk <= k , 1 ]

                                                   return ( names( which.max( table(top) ) ) )
                                                    }
 

myknn ( train = fruits[   , c(2,3)] , test = c(7,8),  cl = fruits$음식종류, k =3  )

▩ 유방암 데이터를 myknn  함수에 넣어서 분류해보기 

github 에 내가 짠 코드를 올려서 다운받게 해놓으면 여러모로 굉장히 유용해집니다.

지난번 2장에서 사용했던 유방암 데이터 분류 전체 코드

# 유방암 데이터를 원래 knn 으로 분류하는 코드

#▩ 1단계: 데이터 수집

wbcd <- read.csv("d:\\data\\wisc_bc_data.csv")

nrow(wbcd)   # 행의 갯수   569개
ncol(wbcd)    # 열의 갯수   32개
str(wbcd)

#▩ 2단계: 데이터 탐색   

# 1.  정답에 해당하는 라벨 컬럼의 데이터 분포를 확인 합니다.

table(wbcd$diagnosis)

#2. 이상치를 확인합니다.

install.packages("outliers")
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(wbcd)  # wbcd 의 컬럼명 확인 

a <- grubbs.flag(wbcd$radius_mean)  # wbcd 의 radius_mean 컬럼에 이상치가 있는지 확인 
#  하기위해 데이터 프레임을 생성 
a[a$Outlier==TRUE,     ]               # a 데이터 프레임의 Outlier 컬럼이 TRUE 인 데이터를 


for  ( i  in  3 : length(colnames(wbcd) ) ) {
  
  a <- grubbs.flag(wbcd[  ,  colnames(wbcd)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(wbcd)[i], ' --->', length(b) )  )
  
}

#3. 결측치를 확인합니다. 

colSums(is.na(wbcd))

#4. 히스토그램 그래프 + 정규분포 그래프를 통해서 데이터들이 정규성을  보이는지 확인합니다.

#install.packages("psych")
library(psych)
pairs.panels(wbcd[c("dimension_se", "symmetry_se","perimeter_se")] )

# ▩ 3단계 : 데이터로 모델 훈련 

# 1. 데이터의 구조를 확인하여 라벨(정답) 컬럼이 factor 인지 확인합니다. 

str(wbcd)  

wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

str(wbcd)

#2.  데이터를 양성과 악성이 잘 섞일 수 있도록 데이터를 섞어 줍니다. 

set.seed(1)

wbcd_shuffle <- wbcd[ sample(nrow(wbcd)) ,  ]

wbcd_shuffle

#3.   훈련 할때 필요한 컬럼만 선택합니다.   

wbcd_shuffle2 <- wbcd_shuffle[   , c(-1,-2) ] # 환자번호와 정답을 제외합니다. 

wbcd_shuffle2

#4. 컬럼들의 단위가 다 다르므로 데이터를 정규화 합니다. 

normalize <- function(x) {  return ( (x-min(x))  / ( max(x) - min(x) )  )   }

wbcd_n <- as.data.frame( lapply( wbcd_shuffle2, normalize )  ) 

summary(wbcd_n)

#5. 전체 569개의 행의 데이터를 훈련(공부) 데이터와 테스트(시험) 데이터로
#   나눠줘야합니다. ( 훈련 데이터 9, 테스트 데이터 1 로 나눈다)

nrow(wbcd_n) #569

n_90 <-  round(0.9*nrow(wbcd_n) )
n_90  # 512

wbcd_train <- wbcd_n[ 1:512,   ]            # 훈련 데이터 구성   
wbcd_test  <- wbcd_n[ 513 : 569,  ]        # 테스트 데이터 구성

nrow(wbcd_train)  # 512
nrow(wbcd_test)   # 57

# 6. 정답도 훈련과 테스트로 나눕니다. 

head(wbcd_shuffle[  ,  2])

wbcd_train_label <- wbcd_shuffle[ 1: 512 ,  2]
wbcd_test_label <- wbcd_shuffle[ 513 : 569 ,  2]

length(wbcd_train_label )  # 512
length(wbcd_test_label)    # 57

#7. 512개의 훈련 데이터와 훈련 데이터의 정답으로 거리계산한 데이터로
#   테스트 데이터 57개를 분류 합니다. 

#install.packages("class")
library(class)

result1 <- knn(train=wbcd_train,  test=wbcd_test, cl=wbcd_train_label, k=1)
result1

#▩ 4단계. 모델 성능 평가

result1 == wbcd_test_label

sum(result1 == wbcd_test_label)

sum(result1 == wbcd_test_label)/57 *100

library(gmodels)

CrossTable( x=wbcd_test_label, y=result1, prop.chisq=FALSE)


과일 데이터는 아래의 함수를 이용해서 분류를 했었는데 유방암 데이터를 컬럼이 30개나 되므로
모두 다 적용할 수 있도록 아래의 코드를 변경해야하고 결과값도 아래의 함수는 하나만 출력하게
되어있는데 여러개가 출력될 수 있도록 코드를 변경해줘야합니다. 

 1.  컬럼 30 개에 대한 거리계산을 해야한다.
 2.  출력되는 값이 여러개의 값이 출력되도록 해야한다. 

                 ↓   
유럽 오스트리아 빈에서 만든 knn 함수는 loop 문을 사용하지 않고 거리계산을 해서 
빠르게 결과를 출력하는 함수 입니다. 만약 loop 을 사용했다면 이렇게 빨리 결과가 나오지 
않습니다.

          훈련  데이터 (512개)  --------------  테스트 데이터( 57개 )

   과일 데이터는 토마토 한개와 훈련 과일 6개와의 거리를 구해서 거리가 6개가  출력되었고
   그 6개의 거리중에서 순위 3위까지의 음식종류를 구한것였다. 

  유방암 데이터는 테스트 데이터 57개에서 1번부터 훈련 데이터 512개와의 거리를 구해서
  거리 512개중에 k 개의 정답들중 최빈값을 작업을 출력해서 분류하는 작업을 총 몇번해야하나요?
                                      k =3  
  테스트 데이터 1번 :    악성, 악성, 양성      --->  악성 
  테스트 데이터 2번 :    양성, 양성, 악성      --->  양성
         :                               :                            :
  테스트 데이터 57번:   악성, 악성, 악성      --->  악성 



오전에 만든 함수 (건우코드)

myknn <- function ( train, test, cl, k ) {
                                                   temp <- c()
                                                   for ( i in 1:nrow(train) ) {
                                             temp <- append( temp,  sqrt( sum ( ( train[ i ,]- test )^2 )  ) )
                                                                               }
                                                   x <- data.frame(cl)
                                                   x$rnk <- rank(temp, ties.method = 'min')
                                                   top <- x[ x$rnk <= k , 1 ]

                                                   return ( names( which.max( table(top) ) ) )
                                                    }

▩ for loop 문을 사용하지 않고 유방암 데이터의 512 x 57 개의 거리를 계산하는 방법 

게시글  1419번   ■ knn 함수를 직접 만들기 위한 문제들 

예제1.  아래 5 x 4 행렬에서  5 x 4 행렬의 차는 ?  

    a                 b

4 3 2 5       3  4  3  2
2 4 2 9       2  1  3  2
8 2 3 3   -   3  2  3  2      =              ?
9 3 2 4       1  2  2  1
3 2 1 2       2  1  4  3

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2,2,1,3,2,3,2,3,2,1,2,2,1,2,1,4,3),  nrow=5, ncol=4, byrow=T)
a-b

예제2.  아래 5 x 4 행렬에서  1 x 4 행렬의 차는 ?  

   a                 b
4 3 2 5       3  4  3  2             4  3  2  5          3   4  3  2          1   -1   -1  3
2 4 2 9                                 2  4  2  9          3  4  3   2        -1   0    -1  7
8 2 3 3   -                       =   8  2  3  3       -  3  4  3   2   =    5  -2    0   1
9 3 2 4                                9  3  2  4          3  4  3   2          6  -1  -1   2
3 2 1 2                                3  2  1  2          3  4  3   2          0  -2   -2  0 

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)
a - b

rror in a - b : non-conformable arrays   <---- 에러가 나면서 수행되지 않습니다. 

예제3.  위의 a 행렬과 b 행렬을 각각 전치해서 서로의 차이를 구하고 나서 결과값을 다시 전치
           시키시오 !

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)

d <- t(a)-c(t(b))   #  a행렬을 전치 시킨 결과와  b행렬을 전치시키고 상수화 시킨 결과와의 차이를 출력

t(d)   # d를 전치시키면 원래 우리가 출력하려던 결과와 같아집니다. 

-1   -1   -1   3 
-1    0   -1   7
 5    -2   0   1
 6    -1   -1  2
 0    -2   -2   0


예제4.   t( t(a)-c(t(b)) )  이 코드를 이용해서 오전에 작성했던 코드의 for loop 을 제거하시오 !

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)

myknn <- function ( train, test, cl, k ) {
          temp <- t( t(train)-c(t(test))  )
          print (temp)
                                                  }
          
myknn(a,b)

-1   -1   -1   3 
-1    0   -1   7
 5    -2   0   1
 6    -1   -1  2
 0    -2   -2   0

설명: 훈련 데이터 5개와 테스트 데이터 1개와의 차이(5행 4열)를 출력하는 함수 

예제5.  차이만 출력하는 예제4번의 코드를 수정해서 아래와 같이 제곱이 된 결과가 출력되게하시오 

-1^2   -1^2   -1^2   3^2 
-1^2    0^2   -1^2   7^2
 5^2    -2^2   0^2   1^2
 6^2   -1^2   -1^2  2^2
 0^2    -2^2   -2^2   0^2

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)

myknn <- function ( train, test, cl, k ) {
          temp <- t( t(train)-c(t(test))  )^2
          print (temp)
                                                  }
          
myknn(a,b)

예제6.  위의 결과에서 제곱된 아래의 결과를 행별로 다 합쳐야 유클리드 거리가 출력되므로
          행의 값들을 다 더하시오 !  ( 함수 : 행의 값들을 다 더하는 함수인 rowSums 를 사용하면 됩니다 )


 1    1    1    9
 1    0    1   49
 25    4    0    1
 36    1    1    4
 0    4    4    0

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)

myknn <- function ( train, test, cl, k ) {
          temp <- t( t(train)-c(t(test))  )^2
          print (rowSums(temp))
                                                  }
          
myknn(a,b)

예제7. 위에서 출력된 결과에 루트를 씌워서 출력되게하시오 !

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)

myknn <- function ( train, test, cl, k ) {
          temp <- t( t(train)-c(t(test))  )^2
          print ( sqrt(rowSums(temp)) )
                                                  }
          
myknn(a,b)                                                                        45분까지 쉬세요 ~~

 3.464102 7.141428 5.477226 6.480741 2.828427

예제8. 아래의 a 행렬에 암에 대한 라벨 컬럼을 생성하시오 !

 a      
          
4 3 2 5  M            
2 4 2 9  M                      
8 2 3 3  B              
9 3 2 4  B                      
3 2 1 2  B       

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
c = matrix( c('M','M','B','B','B'), nrow=5, ncol=1 )
a2 = cbind(a,c)
a2

예제10.  위에서 만든 a2 를 아래의 데이터 프레임으로 생성하시오 !

x y z k  diagnosis      
4 3 2 5  M            
2 4 2 9  M                      
8 2 3 3  B              
9 3 2 4  B                      
3 2 1 2  B     

a3 <- as.data.frame( a2 ) 
names(a3) <- c('x','y','z','k','diagnosis')
a3

예제11. 위에서 만든 데이터프레임 a3 와  b 와의 거리들이 아래와 같이 출력되게하시오

myknn <- function ( train, test, cl, k ) {
          temp <- t( t(train)-c(t(test))  )^2
          print ( sqrt(rowSums(temp)) )
                                                  }

a = matrix( c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2),  nrow=5, ncol=4, byrow=T)
c = matrix( c('M','M','B','B','B'), nrow=5, ncol=1 )
a2 = cbind(a,c)
a3 <- as.data.frame( a2 ) 
names(a3) <- c('x','y','z','k','diagnosis')
b = matrix( c(3,4,3,2),  nrow=1, ncol=4, byrow=T)

myknn(a, b)

3.464102 7.141428 5.477226 6.480741 2.828427

예제12.  위에서 거리가 출력되었는데 아래와 같이 myknn 을 실행하면 가장 가까운 거리의
             라벨이 아래와 같이 출력되게하시오 !

myknn <- function ( train, test, cl, k ) {
          temp <- t( t(train)-c(t(test))  )^2
          print ( sqrt(rowSums(temp)) )
                                                 }

my_knn( a3[,1:4], b, a3$diagonosis, 3) 

B

힌트 :  train <-as.numeric(train)  #숫자로 형 변환(인훈이 코드 참고)

#데이터 생성

a=matrix(c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2), nrow=5, ncol=4, byrow = T)
label<-matrix(c('M','M','B','B','B'), nrow=5)
b=matrix(c(3,4,3,2), nrow=1, ncol=4, byrow=T)
a2<-cbind(a,label)
a3<-as.data.frame(a2)
names(a3)<-c('x','y','z','k','diagonosis')

답:

#데이터 생성

a=matrix(c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2), nrow=5, ncol=4, byrow = T)
label<-matrix(c('M','M','B','B','B'), nrow=5)
b=matrix(c(3,4,3,2), nrow=1, ncol=4, byrow=T)
a2<-cbind(a,label)
a3<-as.data.frame(a2)
names(a3)<-c('x','y','z','k','diagonosis')  # 유방암 데이터와 같이 데이터프레임으로 변환
 
#KNN 함수
my_knn <- function ( train, test, cl, k) {  # 오스트리아 빈 대학에서 만든 함수와 동일하게 구성
        train<-as.numeric(train)   # 숫자로 형 변환
        temp <- t(t(train)-c(t(test)))^2   # 유클리드 거리계산식을 구현하기 위해 거리차를 구하고
        x<-sqrt(rowSums(temp))          # 거리차의 제곱의 행들을 다 더해서 거리를 출력한다.
        x<-as.data.frame(x)                 # 거리값들을 데이터프레임으로 변환하고
        x2<-cbind(cl,x)                       # 데이터 프레임 맨 앞에 정답을 붙여서 x2 를 만든다.
        x2$rank<-rank(x,ties.method = 'min') # 거리에 순위를 부여해서
        top<-x2[x2$rank<=k,1]                   # 그 순위가 k 값 이하인 데이터들중 1번째 컬럼의
        return( names( which.max( table(top) ) ) ) # 데이터를 출력해서 그 중 최빈값을 출력 
}

my_knn(a[,1:4], b, a3$diagonosis, 3) #훈련 데이터에 라벨 컬럼 제외

예제13.  위에서는 훈련 데이터 5개와 테스트 데이터 1개와의 거리를 계산해서 라벨 B  하나를
           출력했는데 이번에는 진짜 유방암 데이터 처럼 테스트 데이터를 4개 만들어서 
           이 4개에 대한 라벨 4개가 출력되게하시오 !

 훈련 데이터                             테스트 데이터 
x y z k  diagnosis                     x  y  z  k 
4 3 2 5  M                              3  4 3  2
2 4 2 9  M                              7  1 4  3
8 2 3 3  B                               2  2  1  1
9 3 2 4  B                               3  2  4  8
3 2 1 2  B     

my_knn(a[,1:4], b, a3$diagonosis, 3)                         답글로 올려주세요 ~~

B  M  M  B 

답:

#데이터 생성
a=matrix(c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2), nrow=5, ncol=4, byrow = T)
label<-matrix(c('M','M','B','B','B'), nrow=5)
a2<-cbind(a,label)
a3<-as.data.frame(a2)
names(a3)<-c('x','y','z','k','diagonosis')
b=matrix(c(3,4,3,2,7,1,4,3,2,2,1,1,3,2,4,8), nrow=4, ncol=4, byrow=T)

#KNN함수
my_knn <- function (train, test, cl, k) {
        train<-as.numeric(train)
        result<-c()
        for (i in 1:nrow(test)){
                temp <- t(t(train)-c(t(test[i,])))^2
                temp<-matrix(temp,nrow=length(cl), byrow=T)
                x<-sqrt(rowSums(temp))
                x<-as.data.frame(x)
                x2<-cbind(cl,x)
                x2$rank<-rank(x,ties.method = 'min')
                top<-x2[x2$rank<=k,1]
                result<-append(result,names( which.max( table(top) ) ) )       
        }
        return(result)
}

my_knn(a[,1:4],b,a3$diagonosis,3)  # 인훈이가 다시 수정해서 올린 코드 참고해주세요 ~~~~~   45분까지 쉬세요


■ 오늘 배운 내용을 총정리해서 유방암 코드에 적용하는 전체 코드

a=matrix(c(4,3,2,5,2,4,2,9,8,2,3,3,9,3,2,4,3,2,1,2), nrow=5, ncol=4, byrow = T)
label<-matrix(c('M','M','B','B','B'), nrow=5)
a2<-cbind(a,label)
a3<-as.data.frame(a2)
names(a3)<-c('x','y','z','k','diagonosis')

b=matrix(c(3,4,3,2,7,1,4,3,2,2,1,1,3,2,4,8), nrow=4, ncol=4, byrow=T)


#KNN함수
myknn <- function(train, test, cl, k) {       # 빈 대학교에서 만든 입력매개변수와 똑같이 구성
  pred <- c()                                       # 파이썬의 비어있는 리스트 만드는 문법과 같음
  for (i in 1:nrow(test)) {                         # 테스트 데이터의 갯수만큼 반복문을 수행하는데 
    temp <- t(t(train)-c(t(test[i,])))^2          # 훈련데이터와 테스트 데이터의 차를 제곱해서
    temp <- sqrt(rowSums(temp))             # 그값에 루트를 씌우고 temp 에 입력하고 
    table <- data.frame(train, kind=cl, d=temp)  # 훈련 데이터와 라벨와 계산된 거리를 데이터프레임
    table$rnk <- rank(table$d, ties.method = 'min') # 으로 생성하고 맨끝에 순위 컬럼을 입력한다
    top <- table[table$rnk <= k,'kind']            # 순위가 k 값 이하인 라벨들을 top 에 담고
    pred <- append(pred, names(which.max(table(top)))) # 최빈값을 pred 라는 변수에 append 시킴
  }
  return(pred)          # 최빈값 라벨들을 출력합니다. 
}

my_knn(a[,1:4], b, a3$diagonosis, 3)


####################################################################


# 유방암 데이터를 원래 knn 으로 분류하는 코드

#▩ 1단계: 데이터 수집

wbcd <- read.csv("d:\\data\\wisc_bc_data.csv")

nrow(wbcd)   # 행의 갯수   569개
ncol(wbcd)    # 열의 갯수   32개
str(wbcd)

#▩ 2단계: 데이터 탐색   

# 1.  정답에 해당하는 라벨 컬럼의 데이터 분포를 확인 합니다.

table(wbcd$diagnosis)

#2. 이상치를 확인합니다.

#install.packages("outliers")
library(outliers)

grubbs.flag <- function(x) {
  outliers <- NULL
  test <- x
  grubbs.result <- grubbs.test(test)
  pv <- grubbs.result$p.value
  while(pv < 0.05) {
    outliers <- c(outliers,as.numeric(strsplit(grubbs.result$alternative," ")[[1]][3]))
    test <- x[!x %in% outliers]
    grubbs.result <- grubbs.test(test)
    pv <- grubbs.result$p.value
  }
  return(data.frame(X=x,Outlier=(x %in% outliers)))
}

colnames(wbcd)  # wbcd 의 컬럼명 확인 

a <- grubbs.flag(wbcd$radius_mean)  # wbcd 의 radius_mean 컬럼에 이상치가 있는지 확인 
#  하기위해 데이터 프레임을 생성 
a[a$Outlier==TRUE,     ]               # a 데이터 프레임의 Outlier 컬럼이 TRUE 인 데이터를 


for  ( i  in  3 : length(colnames(wbcd) ) ) {
  
  a <- grubbs.flag(wbcd[  ,  colnames(wbcd)[i] ]  )
  b <- a[a$Outlier==TRUE, "Outlier" ]
  print ( paste( colnames(wbcd)[i], ' --->', length(b) )  )
  
}

#3. 결측치를 확인합니다. 

colSums(is.na(wbcd))

#4. 히스토그램 그래프 + 정규분포 그래프를 통해서 데이터들이 정규성을  보이는지 확인합니다.

#install.packages("psych")
library(psych)
pairs.panels(wbcd[c("dimension_se", "symmetry_se","perimeter_se")] )

# ▩ 3단계 : 데이터로 모델 훈련 

# 1. 데이터의 구조를 확인하여 라벨(정답) 컬럼이 factor 인지 확인합니다. 

str(wbcd)  

wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

str(wbcd)

#2.  데이터를 양성과 악성이 잘 섞일 수 있도록 데이터를 섞어 줍니다. 

set.seed(1)

wbcd_shuffle <- wbcd[ sample(nrow(wbcd)) ,  ]

wbcd_shuffle

#3.   훈련 할때 필요한 컬럼만 선택합니다.   

wbcd_shuffle2 <- wbcd_shuffle[   , c(-1,-2) ] # 환자번호와 정답을 제외합니다. 

wbcd_shuffle2

#4. 컬럼들의 단위가 다 다르므로 데이터를 정규화 합니다. 

normalize <- function(x) {  return ( (x-min(x))  / ( max(x) - min(x) )  )   }

wbcd_n <- as.data.frame( lapply( wbcd_shuffle2, normalize )  ) 

summary(wbcd_n)

#5. 전체 569개의 행의 데이터를 훈련(공부) 데이터와 테스트(시험) 데이터로
#   나눠줘야합니다. ( 훈련 데이터 9, 테스트 데이터 1 로 나눈다)

nrow(wbcd_n) #569

n_90 <-  round(0.9*nrow(wbcd_n) )
n_90  # 512

wbcd_train <- wbcd_n[ 1:512,   ]            # 훈련 데이터 구성   
wbcd_test  <- wbcd_n[ 513 : 569,  ]        # 테스트 데이터 구성

nrow(wbcd_train)  # 512
nrow(wbcd_test)   # 57

# 6. 정답도 훈련과 테스트로 나눕니다. 

head(wbcd_shuffle[  ,  2])

wbcd_train_label <- wbcd_shuffle[ 1: 512 ,  2]
wbcd_test_label <- wbcd_shuffle[ 513 : 569 ,  2]

length(wbcd_train_label )  # 512
length(wbcd_test_label)    # 57

#7. 512개의 훈련 데이터와 훈련 데이터의 정답으로 거리계산한 데이터로
#   테스트 데이터 57개를 분류 합니다. 

#install.packages("class")
library(class)

result1 <- myknn(train=wbcd_train,  test=wbcd_test, cl=wbcd_train_label, k=1)
result1

#▩ 4단계. 모델 성능 평가

result1 == wbcd_test_label

sum(result1 == wbcd_test_label)

sum(result1 == wbcd_test_label)/57 *100

library(gmodels)

CrossTable( x=wbcd_test_label, y=result1, prop.chisq=FALSE)


문제337. (오늘의 마지막 문제)  

게시글 1460번. 오늘 배운 내용을 총정리해서 유방암 데이터에 적용하는 전체 코드

에서 myknn 함수의 알고리즘을 유클리드 거리에서 맨하튼 거리로 수학식을 변경하고 
유클리드 거리와의 정확도를 비교해보시오 !

유클리드 거리 ?



맨하튼 거리 ?  

▩ 파이썬으로 사이킷런의 knn 함수 만들기 

myknn <- function (train,test,cl,k) {
   result <- c()
   for (i in 1:nrow(test)) {
      temp <- (t( t(train) - c(t(test[i, ])) ))^2
      train_2 <- as.data.frame(train)
      train_2$dis <- sqrt(rowSums(temp))
      print(train_2$dis)
      train_2$rank <- rank(train_2$dis, ties.method='min')
      res <- cbind(cl,train_2)
      res2 <- res[ res$rank  <= k ,1]
      result <- append(result, names(which.max(table(res2))))
   }
   
   return (result)
}

게시글:  1491 myknn 함수의 원리 설명 그림 설명

# 1.  훈련 데이터와 테스트 데이터셋 만들기 

import pandas as pd

fruits = pd.DataFrame(
{'재료':['사과','베이컨','바나나','당근','샐러리','치즈'],
'단맛':[10,2,10,7,4,1],
'아삭한맛':[9,4,1,10,10,1],
'음식종류':['과일','단백질','과일','채소','채소','단백질']}
)

fruits


test_set = pd.DataFrame(
{'재료':['test1','test2','test3','test4'],
'단맛':[15,4,6,5],
'아삭한맛':[2,2,8,3]  } )


게시글 1483 ■ 판다스로 유방암 판정 knn 모델 생성하기

▩ 파이썬으로 myknn 함수를 만들기 위해서 알아야할 2가지 내용 학습 

 1.  numpy 의 broadcast  기능

 2.  판다스의 데이프레임 만드는 방법 

▩ 1.  numpy 의 broadcast  기능

 그림

 넘파이에서는 형상이 다른 배열끼리도 계산을 할 수 있습니다.
 위의 그림처럼 숫자 10이 2x2 행렬로 확대된 후 연산이 이루어지는것을 브로드 캐스트라고 합니다.

예제1.  위의 첫번째 그림을 numpy 로 구현하시오 !

import  numpy  as  np

a = np.array( [ [1,2], [3,4] ] )
a
b = a*10
b

예제2. 위의 두번째 그림을 numpy 로 구현하시오 !

import  numpy   as  np

a = np.array( [ [ 1, 2 ], [3, 4] ] )
b = np.array( [ [10, 20] ]  )
print (a*b)

※ 설명:  브로드 케스트가 된다면 어제 R 코드로 구현한 아래의 코드로 행렬 연산을 할 필요가 없습니다.

             temp <- t(t(train)-c(t(test[i,])))^2         

예제3.  아래의 행렬 2개를 a 와 b 로 만들고 빼기 연산을 하세요 !

        a                   -                   b                =     ?
   4     5                                 2     1
   7     9 
   2     3
   6     8


import  numpy  as   np

a = np.array([ [4,5], [7,9], [2,3], [6,8] ])
b = np.array( [2, 1] )
a-b

▩ 2.  판다스의 데이프레임 만드는 방법 

import pandas as pd

fruits = pd.DataFrame(
{'재료':['사과','베이컨','바나나','당근','샐러리','치즈'],
'단맛':[10,2,10,7,4,1],
'아삭한맛':[9,4,1,10,10,1],
'음식종류':['과일','단백질','과일','채소','채소','단백질']}
)

fruits

예제1. 위의 fruits 데이터프레임에서 단맛과 아삭한맛 컬럼 2개를 선택하고 numpy 로 변환해서
         x_train 변수에 담으시오 ~

x_train = fruits.loc[  :  , ['단맛', '아삭한맛'] ].to_numpy()

예제2. 위의 frutis 데이터프레임에서 음식종류를 numpy 로 변환해서  y_train 변수에 담으시오 ~

y_train = fruits['음식종류'].to_numpy()
y_train

예제3. 위의 x_train 과 y_train 을 가지고 a 라는 데이터프레임을 만드시오 !

x_train2_1 = pd.Series( list(x_train[:,0]) )
x_train2_2 = pd.Series( list(x_train[:,1]) )
y_train2  = pd.Series( list(y_train) )

a2 = pd.concat( [x_train2_1, x_train2_2, y_train2] , axis=1)
a2

위의 작업은 번거로우므로 다음과 같이 다시 작업합니다. 

x_train = fruits.loc[  :  , ['단맛', '아삭한맛'] ]
y_train = fruits['음식종류']

a = pd.concat( [x_train, y_train] , axis=1)
a

브로드 캐스트 기능을 쓰려면 numpy 여야하는데 아래와 같이 values 만 붙이면 
numpy형태를 볼 수 있습니다.

fruits.loc[  :  , ['단맛', '아삭한맛'] ].values  

fruits['음식종류'].values

▩ 파이썬으로 사이킷런의 myknn 함수 만들기 

예제1. 과일 데이터셋의 훈련 데이터와 테스트 데이터를 가져옵니다.

import pandas as pd

fruits = pd.DataFrame(
{'재료':['사과','베이컨','바나나','당근','샐러리','치즈'],
'단맛':[10,2,10,7,4,1],
'아삭한맛':[9,4,1,10,10,1],
'음식종류':['과일','단백질','과일','채소','채소','단백질']}
)

test_set = pd.DataFrame(
{'재료':['test1','test2','test3','test4'],
'단맛':[15,4,6,5],
'아삭한맛':[2,2,8,3]  } )

예제2.  훈련 데이터와 테스트 데이터와의 단맛과 
          아삭한 맛의 차이의 제곱값을 출력하는
          함수를 생성하시오 !

x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        return  temp

result = my_knn( x_train, x_test, y_train, 3)
result

예제3. 위의 코드 아래쪽에 훈련 데이터를 데이터 프레임으로 만들어서 train_2 라는 이름으로
         만드는 코드를 추가하시오 !

x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train)
        return  train_2

result = my_knn( x_train, x_test, y_train, 3)
result

예제4. 위의 코드 아래에 코드를 생성하는데 train_2  데이터 프레임의 dist 라른 파생변수를 추가하시오
          값은 temp 의 행의 값들을 더해서 루트를 쒸운값이 되게하시오 !

import  numpy  as np 
x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train)
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        return  train_2

result = my_knn( x_train, x_test, y_train, 3)
result

예제5. 위에서 만든 train_2 데이터프레임 오른쪽에 dist 에 대한 순위를 출력하는 파생변수를
          rnk 라는 이름으로 만들고 train_2 를 출력하겠금 my_knn 함수를 수정하시오 !

import  numpy  as np 
x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train)
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        train_2['rnk'] = list( map( int, train_2['dist'].rank() ) )
        return  train_2

result = my_knn( x_train, x_test, y_train, 3)
result

예제6. 위의 데이터프레임 train_2 에 kind 라는 파생변수가 생성되게 하는데 값은
         음식종류인 정답 라벨이 되게 코드를 수정하고 train_2 를 출력하겠금 my_knn 수정하세요

import  numpy  as np 
x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train)
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        train_2['rnk'] = list( map( int, train_2['dist'].rank() ) )
        train_2['kind'] = label
        return  train_2

result = my_knn( x_train, x_test, y_train, 3)
result

예제7.  k 값을 받아서 k개의 순위만큼 라벨들을 출력하겠금 my_knn 코드를 수정하시오 !

import  numpy  as np 
x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train)
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        train_2['rnk'] = list( map( int, train_2['dist'].rank() ) )
        train_2['kind'] = label
        top = train_2.loc[ train_2['rnk'] <= k  ,'kind' ]
        return  top

result = my_knn( x_train, x_test, y_train, 3)
result

예제8. 위의 결과의 최빈값을 출력하겠금 코드를 수정하시오 !

힌트 코드:   top.value_counts(dropna=True).idxmax()


import  numpy  as np 
x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train)
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        train_2['rnk'] = list( map( int, train_2['dist'].rank() ) )
        train_2['kind'] = label
        top = train_2.loc[ train_2['rnk'] <= k  ,'kind' ]
        pred = top.value_counts(dropna=True).idxmax()
        return  pred

result = my_knn( x_train, x_test, y_train, 3)
result

예제9.  이제 마지막으로 테스트 데이터에 대한 모든 라벨이 다 출력될 수 있도록 비어있는 리스트
           pred 에 최빈값들을 append 시키고 출력하시오 !

import  numpy  as np 
x_train = fruits.loc[ : , ['단맛','아삭한맛'] ]
y_train = fruits['음식종류']
x_test =  test_set.loc[ : , ['단맛', '아삭한맛'] ]

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train).copy()  # copy() 는 새로 복사해서 새로운 객체를 만드는 명령어
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        train_2['rnk'] = list( map( int, train_2['dist'].rank() ) )
        train_2['kind'] = label
        top = train_2.loc[ train_2['rnk'] <= k  ,'kind' ]
        pred.append( top.value_counts(dropna=True).idxmax() )
    return  np.array(pred)

result = my_knn( x_train, x_test, y_train, 3)
result

['과일', '단백질', '채소', '단백질']

※ copy() 를 사용하면 새로운 객체가 만들어지는것이고 copy() 를 사용하지 않으면 그냥 그 객체를
   가리키는 것입니다.

예제:   

a = [ 1, 2, 3] 
b = a
print(b)  # [1, 2, 3]
a[1] = 6
print(a) # [ 1, 6, 3 ]
print(b) # [1, 6, 3 ]   같은 메모리의 내용을 보고 있기 때문입니다. 

a = [ 1, 2, 3]
b = a.copy()  # copy 를 했기 때문에 b 는 별도의 객체 입니다.
print (b) # [1, 2, 3]
a[1] = 6
print(a)  # [ 1, 6, 3]
print(b)  # [ 1, 2, 3 ]                                     

예제10. 지금 완성한 my_knn 함수를 유방암 데이터에 적용하기 위해서 기존 사이킷런의 knn 
           함수를 이용한 전체코드를 먼저 실행하고 정확도를 확인하세요

■ 판다스로 유방암 판정 knn 모델  생성하기 

#1. 데이터를 로드합니다.
import  pandas  as  pd

wbcd = pd.read_csv("d:\\data\\wisc_bc_data.csv")
wbcd

#※ 판다스는 R 과는 다르게 strtigsAsFactors =True 를 지정하지 않아도 됩니다. 

#2. 데이터를 확인합니다. 
wbcd.info()  # 컬럼명과 데이터 타입을 확인할 수 있습니다. 
wbcd.shape # 몇행 몇열인지 확인할 수 있습니다. 
wbcd.describe() # R 에서의 summary() 함수와 같은 결과를 출력합니다.

#3. 결측치를 확인합니다.
wbcd.isnull().sum()

#4. 이상치를 확인합니다.

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 5)|(x[i]<Q1-IQR*5)].count())
        
outlier_value(wbcd)

           
#5. 명목형 데이터가 있는지 확인합니다.

wbcd.info()

#6. 데이터를 정규화 합니다.
from   sklearn.preprocessing  import  MinMaxScaler

wbcd2 = wbcd.iloc[ :  , 2: ]  # 환자번호와 diagnosis 제외합니다. 
wbcd2

scaler = MinMaxScaler()   

scaler.fit(wbcd2)   #  최대최소법으로 데이터를 계산합니다.

wbcd2_scaled = scaler.transform(wbcd2)  # 위에서 계산한 내용으로 데이터를 
wbcd2_scaled                                    # 변환해서 wbcd2_scaled 에 담습니다.

wbcd2_scaled.shape  # (569, 30)     numpy array 형태로 변경되었습니다. 

y = wbcd['diagnosis'].to_numpy()    # 정답 데이터를 numpy array 로 변경합니다.
print(y)

#7. 훈련데이터와 테스트데이터로 데이터를 분리합니다.(훈련90%, 테스트:10%)

from sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( wbcd2_scaled, y, test_size=0.1, random_state=1)

# x_train : 훈련 데이터 , x_test : 테스트 데이터,
# y_train: 훈련데이터의 정답, y_test: 테스트 데이터의 정답 

# random_state=1 은 어느 자리에서든 동일한 정확도를 보이는 모델을 만들기 위해서
# 설정한 것입니다. 

# test_size=0.1 로 했기 때문에 훈련 90%, 테스트 10% 로 나뉜것 입니다. 

print( x_train.shape)  # (512, 30)
print( x_test.shape)   # (57, 30)
print( y_train.shape)  # (512, )
print( y_test.shape)   # (57, )

#8. 모델을 설정합니다.
from  sklearn.neighbors   import  KNeighborsClassifier

model = KNeighborsClassifier( n_neighbors= 5 )  # knn 모델생성

#9. 모델을 훈련시킵니다.
model.fit( x_train, y_train )

#10. 훈련된 모델로 테스트 데이터를 예측합니다.
result = model.predict(x_test)
result

#11. 모델을 평가합니다.
y_test==result
sum(y_test==result)/57*100

#또는 

from  sklearn.metrics  import  accuracy_score

acurracy = accuracy_score( y_test, result)
acurracy

#12. 모델의 성능을 높입니다.

from  sklearn.metrics  import  confusion_matrix
a = confusion_matrix( y_test, result )
print(a)

0.9824561403508771
[[43  0]
 [ 1 13]]

예제11.  위의 코드에 우리가 완성한 my_knn 함수를 넣고 실행해보시오 ~

#■ 판다스로 유방암 판정 knn 모델  생성하기 

#1. 데이터를 로드합니다.
import  pandas  as  pd

wbcd = pd.read_csv("d:\\data\\wisc_bc_data.csv")
wbcd

#※ 판다스는 R 과는 다르게 strtigsAsFactors =True 를 지정하지 않아도 됩니다. 

#2. 데이터를 확인합니다. 
wbcd.info()  # 컬럼명과 데이터 타입을 확인할 수 있습니다. 
wbcd.shape # 몇행 몇열인지 확인할 수 있습니다. 
wbcd.describe() # R 에서의 summary() 함수와 같은 결과를 출력합니다.

#3. 결측치를 확인합니다.
wbcd.isnull().sum()

#4. 이상치를 확인합니다.

def outlier_value(x):
    for i in x.columns[x.dtypes=='float64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 5)|(x[i]<Q1-IQR*5)].count())
        
outlier_value(wbcd)

           
#5. 명목형 데이터가 있는지 확인합니다.

wbcd.info()

#6. 데이터를 정규화 합니다.
from   sklearn.preprocessing  import  MinMaxScaler

wbcd2 = wbcd.iloc[ :  , 2: ]  # 환자번호와 diagnosis 제외합니다. 
wbcd2

scaler = MinMaxScaler()   

scaler.fit(wbcd2)   #  최대최소법으로 데이터를 계산합니다.

wbcd2_scaled = scaler.transform(wbcd2)  # 위에서 계산한 내용으로 데이터를 
wbcd2_scaled                                    # 변환해서 wbcd2_scaled 에 담습니다.

wbcd2_scaled.shape  # (569, 30)     numpy array 형태로 변경되었습니다. 

y = wbcd['diagnosis'].to_numpy()    # 정답 데이터를 numpy array 로 변경합니다.
print(y)

#7. 훈련데이터와 테스트데이터로 데이터를 분리합니다.(훈련90%, 테스트:10%)

from sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( wbcd2_scaled, y, test_size=0.1, random_state=1)

# x_train : 훈련 데이터 , x_test : 테스트 데이터,
# y_train: 훈련데이터의 정답, y_test: 테스트 데이터의 정답 

# random_state=1 은 어느 자리에서든 동일한 정확도를 보이는 모델을 만들기 위해서
# 설정한 것입니다. 

# test_size=0.1 로 했기 때문에 훈련 90%, 테스트 10% 로 나뉜것 입니다. 

print( x_train.shape)  # (512, 30)
print( x_test.shape)   # (57, 30)
print( y_train.shape)  # (512, )
print( y_test.shape)   # (57, )

#8. 모델을 설정합니다.
#from  sklearn.neighbors   import  KNeighborsClassifier

#model = KNeighborsClassifier( n_neighbors= 5 )  # knn 모델생성

#9. 모델을 훈련시킵니다.
#model.fit( x_train, y_train )

#10. 훈련된 모델로 테스트 데이터를 예측합니다.
#result = model.predict(x_test)

x_train=pd.DataFrame(x_train)
x_test = pd.DataFrame(x_test)

import numpy as np

def  my_knn( train, test, label,  k):
    pred = [ ]
    for   i   in  range( 0, len(test) ):
        temp = ( train.values - test.values[ i ] ) **2
        train_2 = pd.DataFrame(train).copy()  # copy() 는 새로 복사해서 새로운 객체를 만드는 명령어
        train_2['dist'] = np.sqrt( np.sum(temp, axis=1) )
        train_2['rnk'] = list( map( int, train_2['dist'].rank() ) )
        train_2['kind'] = label
        top = train_2.loc[ train_2['rnk'] <= k  ,'kind' ]
        pred.append( top.value_counts(dropna=True).idxmax() )
    return  np.array(pred)


result = my_knn(x_train,x_test, y_train, 21)
result

#11. 모델을 평가합니다.
y_test==result
sum(y_test==result)/57*100

문제338.  위의 my_knn 함수를 유클리드 거리에서 맨하턴 거리로 수학식을 변경하고
             다시 유방암 모델의 정확도를 확인하시오 ~~  

  이론 --------------------------> 코드로 구현 

                ↓

  논문 ------------------------> 코드로 구현 

knn 과 k-means 의 공통점과 차이점을 설명하면서 knn 을 R과 파이썬으로 구현

▩  k-means 의 책의 코드를 구현

데이터셋: 데이터 게시판.  소셜미디어 10대들에 대한 데이터  --> snsdata.csv

#1. 데이터를 로드합니다. 

teens <- read.csv("snsdata.csv")
teens

※ 데이터 설명:  책 416 페이지에 2006년도에 유명 sns 에 프로필이 있었던 미국 고등학생
3만명의 무작위 샘플을 나타내는 데이터셋이고 텍스트 마이닝 툴을 이용해서 상위 500개
의 단어중 36개의 단어를 선별하여 단어의 빈도를 표현한 데이터

nrow(teens) # 30000
ncol(teens)  # 40

# 결측치를 확인합니다.

colSums(is.na(teens))

#설명:  나이의 결측치는 졸업년도로 나이를 추정해서 채워넣을것입니다. 
          성별은 남자와 여자로 컬럼을 만들고 성별이 있는 사람과 없는 사람으로 
           컬럼을 만들것입니다. 

#2. 데이터를 전처리 합니다.

# 고등학생 데이터라는 정확한 데이터 분석을 위해서 나이가 13세 ~ 20세가가 아니면
# 전부 다 NA 처리합니다. 

teens$age <- ifelse( teens$age >= 13 & teens$age < 20, teens$age, NA)

# 성별을 여자이면 1이고 아니면 0으로 하는 파생변수를 추가 

teens$female <- ifelse( teens$gender=='F' & !is.na(teens$gender),  1, 0 )

# 성별이 결측치이면 1이고 결측치가 아니면 0 이 되는 파생변수를 추가

teens$no_gender <- ifelse( is.na(teens$gender) , 1, 0 ) 

# 추가한 파생변수의 분포를 확인합니다.

table( teens$gender, useNA= 'ifany')
table( teens$female,  useNA= 'ifany')
table( teens$no_gender,  useNA= 'ifany')

#설명:  성별에 대한 결측치는 성별이 없는 사람과 있는 사람에 대한 파생변수를 추가하면서
#     따로 결측치 행들을 제거 하지 않았습니다. 

# 졸업년도로 나이를 추정해서 결측치를 채워넣습니다. 

ave_age <- ave(teens$age, teens$gradyear,  FUN=function(x) mean(x, na.rm=TRUE) )
ave_age

# 설명: 졸업년도에 결측치를 제외하고 졸업년도별 평균 나이를 ave_age 변수에 담는다

teens$age <- ifelse( is.na(teens$age), ave_age, teens$age)
colSums(is.na(teens))

#3. 표준화를 수행합니다.
# sns 글에서 나타났던 관심사 횟수를 표현하는 36개의 수치형 데이터 컬럼을 표준화 시킵니다.

interests <-  teens[  ,  5:40]
interests

interests_z <- as.data.frame( lapply( interests, scale) )
summary(interests_z)

#4. kmeans 함수로 5개의 클래스로 분류합니다.

set.seed(2345)
teen_clusters <-  kmeans( interests_z , 5)
teen_clusters

#5. 세그멘테이션한 클래스별 건수를 확인하고 레포팅합니다.

teen_clusters$cluster

문제340.  지금 분류된 숫자 1~5에 대한 건수를 각각 출력하시오 ~

table(teen_clusters$cluster)


▣ ave 함수 이해하기 

예제1.  emp3.csv 를  emp 데이터 프레임으로 만들고 자기가 속한 부서번호의
          평균월급을 출력하시오 !

emp <- read.csv("emp3.csv")
ave( emp$sal, emp$deptno)
emp$deptno_avg <- ave( emp$sal, emp$deptno)
emp

예제2. 이름, 직업, 월급, 자기가 속한 직업의 평균월급을 출력하시오 !

emp$job_avg <- ave( emp$sal, emp$job)
emp[   ,  c('ename', 'job', 'sal', 'job_avg') ]

예제3. 사원 테이블의 커미션의 결측치를 자기가 속한 부서번호의 평균월급으로
          치환하시오 !

emp$comm[ is.na(emp$comm) ] <-  ave( emp$sal, emp$deptno)[is.na(emp$comm)]

emp

문제339. (오늘의 마지막 문제)  오늘 만든 my_knn 함수를 iris 데이터에 적용해서 
             knn 머신러닝 모델을 생성하시오 !

▩ k-means 함수를 R 로 직접 만들기 

k-means 함수의 원리 


목표:  2개의 분류로 군집화를 하고 싶다
        기계가 알아서 군집화를 하게하고 싶다

        "코딩은 글쓰기 입니다."

1. 데이터중 2개를 랜덤으로 선택합니다.                             46분까지 쉬세요 ~~
2. 거리와 종류(정답) 을 저장할 테이블 a 를 생성합니다. 
3. 선택한 데이터 2개와 거리를 각각 구합니다. 
4. 3번에서 구한 거리와 기존 거리와 비교해서 구한 거리가 기존 거리보다 작으면
   테이블 a 의 거리를 갱신합니다. 
5. 2개로 클러스터링된 컬럼들의 평균값을 각각 구해서 평균점(중심점)을 찾습니다.
6. 3번~5번을 무한반복하는데 중심점의 변화가 없을때 중단합니다.


▩ R 에 내장되어있는 k-means 함수를 사용하여 아래의 데이터를 분류해보기

# 1. 데이터를 로드합니다.

fruits <- data.frame( '재료'=c('사과','베이컨','배','당근','참외','치즈','햄','계란'),
                      '단맛'  = c(10,1,10,7,8,1,2,3),
                      '아삭한맛'=c(9,3,8,10,7,1,1,2),
                      '음식종류'=c('과일','단백질','과일','과일','과일','단백질','단백질','단백질')
)

fruits

# 2. 군집에 필요한 컬럼들만 선택합니다.

data = fruits[     , c(2,3) ]
data

#3. data 를 시각화 합니다. 

plot(data)

#4. 내장함수인 kmeans 로 2개로 data 를 분류합니다. 

set.seed(7)
km <- kmeans( data, 2) 
km$cluster

cbind(fruits, km$cluster)

# 설명:  분류가 잘된것을 확인할 수 있습니다. 

#5. factoextra  패키지를 이용해서 시각화 하시오 !
library(factoextra)

fviz_cluster( km, data=data, stand=F)

▩ my_kmeans 함수 코딩(글쓰기)을 위해 미리 알아야할 함수(어휘)들

1.  랜덤으로 데이터 선택하는 함수

* 위의 data 데이터프레임의 갯수를 확인하기 

data
nrow(data) # 8
sample(8)  # 1번 부터 8번까지의 숫자가 랜덤순서로 출력되고 있음 
sample(nrow(data)) 

[1] 7 2 6 8 4 3 5 1

* 위의 8개의 숫자중에 2개를 랜덤으로 추출하기 

sample(nrow(data), 2 )

* data 프레임에서 2개의 행을 랜덤으로 추출하기 

data[ 행, 열 ] 

data[ sample(nrow(data), 2 ) ,  ] 

2.  컬럼들의 데이터를 평균내는 함수 

colMeans(data)

▩ k-means 글쓰기 

기존함수: km <- kmeans( data, 2) 

#1. 데이터중 2개를 랜덤으로 선택합니다.                          
my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  
        print(cluster)
                                          }

km <- my_kmeans(data,2)
km

#2. 위의 선택된 2개의 군집 중앙점의 라벨을 1과 2로 각각 지정하기 위해
#   cluster  파생변수를 kind 라는 이름하고 값을 1 또는 2가 되게하시오

my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  
        cluster$kind <- sample(nrow(cluster)) # cluster 건수가 2개라서 숫자 1과 2가 출력
        print (cluster)
                                          }

km <- my_kmeans(data,2)
km

# 3. 거리와 종류(정답) 을 저장할 테이블 a 를 생성합니다.
 
my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  
        cluster$kind <- sample(nrow(cluster)) # cluster 건수가 2개라서 숫자 1과 2가 출력
        a <- data.frame( test, kind=0, d=999999) #kind 아직 뭔지 모르니까 0으로 하고
       print(a)                                              # 거리 d 는 아주 먼 숫자를 입력합니다.
                                          }

km <- my_kmeans(data,2)
km

# 4. 선택한 데이터(중심점) 2개와 데이터간의 유클리드 거리를 각각 구합니다. 

my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  
        cluster$kind <- sample(nrow(cluster)) # cluster 건수가 2개라서 숫자 1과 2가 출력
        a <- data.frame( test, kind=0, d=999999) #kind 아직 뭔지 모르니까 0으로 하고
                                                              # 거리 d 는 아주 먼 숫자를 입력합니다.
        for  ( i  in  1:nrow(cluster) ) {  # 클러스터 1번과 2번에 대한 거리를 각각 계산합니다. 

            temp <-  t( t(test) - c(t(cluster[  i,  -ncol(cluster) ] )) )^2  
            temp <-  sqrt(rowSums(temp) )
            print(temp)
                                              }      

                             
                         }

km <- my_kmeans(data,2)
km

#  5. 방금 위의 4번에서 구한 거리와 기존 거리와 비교해서 구한 거리가 기존 거리보다 작으면
#     테이블 a 의 거리를 갱신합니다. 

my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  # 기존 데이터에서 2개의 데이터를 선택합니다. 
        cluster$kind <- sample(nrow(cluster)) # cluster 건수가 2개라서 숫자 1과 2가 출력
        a <- data.frame( test, kind=0, d=999999) #kind 아직 뭔지 모르니까 0으로 하고
                                                              # 거리 d 는 아주 먼 숫자를 입력합니다.
        for  ( i  in  1:nrow(cluster) ) {  # 클러스터 1번과 2번에 대한 거리를 각각 계산합니다. 

            temp <-  t( t(test) - c(t(cluster[  i,  -ncol(cluster) ] )) )^2  
            temp <-  sqrt(rowSums(temp) )      # 데이터와 중심점과 거리를 temp 에 담은 후에 
            a$kind <- ifelse( a$d > temp, cluster$kind[i], a$kind )  # 기존 거리보다 지금 거리가
                                                                     #작으면 해당하는 군집으로 라벨링합니다.
            a$d <-  ifelse( a$d > temp, temp,  a$d )  # 기존거리보다 지금 거리가 작다면 
                                                                   # 지금 거리를 a 테이블에 d 에 갱신해라 
            print(a)        
                 }

 }                                                        즐거운 점심시간 되세요 ~~

km <- my_kmeans(data,2)

k-means 함수의 원리를  다시 그림으로 설명 
                                                                  단맛   아삭한맛     kind       d 
                                                                     1        3             1        3
                                                                     10      9             2        1
                                                                    10       8             1        6
                                                                     7       10            1        8
                                                                     8       7              1        5
                                                                   





# 6.  2개로 클러스터링된 컬럼들의 평균값을 각각 구해서 평균점(중심점)을 찾습니다.

fruits <- data.frame( '재료'=c('사과','베이컨','배','당근','참외','치즈','햄','계란'),
                      '단맛'  = c(10,1,10,7,8,1,2,3),
                      '아삭한맛'=c(9,3,8,10,7,1,1,2),
                      '음식종류'=c('과일','단백질','과일','과일','과일','단백질','단백질','단백질')
)

fruits

# 군집에 필요한 컬럼들만 선택합니다.

data = fruits[     , c(2,3) ]
data

my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  # 기존 데이터에서 2개의 데이터를 선택합니다. 
        cluster$kind <- sample(nrow(cluster)) # cluster 건수가 2개라서 숫자 1과 2가 출력
        a <- data.frame( test, kind=0, d=999999) #kind 아직 뭔지 모르니까 0으로 하고
                                                              # 거리 d 는 아주 먼 숫자를 입력합니다.
        for  ( i  in  1:nrow(cluster) ) {  # 클러스터 1번과 2번에 대한 거리를 각각 계산합니다. 

            temp <-  t( t(test) - c(t(cluster[  i,  -ncol(cluster) ] )) )^2  
            temp <-  sqrt(rowSums(temp) )      # 데이터와 중심점과 거리를 temp 에 담은 후에 
            a$kind <- ifelse( a$d > temp, cluster$kind[i], a$kind )  # 기존 거리보다 지금 거리가
                                                                     #작으면 해당하는 군집으로 라벨링합니다.
            a$d <-  ifelse( a$d > temp, temp,  a$d )  # 기존거리보다 지금 거리가 작다면 
                                                                   # 지금 거리를 a 테이블에 d 에 갱신해라       
                 }
        for   (  i  in   1:nrow(cluster) ) { 
                 cluster[ cluster$kind==i,  -ncol(cluster) ] <-  colMeans( a[ a$kind==i,  1:(ncol(a)-2) ] ) 
                                                } 
              print(cluster)
 }                                                 

km <- my_kmeans(data,2)

#  7. 중심점을 이동하고 다시 새로운 중심과 데이터와의 거리를 계산하고 계산된 거리와 기존거리를 비교해서
       라벨을 변경하고 거리를 변경하는 이 작업을 무한반복하는데 중심점의 변화가 없을때 중단합니다.


fruits <- data.frame( '재료'=c('사과','베이컨','배','당근','참외','치즈','햄','계란'),
                      '단맛'  = c(10,1,10,7,8,1,2,3),
                      '아삭한맛'=c(9,3,8,10,7,1,1,2),
                      '음식종류'=c('과일','단백질','과일','과일','과일','단백질','단백질','단백질')
)

fruits

# 군집에 필요한 컬럼들만 선택합니다.

data = fruits[     , c(2,3) ]
data

my_kmeans <- function(test, k) { 
        cluster <-  test[ sample(nrow(test), k),  ]  # 기존 데이터에서 2개의 데이터를 선택합니다. 
        cluster$kind <- sample(nrow(cluster)) # cluster 건수가 2개라서 숫자 1과 2가 출력
        a <- data.frame( test, kind=0, d=999999) #kind 아직 뭔지 모르니까 0으로 하고
                                                              # 거리 d 는 아주 먼 숫자를 입력합니다.
 while(1)  {

        t_cluster  <- cluster  

        for  ( i  in  1:nrow(cluster) ) {  # 클러스터 1번과 2번에 대한 거리를 각각 계산합니다. 

            temp <-  t( t(test) - c(t(cluster[  i,  -ncol(cluster) ] )) )^2  
            temp <-  sqrt(rowSums(temp) )      # 데이터와 중심점과 거리를 temp 에 담은 후에 
            a$kind <- ifelse( a$d > temp, cluster$kind[i], a$kind )  # 기존 거리보다 지금 거리가
                                                                     #작으면 해당하는 군집으로 라벨링합니다.
            a$d <-  ifelse( a$d > temp, temp,  a$d )  # 기존거리보다 지금 거리가 작다면 
                                                                   # 지금 거리를 a 테이블에 d 에 갱신해라       
                 }
        for   (  i  in   1:nrow(cluster) ) { 
                 cluster[ cluster$kind==i,  -ncol(cluster) ] <-  colMeans( a[ a$kind==i,  1:(ncol(a)-2) ] ) 
                                                } 
     
     if (  !sum(t_cluster-cluster) ) {

              return ( a$kind)            
                                            }

               }  # while 문 닫는 중괄호                 
 }                                                 

set.seed(1)
km <- my_kmeans(data,2)


문제340. 지금 만든 my_kmeans 함수가 과일 데이터의 과일과 단백질을 잘 분류했는지 확인해보시오 !

set.seed(7)
km <- my_kmeans(data,2)
km

cbind(fruits,km)

문제341. 지금 만든 my_kmeans 함수를 유방암 데이터의 분류에 적용하기 위해 지난번에 수행했던
             k-means 유방암 데이터 분류 코드를 실행하세요 ~

# 1. 데이터를 로드합니다.
wisc <- read.csv("wisc_bc_data.csv", header=T)
head(wisc)
ncol(wisc)

#2. 필요한 컬럼만 선택합니다.
wisc2 <- wisc[   , 3:32 ]   # 환자id 와 정답을 제외한 나머지 컬럼들
head(wisc2)

# 3. 정규화를 진행합니다.
normalize <- function(x) {
  return   ((x-min(x)) /  ( max(x) - min(x) ) ) 
}

wisc_n <-  as.data.frame( lapply( wisc2, normalize )   )
# ncol(wisc_n)
# 4. 모델을 생성합니다.

my_kmeans <- function(test, k){
  set.seed(100)
  cluster <- test[sample(nrow(test), k), ]
  cluster$kind <- sample(nrow(cluster))
  # print(cluster$kind)
  table <- data.frame(test, kind = 0, d = 9999999)
  # print(table)
  while(1){
    t_cluster <- cluster
    
    for (i in 1:nrow(cluster)){
      temp <- t(t(test) - c(t(cluster[i,- ncol(cluster)])))^2
      temp <- sqrt(rowSums(temp))
      table$kind <- ifelse(table$d > temp, cluster$kind[i], table$kind)
      table$d <- ifelse(table$d > temp, temp, table$d)
      # print(table$d)
    }
    
    for (i in 1:nrow(cluster)){
      cluster[cluster$kind == i, -31] <- colMeans(table[table$kind == i, 1:(ncol(table) - 2)])
    }
    # print(cluster)
    # print(ncol(cluster))
    # print(ncol())
    if (!sum(t_cluster - cluster)){
      
      return(table$kind)
      
    }
  }
}

set.seed(7)
km <- kmeans( wisc_n, 2)


# 6. 정확도를 확인합니다. 
install.packages("gmodels")
library(gmodels)
CrossTable( wisc$diagnosis,  km$cluster)

print(528/569) # 0.927

기존 k-means 의 정확도는 0.927 정확도가 출력되고 있습니다. 

문제342.  위의 kmeans 함수를 my_kmeans 로 바꿔서 돌리고 수행해보시오 ~

set.seed(7)
km <- my_kmeans( wisc_n, 2)
km

# 6. 정확도를 확인합니다. 
#install.packages("gmodels")
library(gmodels)
CrossTable( wisc$diagnosis,  km)

print(525/569) # 0.922

[1] 0.9226714

우리가 오늘 만든 my_kmeans 함수도 0.92 에 해당하는 정확도가 출력이 되었습니다. 

■ 사이킷런의 k-means 함수를 이용해서 유방암 데이터 분류하기 

# 1. 데이터를 로드합니다. 
import  pandas  as  pd
df = pd.read_csv("d:\\data\\wisc_bc_data.csv")
df

# 2. 결측치를 확인합니다.
df.isnull().sum()

# 3. 필요한 컬럼만 선택합니다.
df.info()
data = df.iloc[ : , 2: ] 
data

# 4. 정규화를 진행합니다. 
from  sklearn.preprocessing  import   MinMaxScaler()

scaler = MinMaxScaler()
scaler.fit(data)
data_scaled = scaler.transform(data)
data_scaled

# 5. k-means 모델을 생성합니다.
from  sklearn.cluster  import  KMeans 

model = KMeans( n_clusters=2, random_state=7 )

# 6. k값을 2로 주고 악성과 양성을 분류합니다.

model.fit(data_scaled)

km = model.labels_
km

# 7. 정확도를 확인해 봅니다. 

y_train = df.diagnosis.apply(lambda  x:x=='M').astype(int)

sum(y_train ==km)/len(y_train) 

0.9279437609841827

문제343.  아래의 과일 데이터를 2개로 사이킷런의 k-means 를 이용해서 분류하시오 !

import pandas as pd

fruits = pd.DataFrame(
{'재료': ['사과','베이컨','배','당근','참외','치즈','햄','계란'],
'단맛':[10,1,10,7,8,1,2,3],
'아삭한맛':[9,3,8,10,7,1,1,2],
'음식종류':['과일','단백질','과일','과일','과일','단백질','단백질','단백질']}
)

fruits 

▩ 파이썬으로 직접 사이킷런의 kmeans 를 개발하기 

" 코딩은 글쓰기 입니다 "  

 1. 글쓰기는 기승전결과 같은 구조가 있어야한다  
 2. 글쓰기는 어휘력이 풍부하면 더 글이 좋아집니다.

* R 로 k-means 를 구현했을때의 함수 가져옵니다.

my_kmeans <- function(test, k){
  cluster <- test[sample(nrow(test), k), ]
  cluster$kind <- sample(nrow(cluster))
  table <- data.frame(test, kind = 0, d = 9999999)

  while(1){
    t_cluster <- cluster
    
    for (i in 1:nrow(cluster)){
      temp <- t(t(test) - c(t(cluster[i,- ncol(cluster)])))^2
      temp <- sqrt(rowSums(temp))
      table$kind <- ifelse(table$d > temp, cluster$kind[i], table$kind)
      table$d <- ifelse(table$d > temp, temp, table$d)
    }
    
    for (i in 1:nrow(cluster)){
      cluster[cluster$kind == i, -31] <- colMeans(table[table$kind == i, 1:(ncol(table) - 2)])
    }
    if (!sum(t_cluster - cluster)){    
      return(table$kind)
      
    }
  }
}

* 오전에 만들었던 R kmeans 를 파이썬으로 구현하기 위해 알아야할 3가지 함수 

1. np.random.choice

2. numpy broadcast

3. np.sum

* np.random.choice 예제

import  numpy  as  np 

a = [ 1, 2, 4, 3, 5, 0 ]
np.random.choice(a)

# a 리스트에서 2개의 요소를 랜덤으로 추출하려면 ?

np.random.choice(a, 2)

# 아래의 fruits 에서 선별한 2개의 컬럼으로 만든 data 라는 데이터프레임에서 랜덤으로
# 2개의 행을 추출하시오 !

import pandas as pd

fruits = pd.DataFrame(
{'재료': ['사과','베이컨','배','당근','참외','치즈','햄','계란'],
'단맛':[10,1,10,7,8,1,2,3],
'아삭한맛':[9,3,8,10,7,1,1,2],
'음식종류':['과일','단백질','과일','과일','과일','단백질','단백질','단백질']}
)

data = fruits.loc[ : , ['단맛', '아삭한맛'] ]
data 

#답:

data.iloc[ np.random.choice(len(data) , 2, replace=False),  : ]

#설명: 똑같은게 두개가 나오면 안되므로 replace=False 를 사용해서 비복원추출되게 해야합니다. 

* numpy 의 mean 함수 사용법 (R 에서의 colMeans 와 동일함)

data 

np.mean(data, axis=0)  # 열의 평균값을 출력

np.mean(data, axis=1)  # 행의 평균값을 출력 

예제1.  아래의 R 코드를 파이썬으로 구현하시오 

my_kmeans <- function(test, k){
  cluster <- test[sample(nrow(test), k), ]
  cluster$kind <- sample(nrow(cluster))
  table <- data.frame(test, kind = 0, d = 9999999)

답:
def  my_kmeans( test, k ):
    cluster = test.iloc[ np.random.choice(len(test), k) , : ]
    cluster['kind'] = np.random.choice(np.arange(k),k, replace=False)
    print(cluster)

my_kmeans(data, 2) 

문제344. (오늘의 마지막 문제)  R 코드로 작성했던 kmeans 함수를 파이썬 코드로 완성하고
            과일 데이터를 분류해서 잘분류되는지 확인하시오 ~~~~


 5시 신호 보냈습니다. ~  마지막 문제 올리시고 자유롭게 자습 또는 스터디 하세요 ~~

my_kmeans <- function(test, k){             
  cluster <- test[sample(nrow(test), k), ]
  cluster$kind <- sample(nrow(cluster))
  table <- data.frame(test, kind = 0, d = 9999999)

  while(1){
    t_cluster <- cluster
    
    for (i in 1:nrow(cluster)){
      temp <- t(t(test) - c(t(cluster[i,- ncol(cluster)])))^2
      temp <- sqrt(rowSums(temp))
      table$kind <- ifelse(table$d > temp, cluster$kind[i], table$kind)
      table$d <- ifelse(table$d > temp, temp, table$d)
    }
    
    for (i in 1:nrow(cluster)){
      cluster[cluster$kind == i, -31] <- colMeans(table[table$kind == i, 1:(ncol(table) - 2)])
    }
    if (!sum(t_cluster - cluster)){    
      return(table$kind)
      
    }
  }
}

▩ 파이썬으로 k-means 함수 만들기 without  사이킷런 

1.지도학습 :    knn 함수, 로직스틱 회귀(서포트벡터 머신) 

2. 비지도학습 :  k-means
 
* k-means 함수를 파이썬으로 구현하기 위한 6가지 단계

1. 데이터중 2개를 랜덤으로 선택합니다.
2. 거리와 종류(정답)을 저장할 a 테이블을 생성합니다.
3. 선택한 데이터 2개와 거리를 각각 구합니다.
4. 구한 거리와 기존거리와 비교해서 구한 거리가 기존거리보다 작으면 
    테이블 a 의 거리를 갱신하고 정답도 갱신합니다.
5. 2개로 클러스터링된 중심점을 갱신합니다.
6. 위의 3번~5번을 무한반복하는데 중심점의 변화가 없을때 중단합니다.

예제1. 사이킷런의 kmeans 함수로 과일 데이터를 분류하고 시각화하기 (문제343번 코드를 가져오세요)

import pandas as pd

# 1. 데이터를 로드합니다.
fruits = pd.DataFrame(
{'재료': ['사과','베이컨','배','당근','참외','치즈','햄','계란'],
 '단맛':[10,1,10,7,8,1,2,3],
 '아삭한맛':[9,3,8,10,7,1,1,2],
 '음식종류':['과일','단백질','과일','과일','과일','단백질','단백질','단백질']}
)
fruits

# 2. 필요한 컬럼만 선택합니다.
#fruits.info()
data = fruits.iloc[ :, 1:3]
data

# 3. 정규화를 진행합니다.
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(data)
data_scaled = scaler.transform(data)
data_scaled

# 4. k-means 모델을 생성합니다.
from sklearn.cluster import KMeans

model = KMeans( n_clusters=2, random_state=7)

# 5. k값을 2로 주고 과일과 단백질을 분류합니다.
model.fit(data_scaled)  # 모델 훈련

km = model.labels_
km  # [1, 0, 1, 1, 1, 0, 0, 0]

# 6. 정확도를 확인해봅니다. (정답 라벨과 비교)
y_train = fruits.음식종류.apply(lambda x:x=='과일').astype(int)

sum(y_train == km) / len(y_train)  # 1.0

#7. 분류된 결과를 시각화 합니다.
import matplotlib.pyplot as plt
import seaborn as sns      # 간단하고 강력한 시각화 모듈

# 그래프에서 한글깨짐 방지코드
from matplotlib import font_manager, rc
font_path = "c:\\Windows\\Fonts\\malgun.ttf" #폰트파일의 위치 실습파일안에있음
font_name = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_name)

plt.figure(figsize = (5,5))     # 그래프의 가로, 세로 크기 

ax = sns.swarmplot(data = fruits, x ='단맛', y = '아삭한맛', size = 10, hue = km)

#sns.scatterplot('단맛', '아삭한맛', data=fruits, hue=km)

설명: 시각화된 결과로 보면 너무나도 신기하게 잘 분류하고 있습니다. 

예제2. 아래처럼 사이킷런의 KMeans 함수를 만들기 위해  fruits 데이터에서 
         랜덤으로 2개의 데이터를 선택하게 하는 코드를 작성하시오 !

기존함수:
from sklearn.cluster import KMeans
model = KMeans( n_clusters=2, random_state=7)

답:
import numpy as np 

def  my_kmeans( test, k):
    cluster = test.iloc[ np.random.choice( len(test), k, replace=False), :  ]
    return  cluster

my_kmeans( fruits, 2 )             

예제3. 숫자 1과 2과 순서를 랜덤으로 생성해서 생성되게 하시오 !

np.random.choice( [1,2], 2, replace=False )  # 비복원 추출로 숫자 1과 2를 출력

예제4. 위의 추출된 데이터 1과 2를 파생변수로 cluster 테이블에 추가되게해서
         아래의 결과가 출력되겠금 my_kmeans 함수를 수정하시오 

def my_kmeans(test, k):
    cluster = test.iloc[np.random.choice(len(test), k, replace = False), :]
    cluster['kind'] = np.random.choice([1,2], 2, replace = False)
    return cluster

my_kmeans(fruits, 2)

예제5.  정답(kind) 과 거리(d) 를 담기 위한 데이터 프레임 a 를 생성하는데
          정답(kind) 데이터는 모두 a 가 되게하고 거리(d) 데이터는 모두 999999가 
          되게하는 a 데이터 프레임을 아래와 같이 만드시오 !

a = pd.DataFrame( {'kind' : [0]*8, 'd' : [999999]*8 } )
a

def my_kmeans(test, k):
    cluster = test.iloc[np.random.choice(len(test), k, replace = False), :]
    cluster['kind'] = np.random.choice([1,2], 2, replace = False)
    a = pd.DataFrame( {'kind' : [0]*len(test), 'd' : [999999]*len(test) } )
    return a

my_kmeans(fruits, 2)


예제6. 아래의 a2 의 단맛과 아삭한 맛과 b2 변수에 담긴 단맛과 아삭한 맛과의
          유클리드 거리를 구하시오 !  (답글로 올려주세요 ~)

a2 = pd.DataFrame( {'단맛' : [7], '아삭한맛' : [9] } )
b2 =pd.DataFrame( { '단맛' : [10], '아삭한맛' : [2] } )

답:
distance = np.sqrt(np.sum((a2-b2)**2,axis=1))
distance

예제7. 아래의 fruits 의  단맛과 아삭한맛과 b2 와의 유클리드 거리가 출력되게하시오 !


fruits = pd.DataFrame(
{'재료': ['사과','베이컨','배','당근','참외','치즈','햄','계란'],
'단맛':[10,1,10,7,8,1,2,3],
'아삭한맛':[9,3,8,10,7,1,1,2],
'음식종류':['과일','단백질','과일','과일','과일','단백질','단백질','단백질']}
)

np.sqrt(np.sum((fruits.iloc[:,[1,2]].values - b2.values)**2, axis = 1))

예제8. 이번에는 중심점이 2개이므로 b2 를 아래와 같이 만들고 fruits 의 단맛과 아삭한맛
         과의 거리를 각각 구하시오 ~  (답글로 달아주세요 ~)

fruits = pd.DataFrame(
{'재료': ['사과','베이컨','배','당근','참외','치즈','햄','계란'],
'단맛':[10,1,10,7,8,1,2,3],
'아삭한맛':[9,3,8,10,7,1,1,2],
'음식종류':['과일','단백질','과일','과일','과일','단백질','단백질','단백질']}
)

b2 =pd.DataFrame( { '단맛' : [10, 5], '아삭한맛' : [2, 7] } )  # 중심점을 2개로 만듬

예제9. 아래의 my_kmeans 함수안에 a 데이터프레임에 거리(d) 컬럼에 
         중심점 cluster  와  fruits 의 단맛과 아삭한 맛과의 거리가 갱신되게 하는데
         갱신될때 기존 거리보다 더 가까운 거리로 갱신되게 하시오 !
         ( 제일 처음에 기존거리는 다 999999 였습니다.)

 "누구에게 의지하지 않고 직접 내가 코드 구현하기 ! "

def my_kmeans(test, k):
    cluster = test.iloc[np.random.choice(len(test), k, replace = False), :]
    cluster['kind'] = np.random.choice([1,2], 2, replace = False)
    a = pd.DataFrame( {'kind' : [0]*len(test), 'd' : [999999]*len(test) } )
    
    for  i  in  range(len(cluster)):
        temp = np.sqrt( np.sum( (test.values - cluster.iloc[ : ,  :-1].values[i]) **2, axis=1))
        print(temp) # 각각의 중심점에 대해서 거리 8개가 리스트에 담김
        for  j  in  range( len(temp)):
            a['d'][j] = temp[j] if  a.d[j] > temp[j] else a.d[j]   # a 데이터 프레임에 거리(d) 의
    return  a                                                           # j 번째 행의 데이터를 갱신하는데
                                                                          # 기존거리와 비교해서 작으면 갱신 

my_kmeans(fruits.iloc[:,[1,2]], 2)

예제10. (점심시간 문제) 아래와 같이 a 데이터프레임에 kind 컬럼에 지금 갱신된 거리의 
           정답(kind) 로 갱신되게 하시오 ! (아래의 kind 에 1 또는 2로 갱신되게하시오 )

 "누구에게 의지하지 않고 직접 내가 코드 구현하기 ! "

import numpy as np

def my_kmeans(test, k):
    cluster = test.iloc[np.random.choice( len(test), k, replace=False), :]
    cluster['kind']=np.random.choice([1,2],2,replace=False)
    a=pd.DataFrame({'kind':[0]*len(test), 'd':[999999]*len(test)})
    
    for i in range(len(cluster)):
        temp = np.sqrt(np.sum((test.values-cluster.iloc[ : , :-1].values[i])**2, axis=1))
         #각각의 중심점에 대해서 거리8개가 리스트에 담김
        for j in range(len(temp)):
            a['kind'][j] = cluster.kind.iloc[i]  if a.d[j] > temp[j] else a['kind'][j]
            a['d'][j]=temp[j] if a.d[j]>temp[j] else a.d[j]
    return a
            #a데이터 프레임에 거리(d)의 j번째 행의 데이터를 갱신하는데
            #기존거리와 비교해서 작으면 갱신
        

my_kmeans(fruits.iloc[:, [1,2]],2)
 

예제11.  위에서 라벨링된  1에 대한 x 좌표(단맛) 와 y좌표(아삭한맛) 을 출력하시오 !


먼저 10장을 보고 다시 완성하러 돌아오겠습니다. 

■ 10장. 모델 성능 평가 (p435)

학습목표:  앞에서는 우리가 여러가지 머신러닝 모델을 생성해 보았습니다.
              여러가지 머신러닝 모델에서 현재 분석하는 데이터에 맞는 모델은
              이 모델이 맞다라고 판단할 수 있게 도움을 주는 내용이 이번장의 내용

관련된 면접질문:  제출한 데이터 분석 포트폴리오에서 모델을 이 머신러닝 모델로 
                        선택한 이유가 무엇입니까 ?

답변:  모델 성능 평가를 통해서 확인한 결과 이 모델이 가장 우수했습니다.

관련된 면접질문 :  모델의 성능 평가할때 사용한 척도가 무엇입니까?

답변: 정확도와 더블어 다른 성능척도들을 같이 비교해보았습니다. 

■ 모델 성능 평가가 중요한 이유는 무엇입니까 ?

 머신러닝이 수행한 결과(분류, 예측) 에 대한 공정한 평가를 통해
 머신러닝이 앞으로도 미래의 데이터에 대해서 잘 분류하고 
 예측할 수 있도록 해주고 분류 결과가 요행수로 맞힌게 아니라는것을 
 확인하게 해주며 분류결과를 좀더 일반화 할 수 있기 때문입니다.

■ 그동안 사용했던 성능평가에서 사용한 방법 ?

 분류를 할 때는 "정확도"로 확인했었고 회귀분석일 때는 "결정계수"로 성능평가를 했습니다. 

■  모델 성능평가를 위해 정확도만으로는 충분하지 않은 이유 ?

  암판정을 하는 분류기가 99% 의 정확도를 갖고 있다고 하면 1%의 오류율이 있기 때문에
  어떤 데이터에 대해서는 오류를 범할 수도 있게 됩니다. 
  그래서 정확도만으로는 성능을 측정하는데 충분하지가 않습니다.
  정확도와 더불어서 뷴류기의 유용성에 대한 다른 성능 척도를 정의하는게 중요합니다.

  " 정확도 + 다른 성능 척도"

■ 그럼 다른 성능 척도에는 무엇이 있는가 ?

 1. 카파통계량
 2. 민감도와 특이도
 3. 정밀도와 재현율
 4. Roc 곡선
 5. F1 score 

■ 정확도를 계산하는 방법 

                          TP + TN
정확도 = -----------------------------------
                   TP + TN + FP + FN

 Positive ?  관심범주    

               암을 분류하는 모형의 관심범주는 ?   암(positive)
                                                              정상(negative)

               스펨메일을 분류하는 모형의 관심범주는?  스팸메일(positive)
                                                                      햄메일(negative)

예측이 정확한 경우 :  TP(True Positive) :  실제값이 positive 이고 예측값도 positive 인 경우
                             TN(True Negative) : 실제값이 negative 이고 예측값도 negative 인 경우

예측이 틀린경우 :    FP(False Positive) : 실제값은 negative 였으나 예측값이 positive 인 경우
                           FN(False Negative): 실제값은 positive 였으나 예측값이 negative 인 경우

책 444 페이지의 그림 10.3 

예제1.  책 444 페이지의 그림 10.3 의 결과를 R 로 출력하시오 !

sms_result <- read.csv("sms_results.csv")

library(gmodels)
CrossTable( sms_result$actual_type,  sms_result$predict_type) 

■ 다른 성능척도 ?

1. 카파통계량
2. 민감도와 특이도
3. 정밀도와 재현율
4. Roc 곡선
5. F1 score

▩ 카파 통계량 (p449)

  두 관찰자간의 측정 범주값에 대한 일치도를 측정하는 방법입니다.

                     Pr(a)  -  Pr(e)
 kappa = ----------------------------------
                      1 - Pr(e)


Pr(a) : 데이터에서 관찰된 2명의 평가자들의 일치확률
Pr(e) : 2명의 평가자들이 데이터로 부터 계산된 확률적 일치확률(우연히 일치할 확률)

 카파 통계량이 0 이면 완전 불일치이고 1이면 완전 일치입니다. 

   kappa      일치정도
  0.8 ~ 1.0    매우 좋은 일치
  0.6 ~ 0.8    좋은 일치
  0.4 ~ 0.6    보통 일치
  0.2 ~ 0.4    어느정도 일치
  0.0 ~ 0.2    거의 일치하지 않음 

예제:  시험을 응시한 학생이 100명이라고 할때, 2명의 평가자가 합격, 불합격을 각각 판정하고
          두 평가자의 일치도를 아래와 같이 보여주고 있다.
 
                        평가자 A
                         합격      불합격
평가자 B   합격      40           10         50
             불합격     20          30          50
                          60           40        100


                     Pr(a)  -  Pr(e)
 kappa = ----------------------------------
                      1 - Pr(e)


Pr(a) :  데이터에서 관찰된 2명의 평가자들의 일치 확률 

                        40  +  30
 Pr(a) = ------------------------------ = 0.7
                          100

Pr(e) : 2명의 평가자들이 데이터로 부터 계산된 확률적 일치확률(우연히 일치할 확률)

                            평가자 A
                         합격      불합격
평가자 B   합격      40           10         50
             불합격     20          30          50
                          60           40        100

평가자 A :  합격을 60번, 불합격을 40번 주었다.
               평가자 A 는 합격을 60/100 = 0.6 의 확률
                             불합격을 40/100 = 0.4 의 확률

평가자 B :  합격을 50번, 불합격을 50번 주었다.
               평가자 B 는 합격을 50/100 = 0.5 의 확률
                             불합격을 50/100 = 0.5 의 확률

Pr(e) : 2명의 평가자들이 데이터로 부터 계산된 확률적 일치확률(우연히 일치할 확률)

 평가자 A 와 평가자 B 모두 확률적으로 "합격" 을 줄 확률은 ? 0.6 x 0.5 = 0.3 
 평가자 A 와 평가자 B 모두 확률적으로 "불합격" 을 줄 확률으? 0.4 x 0.5 = 0.2
 Pr(e) 는 데이터로 부터 계산된 확률적으로 일치할 확률이므로 이 둘을 더해서 
 0.3 + 0.2 = 0.5  입니다.

                 Pr(a) - Pr(e)            0.7 - 0.5             0.2
Kappa = ---------------------- = -------------- =  ---------= 0.4 (보통일치)
                   1 - Pr(e)               1 - 0.5               0.5

   kappa      일치정도
  0.8 ~ 1.0    매우 좋은 일치
  0.6 ~ 0.8    좋은 일치
  0.4 ~ 0.6    보통 일치
  0.2 ~ 0.4    어느정도 일치
  0.0 ~ 0.2    거의 일치하지 않음 

예제1. 스팸메일 분류 모델의 카파 통계량을 구하시오 !

sms_result <-  read.csv("sms_results.csv")
head(sms_result) 

install.packages("vcd")
library(vcd)

table( sms_result$actual_type, sms_result$predict_type)

Kappa( table( sms_result$actual_type, sms_result$predict_type) )

            value     ASE     z Pr(>|z|)
Unweighted 0.8825 0.01949 45.27        0
Weighted   0.8825 0.01949 45.27        0

예제2. 아래의 혼동행렬에서 카파통계량은 얼마인가 ?

                       예측 

                      False   True
실제      False      70     30   100
           True       40     60   100
           합계      110    90    200

답:  a <- as.table( matrix( c(70, 30, 40, 60), byrow=T, nrow=2, ncol=2) ) 
      a
      Kappa(a)  # 0.3 으로 어느정도 일치

예제3. 카파통계량을 파이썬으로 구하시오 !

from  sklearn.metrics  import  cohen_kappa_score

y_true = [ 2, 0, 2, 2, 0, 1 ]
y_pred = [0, 0, 2, 2, 0, 2 ] 

cohen_kappa_score( y_true, y_pred) 


예제4. sms_results.csv 의 스팸메일 분류 모형의 카파통계량을 파이썬으로 구하시오 !

from  sklearn.metrics  import  cohen_kappa_score

sms_result = pd.read_csv("d:\\data\\sms_results.csv")

cohen_kappa_score( sms_result['actual_type'] , sms_result['predict_type'] ) 

예제5.  iris 데이터를 분류하는 knn 모델의 정확도와 카파통계량을 같이 출력하시오 !

참고 코드:   세현이의 아이리스 knn 모델 (게시글 1709번)

# 데이터 로드
import pandas as pd
iris = pd.read_csv("d:\\data\\iris2.csv")
# 결측치, 이상치, 명목형데이터 없음

# 데이터 정규화
from   sklearn.preprocessing  import  MinMaxScaler 
scaler = MinMaxScaler()   
scaler.fit(iris.iloc[:,:-1])
x = scaler.transform(iris.iloc[:,:-1])
y = iris['Species'].to_numpy()

# 훈련/테스트 데이터 분리 (훈련90%, 테스트:10%)
from sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)

# my_knn 함수 생성
def my_knn(train, test, label, k):
    pred = []
    for i in range(0,len(test)):
        temp = np.sqrt(np.sum((train - test[i])**2,axis=1))
        result=pd.DataFrame({'rnk': list(map(int,pd.Series(temp).rank())),
                                               'label': label})
        pred.append((result['label'][result.rnk <= k].mode())[0])
    return pred

# 모델 예측
result = my_knn(x_train, x_test, y_train, 1)

# 모델을 평가
from  sklearn.metrics  import  accuracy_score
accuracy_score( y_test, result)  # 0.9333333333333333

# 카페 통계량 
from  sklearn.metrics  import  cohen_kappa_score

cohen_kappa_score( y_test ,  result )   # 0.8863636363636364


▩ 다른 성능 척도 

1. 카파통계량
2. 민감도와 특이도
3. 정밀도와 재현율
4. Roc 곡선
5. F1 score

▩ 민감도와 특이도 (p454)

 유용한 분류기를 찾으려면 보통 지나치게 보수적인 예측과 지나치게 공격적인 예측사이에
 균형이 필요합니다.

 보수적인 예측과 공격전이 예측에 대한 것을 정하는 기준이 되는 정보가 민감도와 특이도 입니다.

* 민감도:  실제로 '긍정' 인 범주 중에서 '긍정'으로 올바르게 예측한 비율

            예측
실제     TN   FP
           FN   TP 

                                  TP
 sensitivity = ----------------------------------
                            TP  +  FN

예:  실제로 구매한 사람중에서 구매할 것이라고 예측한(TP) 사람의 비율

* 특이도: 실제로 '부정' 인 범주중에서 '부정' 으로 올바르게 예측한(TN) 비율

            예측
실제     TN   FP
           FN   TP 

                               TN 
specificity = ------------------------------
                          TN + FP

예: 실제로 구매안한 사람중에서 구매안할것으로 예측한 비율.    

민감도와 특이도도 카파통계량 처럼 0~1까지의 범위에 있으며,
값이 1에 가까울 수록 바람직하나 실제로는 한쪽이 높으면 한쪽이 낮아져서 둘 다 높게 맞출수가 
없습니다. 

만약에 유방암을 예측하는 머신러닝 모델을 여러개 만들어 놓고 그중에 하나를 선택해야 한다면 

                         knn 모델          의사결정트리             신경망

정확도                    99%                99%                       99%
카파통계량              0.92                0.91                        0.91
민감도                    0.6                   0.7                         0.7
특이도                    0.5                   0.4                         0.5


민감도를 최고로 높게 맞춰놓고 그중에 특이도가 높은것을 선택하면됩니다.
민감도가 높으면 특이도가 다소 낮더라도 가치는 충분합니다. 

예제1.  스팸분류기 모델의 민감도와 특이도를 R 로 구하시오 !
               예측
                 ham          spam
실제  ham    TN (1203)     FP(4)
        spam   FN (31)        TP(152)

sms_result <- read.csv("sms_results.csv", stringsAsFactors=TRUE )

#install.packages("caret")
library(caret)

sensitivity( sms_result$predict_type, sms_result$actual_type, positive='spam') # 0.8306011

specificity(  sms_result$predict_type, sms_result$actual_type, negative='ham')  #  0.996686

예제2. 아래의 혼동행렬에서 민감도와 특이도를 각각 구하시오 !

                       예측 
                      False   True
실제      False      70     30   100
           True       40     60   100
           합계      110    90    200

      위의 혼동행렬을 아래와 같이 변경하고 수행합니다.

실제
                  True     False                                           45분까지 쉬세요 ~~
예측  True     60        30     90
        False     40        70     110
        합계     100       100     200

a <- as.table( matrix( c(60, 30, 40, 70), byrow=T, nrow=2, ncol=2) ) 
sensitivity(a)
specificity(a)

문제345. (오늘의 마지막 문제)  유방암 데이터를 파이썬으로 분류하는 전체 코드를 가져다가 
            정확도, 카파통계량, 민감도, 특이도를 각각 출력하시오 !

게시글 1710번을 참고하세요.  오늘의 마지막 문제를 풀기위한 유방암 분류 전체 코드 

5시 신호보냈습니다. ~~~ 나머지 시간은 자유롭게 자습 또는 스터디 하세요~
6시 신호보냈습니다. ~~~~~

#1. 데이터를 로드합니다.
import  pandas  as  pd

wbcd = pd.read_csv("d:\\data\\wisc_bc_data.csv")

# 데이터를 정규화 합니다.
from   sklearn.preprocessing  import  MinMaxScaler

wbcd2 = wbcd.iloc[ :  , 2: ]  # 환자번호와 diagnosis 제외합니다. 

scaler = MinMaxScaler()   
scaler.fit(wbcd2)   #  최대최소법으로 데이터를 계산합니다.
wbcd2_scaled = scaler.transform(wbcd2)  # 위에서 계산한 내용으로 데이터를 

y = wbcd['diagnosis'].to_numpy()    # 정답 데이터를 numpy array 로 변경합니다.

# 훈련데이터와 테스트데이터로 데이터를 분리합니다.(훈련90%, 테스트:10%)

from sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( wbcd2_scaled, y, test_size=0.1, random_state=1)

#모델을 설정합니다.
from  sklearn.neighbors   import  KNeighborsClassifier

model = KNeighborsClassifier( n_neighbors= 5 )  # knn 모델생성

#모델을 훈련시킵니다.
model.fit( x_train, y_train )

#훈련된 모델로 테스트 데이터를 예측합니다.
result = model.predict(x_test)
result

#모델을 평가합니다.

from  sklearn.metrics  import  accuracy_score

acurracy = accuracy_score( y_test, result)
acurracy

#12. 모델의 성능을 높입니다.

from  sklearn.metrics  import  confusion_matrix
a = confusion_matrix( y_test, result )
print(a)
from sklearn.metrics import confusion_matrix
tn, fp, fn, tp = confusion_matrix( y_test, result ).ravel()
print(tn)
print(fp)
print(fn)
print(tp)


10장의 수업내용은 정확도외에 또 다른 성능 척도를 배우는 내용입니다.

                                    예측
                                  TN   FP
                      실제      FN   TP
                                       
 1. 정확도

 2. 카파 통계량 : 두 관찰자간(예측,실제)의 측정 범주값에 대한 일치도 (0~1)
 3. 민감도 :  실제로 '긍정' 인 범주 중에서 '긍정' 으로 올바르게 예측한 비율(tp/(tp+fn))
 4. 특이도 :  실제로 '부정' 인 범주 중에서 '부정' 으로 올바르게 예측한 비율 (tn/(tn+fp))
 5. 정밀도  :  '긍정' 으로 예측한것 중에서 실제로 '긍정' 인것의 비율 ( tp/(tp+fp) )
 6. 재현율 :   민감도와 같습니다.  ( tp/(tp+fp) )
 7. F1 score
 8. Roc 곡선
 9. auc score 

■ 정밀도와 재현율 (p456)

정밀도:   '긍정' 으로 예측한것 중에서 실제로 '긍정' 인것의 비율 ( tp/(tp+fp) )


재현율 :   민감도와 같습니다.  ( tp/(tp+fp) )


1. 소극적 예측 :  암이라고 판단하는것 자체를 소극적으로 봐서 확실한 경우가 아니면 
                      암으로 판단을 안하는 것이다.

                      정밀도 ↑     재현율  ↓
   
2. 공격적 예측 :  조금만 의심이 가도 다 암이라고 판단한다. 

                      정밀도  ↓     재현율  ↑

  이 모델은 암인데 암이라고 판단하면 잘한거고 혹시 암이 아닌 사람을 암으로 예측한것인데
  실제로 나중에 검사결과가 정상이었다면 오히려 다행인것이어서 그냥 다시 재검사 받으면 된다. 


예제1.  아래의 스팸분류기의 정밀도를 구하시오 !  precision = tp/(tp+fp)

   TN(1203)    FP(4)
   FN(31)       TP(152)

sms_result <- read.csv("sms_results.csv",  stringsAsFactors=TRUE)
library(caret)
posPredValue( sms_result$predict_type,  sms_result$actual_type, positive='spam')

 0.974359

예제2.  아래의 혼동행렬에서 정밀도를 구하시오 !  precision = tp/(tp+fp)
책 형태:
                                   예측 

                          관심범주x   관심범주o
실제  관심범주x      TN(70)        FP(30)  100
        관심범주o     FN(40)        TP(60)  100
                            110             90    200
      
R 패키지에 넣는 형태 :
                                   실제
                         관심범주x       관심범주o
예측 관심범주x       TN(70)          FN(40) 
       관심범주o       FP(30)          TP(60) 

a <- as.table( matrix( c(70, 40, 30, 60), byrow=T, nrow=2, ncol=2) )
posPredValue(a, positive='B')  # 0.6666667


예제3. 위의 혼동행렬의 재현율을 구하시오 !   재현율을 민감도와 같다  (tp/(tp+fn))

책 형태:
                                   예측 

                          관심범주x   관심범주o
실제  관심범주x      TN(70)        FP(30)  100
        관심범주o     FN(40)        TP(60)  100
                            110             90    200
      
R 패키지에 넣는 형태 :
                                   실제
                         관심범주x       관심범주o
예측 관심범주x       TN(70)          FN(40) 
       관심범주o       FP(30)          TP(60) 

a <- as.table( matrix( c(70, 40, 30, 60), byrow=T, nrow=2, ncol=2) )

sensitivity(a, positive='B')   #0.6

예제4. 스팸분류기의 정밀도와 재현율을 파이썬으로 구하시오 !

sms_result = pd.read_csv("d:\\data\\sms_results.csv")

y_test = sms_result['actual_type'] # 실제값
result = sms_result['predict_type'] # 예측값

from  sklearn.metrics  import  confusion_matrix

tn, fp, fn, tp = confusion_matrix( y_test, result ).ravel()
print( tn, fp, fn, tp) # 1203, 4, 31, 152

precision = tp/(tp+fp)
print( '정밀도' , precision)  # 0.9743589743589743

recall = tp/(tp+fn)
print('재현율', recall)  # 0.8306010928961749

예제5. 스팸분류 모델의 정확도, 카파통계량, 민감도, 특이도, 정밀도, 재현율을 각각 구하시오 !

sms_result = pd.read_csv("d:\\data\\sms_results.csv")

y_test = sms_result['actual_type'] # 실제값
result = sms_result['predict_type'] # 예측값

from sklearn.metrics import  accuracy_score
accuracy = accuracy_score(y_test, result)
print('정확도', accuracy)

from sklearn.metrics  import  cohen_kappa_score
print('카파통계량', cohen_kappa_score(y_test, result) )

from  sklearn.metrics  import  confusion_matrix

tn, fp, fn, tp = confusion_matrix( y_test, result ).ravel()
print( tn, fp, fn, tp) # 1203, 4, 31, 152

sensitivity = tp/(tp+fn)
print('민감도', sensitivity)

specificity = tn/(tn+fp)
print('특이도', specificity )

precision = tp/(tp+fp)
print( '정밀도' , precision)  

recall = tp/(tp+fn)
print('재현율', recall)  

예제6. 유방암을 분류하는 모델의 정확도와 카파통계량, 민감도, 특이도, 정밀도, 재현율을 각각 구하시오

아래의 코드를 참고하세요 ~    답글로 달아주세요 ~~

#1. 데이터를 로드합니다.
import  pandas  as  pd

wbcd = pd.read_csv("d:\\data\\wisc_bc_data.csv")

# 데이터를 정규화 합니다.
from   sklearn.preprocessing  import  MinMaxScaler

wbcd2 = wbcd.iloc[ :  , 2: ]  # 환자번호와 diagnosis 제외합니다. 

scaler = MinMaxScaler()   
scaler.fit(wbcd2)   #  최대최소법으로 데이터를 계산합니다.
wbcd2_scaled = scaler.transform(wbcd2)  # 위에서 계산한 내용으로 데이터를 

y = wbcd['diagnosis'].to_numpy()    # 정답 데이터를 numpy array 로 변경합니다.

# 훈련데이터와 테스트데이터로 데이터를 분리합니다.(훈련90%, 테스트:10%)

from sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( wbcd2_scaled, y, test_size=0.1, random_state=1)

#모델을 설정합니다.
from  sklearn.neighbors   import  KNeighborsClassifier

model = KNeighborsClassifier( n_neighbors= 5 )  # knn 모델생성

#모델을 훈련시킵니다.
model.fit( x_train, y_train )

#훈련된 모델로 테스트 데이터를 예측합니다.
result = model.predict(x_test)
result

# 모델을 평가합니다.  (정확도)
from  sklearn.metrics  import  accuracy_score
acurracy = accuracy_score( y_test, result)
print(acurracy)            

▩ F-척도  (p458)

 정밀도와 재현율을 하나의 값으로 결합한 성능 척도를 F-척도라고 합니다.
 F1점수 로 부르기도 합니다. F-척도는 조화평균을 이용해서 정밀도와 재현율을 
 결합합니다.

                   2 x 정밀도 x 재현율                  2 x tp 
 F-척도 = ------------------------------- = ------------------------
                    재현율 + 정밀도               2 x  tp + fp  + fn

예제1. 유방암의 악성종양을 분류하는 의사결정트리 분류모델의 F-척도를 출력하시오 !

from sklearn.metrics import confusion_matrix
tn, fp, fn, tp = confusion_matrix( y_test, result ).ravel()

f1_score = (2*tp) / ( 2*tp+fp+fn) 
print(f1_score)

또는 

from sklearn.metrics import classification_report

target_names = ['class B', 'class M']
print(classification_report(y_test, result, target_names=target_names))

관심범주의 F-척도가 0.96으로 출력되고 있습니다.    

예제2. 오늘 점심시간 문제인 독일은행 채무 불이행자 예측 모델인 의사결정트리의
         F1 score 를 출력하시오 !

#▩ 판다스로 독일 은행의 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape  # (1000, 17)

# 2. 데이터 탐색(결측치 확인)
credit.isnull().sum()

# 3. 데이터 탐색(이상치 확인)
credit.info()

def outlier_value(x):
    for i in x.columns[x.dtypes=='int64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
outlier_value( credit )


# 4. 데이터 탐색(명목형 데이터)
credit.info()

credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )
credit2.info()

# 5. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
x
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성
y

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

print ( x_train.shape)  # (900, 44)
print ( x_test.shape)   # (100, 44)
print ( y_train.shape)  # (900, )
print ( y_test.shape)   # (100, )

# 6. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 7. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

print( x_train2.max(),  x_train2.min() )
print( x_test2.max(), x_test2.min()  )

# 8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5)


# 9. 모델 훈련
# 모델명.fit(훈련 데이터, 정답 데이터)

model.fit( x_train2,  y_train)

# 10. 모델 예측

result = model.predict( x_test2 )
result

# 11. 모델 평가

sum( result == y_test ) / len(y_test)

from  sklearn.metrics  import  confusion_matrix

a = confusion_matrix( y_test, result )
a

from sklearn.metrics import classification_report

target_names = ['채무이행자', '채무불이행자']
print(classification_report(y_test, result, target_names=target_names))  # 0.60


# 모델을 평가합니다.  (정확도)
from  sklearn.metrics  import  accuracy_score
acurracy = accuracy_score( y_test, result)
print(acurracy)       # 0.76

예제3. 예제2번에서 만든 의사결정트리 모델의 정확도를 더 높이시오 !
          f1 score 도 같이 올라가는지 확인하시오 (랜덤포레스트로 변경하세요)

#의사결정트리 
from  sklearn.tree  import  DecisionTreeClassifier
model = DecisionTreeClassifier(criterion='entropy', max_depth=5)

# 랜덤포레스트
from  sklearn.ensemble  import  RandomForestClassifier
model = RandomForestClassifier(random_state=1) 

모델         의사결정트리       랜덤포레스트 
정확도 :        0.76                   0.77
F1-척도:        0.60                   0.55 

※ 설명:  F1 스코아 값이 작으면 잘못 판단한 false 값들에  문제가 있다는 것입니다.
FP 에 문제가 있는것인지 FN 에 문제가 있는것이지를 확인하는 작업이 필요합니다.

                      Precision(정밀도) x recall(재현율)
F1 score =  2 x ---------------------------------
                      Precision(정밀도) + recall(재현율)

예제4. 위의 2개의 모델중에 FN 값과 FP 값을 각각 확인해서 어떤 모델이 더 False 값이 
         많은지 확인하시오 !

#의사결정트리 
from  sklearn.tree  import  DecisionTreeClassifier
model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=1)
#f척도 :0.60
array([[58, 12],
       [12, 18]], dtype=int64)

# 랜덤포레스트
from  sklearn.ensemble  import  RandomForestClassifier
model = RandomForestClassifier(random_state=1) 
#f척도: 0.55
array([[63,  7],
       [16, 14]], dtype=int64)


▩ 정확도외에 또 다른 성능 척도 

1. 정확도
2. 카파통계량
3. 민감도
4. 특이도
5. 정밀도
6. 재현율
7. F-척도
8. Roc 커브
9. AUC 

▩  Roc 커브  (p459)

 Receiver  Operating  Characteristic  의 약자입니다.

 같은 데이터에 대한 여러 모델들의 성능을 시각화해서 비교해주는데 도움을 주는 그래프

 그림 10.4




  




위의 2차원에 그려지는 그래프이고 x축이 False positive rate 이고 y 축이 True positive rate 입니다.
다른 성능척도인 민감도와 특이도를 숫자로만 보는게 아니라 시각화 해서 
여러 모델중에 가장 좋은 모델을 쉽게 찾게 해주는 그래프가 바로 ROC 커브 그래프입니다.

ROC 커브의 x축은 FPR(False Positive Rate) 로 정상환자를 암환자로 잘못 판정한 비율
                y축은 TPR(True  Positive Rate) 로 암환자를 암환자로 잘 판정한 비율

FPR 은 낮추면서 TPR 을 높일 수 있는 모형이 가장 좋은 모형이고 최대한 FRP 은 낮추면서
TRP 을 높일 수 있는 cut off 지점을 찾는게 ROC 그래프를 그리는 이유가 됩니다. 

예제1.  독일은행의 채무불이행자를 예측하는 모델의 ROC 커브를 그리시오 !

#1. 데이터를 로드한다.

credit <- read.csv("credit.csv", stringsAsFactor=TRUE)
str(credit) 

#2. 데이터에 각 컬럼들을 이해한다. 

#라벨 컬럼 :  default  --->  yes : 대출금 상환 안함 
#no  : 대출금 상환 

prop.table( table(credit$default)  )
summary( credit$amount)

#3. 데이터가 명목형 데이터인지 확인해본다.

str(credit) 

#4. 데이터를 shuffle 시킨다.

set.seed(31)
credit_shuffle <-  credit[ sample( nrow(credit) ),  ]

#5. 데이터를 9 대 1로 나눈다.

train_num <- round( 0.9 * nrow(credit_shuffle), 0) 

credit_train <- credit_shuffle[1:train_num ,  ]

credit_test  <- credit_shuffle[(train_num+1) : nrow(credit_shuffle),  ]


#6. C5.0 패키지와 훈련 데이터를 이용해서 모델을 생성한다.

library(C50)

credit_model <- C5.0( credit_train[ ,-17] , credit_train[  , 17] )

#7. 위에서 만든 모델을 이용해서 테스트 데이터의 라벨을 예측한다.

credit_result <-  predict( credit_model, credit_test[  , -17] )

#8. 이원 교차표로 결과를 확인한다.

library(gmodels)

CrossTable( credit_test[   , 17], credit_result )

설명: 정확도 77%의 모델입니다. 

#9. 실제값과 예측값 그리고 예측 확률을 담는 credit_results 라는 데이터 프레임을 생성합니다.

credit_test_prob <- predict ( credit_model,  credit_test[  , -17], type="prob" )
credit_test_prob

            no        yes
550 0.86690904 0.13309096
601 0.17472222 0.82527778
696 0.86690904 0.13309096
949 0.86690904 0.13309096

설명:  type='prob' 옵션을 주게 되면 확률이 출력됩니다. 

credit_results <- data.frame( actural_type=credit_test[  , 17],
                                       predict_type=credit_result,
                                       prob_yes = round(credit_test_prob[ , 2], 5),
                                       prob_no = round( credit_test_prob[ , 1], 5)  )

credit_results

# 10. credit_results 데이터 프레임을 csv 파일로 저장합니다. 

write.csv( credit_results, 'd:\\data\\final_results.csv', row.names=FALSE)

#11. ROC 커브 그래프 그리기 

install.packages("ROCR")
library(ROCR)

head(credit_results)

pred <- prediction( predictions= credit_results$prob_yes,
                           labels = credit_results$actural_type) 

pred

# 설명:  prediction( predictions= 관김범주의 확률, labels=실제정답) 
# pred 에 담기는게 ROC 커브를 그리기 위한 100개의 데이터 포인트가 담긴다. 

perf <-  performance( pred, measure="tpr", x.measure="fpr")
perf

#설명: roc 커브의 x축인 fpr, y축인 tpr 에 해당되는 실제 data point 24개가 생성됨 

plot(  perf,  main="ROC 커브",  col="blue", lwd=2)

# 대각선 출력

abline( a=0, b=1, lwd=2, lty=2)

# 설명:  a 는 직선의 절편, 1은 직선의 기울기, 
           lwd 는 선의 굵기, lty=2 는 점선으로 표현

#12.  AUC 계산하기

AUC ?  Area  Under  Curve 의 약자로 곡선 아래쪽의 넓이를 말합니다. 

value <-  performance( pred, measure="auc")

str(value)
unlist( value@y.values)  #  0.6668286       

책 461 페이지 

예제2.  예제1번의 의사결정트리 모델의 성능을 더 높이고 roc 커브를 다시 그리는데
          auc값도 더 커지는지 확인하시오 !

  seed값을 659 로 주고 trails 을 100으로 주세요 ~~

auc 가 0.6668286   에서 0.7998904 로 올라갔습니다. 

예제3. 위의 실험을 파이썬으로 수행하시오 !

# 1. 데이터를 로드
import  pandas  as  pd 
credit = pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape  # (1000, 17)

# credit['default']=(credit['default']=='yes').astype(int)
# 예측하기 전 숫자로 바꾼 파생변수를 추가하는 방법

# 2. 데이터 탐색(결측치 확인)
# credit.isnull().sum()

# 3. 데이터 탐색(이상치 확인)
# credit.info()

def outlier_value(x):
    for i in x.columns[x.dtypes=='int64']:
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        print(i,x[i][(x[i]>Q3+IQR* 1.5)|(x[i]<Q1-IQR*1.5)].count())
        
# outlier_value( credit )

# 4. 데이터 탐색(명목형 데이터)
# credit.info()

credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )
# credit2.info()

# 5. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                # 학습 시킬 데이터 생성 
# x
y =  credit.iloc[ :  , -1].to_numpy()  # 정답 데이터 생성
# y

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

# print ( x_train.shape)  # (900, 44)
# print ( x_test.shape)   # (100, 44)
# print ( y_train.shape)  # (900, )
# print ( y_test.shape)   # (100, )

# 6. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 7. 계산된 내용으로 훈련 데이터를 변형
x_train2 = scaler.transform(x_train)

# 계산된 내용으로 테스트 데이터를 변형
x_test2 = scaler.transform(x_test)

# print( x_train2.max(), x_train2.min() )  # 1.0  0.0
# print( x_test2.max(), x_test2.min()  )   # 1.21428571  0.0

# 8. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier

model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=1)

# 9. 모델 훈련
model.fit( x_train2,  y_train)  # 모델명.fit(훈련 데이터, 정답 데이터)

# 10. 모델 예측
result = model.predict( x_test2 )
#result

preds2 = model.predict_proba(x_test2)[:, 1]
preds2

y_test2=[]
for i in  list(y_test):
    if i =='yes':  # yes : 대출금 상환 안함 
        y_test2.append(1)
    else:
        y_test2.append(0)

fpr, tpr, threshold = metrics.roc_curve(y_test2, preds)
roc_auc = metrics.auc(fpr, tpr)
roc_auc

#설명 :  metrics.roc_curve( 예측값, 예측확률 )
                                        ↑
#            예측값을 0과 1로 제공해줘야합니다. 1이 관심범주 입니다. 
#            예측확률은 관심범주쪽의 확률로 제공해줘야합니다.

# 실행문:   metrics.roc_curve(y_test2, preds2 )

# method I: plt
import matplotlib.pyplot as plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

예제4. 유방암 데이터를 분류하는 분류 모델의 정확도를 확인합니다.

#1. 데이터를 로드합니다.
import  pandas  as  pd

wbcd = pd.read_csv("d:\\data\\wisc_bc_data.csv")

# 데이터를 정규화 합니다.
from   sklearn.preprocessing  import  MinMaxScaler

wbcd2 = wbcd.iloc[ :  , 2: ]  # 환자번호와 diagnosis 제외합니다. 

scaler = MinMaxScaler()   
scaler.fit(wbcd2)   #  최대최소법으로 데이터를 계산합니다.
wbcd2_scaled = scaler.transform(wbcd2)  # 위에서 계산한 내용으로 데이터를 

y = wbcd['diagnosis'].to_numpy()    # 정답 데이터를 numpy array 로 변경합니다.

# 훈련데이터와 테스트데이터로 데이터를 분리합니다.(훈련90%, 테스트:10%)

from sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( wbcd2_scaled, y, test_size=0.1, random_state=1)

#모델을 설정합니다.
from  sklearn.neighbors   import  KNeighborsClassifier

model = KNeighborsClassifier( n_neighbors= 5 )  # knn 모델생성

#모델을 훈련시킵니다.
model.fit( x_train, y_train )

#훈련된 모델로 테스트 데이터를 예측합니다.
result = model.predict(x_test)
result

# 모델을 평가합니다.  (정확도)
from  sklearn.metrics  import  accuracy_score
acurracy = accuracy_score( y_test, result)
print(acurracy)                                    #0.9824561403508771

#. 다른 성능평가 (카파통계량)
from sklearn.metrics import cohen_kappa_score
print(cohen_kappa_score(y_test, result))           #0.9514893617021276

#. 다른 성능평가 (민감도)
from sklearn.metrics import recall_score
print(recall_score(y_test, result, pos_label='M'))  

#. 다른 성능평가 (특이도)
from sklearn.metrics import confusion_matrix
tn, fp, fn, tp= confusion_matrix( y_test, result ).ravel()

sensitivity=tp/(tp+fn)                           
print(sensitivity)

#. 다른 성능평가 (특이도)

specificity=tn/(tn+fp)
print(specificity)                                


예제5. 위의 유방암 분류 머신러닝 모델의 ROC 커브를 그리시오 ~

 카페에 답글로 올려주세요 ~~    45분까지 쉬세요 ~~~




▩ ROC 곡선에서 확인해야할 두가지 정보 

1. AUC score 
2. cut off 지점 : 참긍정률과 거짓 긍정률의 적절한 기준점의 절사점

■ x축을 fpr 하고 y축을 tpr 하는 2차원 그래프에 cutoff 지점을 시각화하는 코드 

#1. 데이터를 로드한다.

credit <- read.csv("credit.csv", stringsAsFactor=TRUE)
str(credit) 

#2. 데이터에 각 컬럼들을 이해한다. 

#라벨 컬럼 :  default  --->  yes : 대출금 상환 안함 
#no  : 대출금 상환 

prop.table( table(credit$default)  )
summary( credit$amount)

#3. 데이터가 명목형 데이터인지 확인해본다.

str(credit) 

#4. 데이터를 shuffle 시킨다.

set.seed(659)
credit_shuffle <-  credit[ sample( nrow(credit) ),  ]

#5. 데이터를 9 대 1로 나눈다.

train_num <- round( 0.9 * nrow(credit_shuffle), 0) 

credit_train <- credit_shuffle[1:train_num ,  ]
credit_test  <- credit_shuffle[(train_num+1) : nrow(credit_shuffle),  ]

nrow(credit_train)
nrow(credit_test)

#6. C5.0 패키지와 훈련 데이터를 이용해서 모델을 생성한다.

library(C50)
credit_model <- C5.0( credit_train[ ,-17] , credit_train[  , 17], trials=100 )

#7. 위에서 만든 모델을 이용해서 테스트 데이터의 라벨을 예측한다.

credit_result <-  predict( credit_model, credit_test[  , -17] )
credit_result

#8. 이원 교차표로 결과를 확인한다.

library(gmodels)

CrossTable( credit_test[   , 17], credit_result )

credit_test_prob <- predict ( credit_model,  credit_test[  , -17], type="prob" )
credit_test_prob


credit_results <- data.frame( actural_type=credit_test[  , 17],
                              predict_type=credit_result,
                              prob_yes = round(credit_test_prob[ , 2], 5),
                              prob_no = round( credit_test_prob[ , 1], 5)  )

credit_results

write.csv( credit_results, 'd:\\data\\final_results.csv', row.names=FALSE)

#install.packages("ROCR")
library(ROCR)
head(credit_results)

pred <- prediction( predictions= credit_results$prob_yes,
                           labels = credit_results$actural_type) 

# 정확도와 cutoff 출력하는 부분 
# perf <- performance(pred, measure = "tpr", x.measure = "fpr")
eval <- performance(pred,"acc")  # y 축을 정확도로 출력
eval  # 392개의 데이터 포인트 추출 
plot(eval)

#설명:  h는 수평선, v 가 수직선의 지점 

#Identifying the best cutoff  and Accuracy
eval #  x축이 cutoff 이고 y축이 정확도를 그래프로 시각화 하기 위한 392개의 데이터 포인트

slot(eval,"y.values") # 392개의 데이터 포인트를 출력 
max <- which.max(slot(eval,"y.values")[[1]])   # 392개의 데이터 포인트중에 max 값에 해당하는
# 인덱스 번호 출력 
max  # 61번째 인덱스 번호에 해당하는 데이터가 가장 큰 값

acc <- slot(eval,"y.values")[[1]][[max]]  #   y축에 정확도중에 61번째에 해당하는 값을 출력
cut <- slot(eval,"x.values")[[1]][[max]]  #   x축에 cutoff 값들중에서 61번째 해당하는 값을 출력
print(c(Accuracy=acc, Cutoff = cut))


perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
max
tpr <- slot(perf,"y.values")[[1]][[max]]
fpr <- slot(perf,"x.values")[[1]][[max]]
print(c(tpr,fpr))
abline(h=0.625, v=0.03947368)


코드 설명:   독일은행 채무 불이행자 예측 모델의 ROC 커브의 절사점인 cutoff 지점을 알아내기
                위해서 먼저  y축을 정확도로 두고 x 축을 cutoff 로 해서 정확도가 최대인 지점의 
                cutoff 를 알아내고 그 cutoff 를 가지고 x축 거짓긍정률로 하고 y 축을 참긍정률로 하는
                그래프에 cutoff 지점으로 설정한다. 

문제346.(오늘의 마지막 문제) 아래의 비어있는 표의 값을 채워넣으시오~

                        의사결정트리 (trials=100)            의사결정트리(trials=1)             

정확도:                  0.87
카파통계량:            0.61
f1 score :               0.69
auc score:              0.79
cut off  :                0.54
 
cut off  value 가 낮을수록 민감도는 증가하고, 특이도는 감소한다.
cut off  value 가 높을수록 민감도는 감소하고, 특이도는 증가한다.

5시 신호보냈습니다.  ~~ 오늘의 마지막 문제 답 올리시고 자유롭게 자습 또는 스터디하세요 ~~
6시 신호보냈습니다. ~~~



#■다른 성능척도 총정리 코드:             

#■ 실제값과 예측값 대입

credit_test_prob <- predict(credit_model, credit_test[   , -17], type = "prob")
credit_test_prob

# combine the results into a data frame
credit_results <- data.frame(actual_type =credit_test[  , 17],
                             predict_type = credit_result,
                             prob_yes = round(credit_test_prob[ , 2], 5),
                             prob_no = round(credit_test_prob[ , 1], 5))

#3. 예측 데이터 프레임을 csv 로 저장합니다.
# uncomment this line to output the sms_results to CSV
write.csv(credit_results, "final_results.csv", row.names = FALSE)


#■ 실제값과 예측값 대입

actual_type <- credit_test[  , 17]
predict_type <-  credit_result
positive_value <- 'yes'
negative_value <- 'no'

#■ 정확도

g <- CrossTable( actual_type, predict_type )
x <- sum(g$prop.tbl *diag(2))   # 정확도 확인하는 코드
x

#■ 카파통계량 
#install.packages("vcd")
library(vcd)
Kappa( table( actual_type, predict_type)  ) 

#■ 민감도
#install.packages("caret")
library(caret)
sensitivity( predict_type, actual_type,  positive=positive_value)

#■ 특이도
specificity(  predict_type, actual_type, negative=negative_value)  

#■ 정밀도
posPredValue( predict_type, actual_type, positive=positive_value) 

#■ 재현율 
sensitivity( predict_type, actual_type,  positive=positive_value) 


# calculate AUC
perf.auc <- performance(pred, measure = "auc")
str(perf.auc)
unlist(perf.auc@y.values)


#■ F척도

#1. F1 score 공식
# Fmeasure <- 2 * precision * recall / (precision + recall)

#2. 패키지를 이용하는 방법 
#install.packages("MLmetrics")
library(MLmetrics)

F1_Score(actual_type, predict_type, positive = positive_value)

■ 10장 성능평가 복습

정확도  :   실제 분류 범주를 정확하게 예측한 비율
카파통계량 : 두 관찰자(예측,실제)의 측정 범주값에 대한 일치도(0~1)

민감도 :  실제로 '긍정' 인 범주 중에서 '긍정' 으로 올바르게 예측한 비율(tp/(tp+fn))
특이도 :  실제로 '부정' 인 범주 중에서 '부정' 으로 올바르게 예측한 비율(tn/(tn+fp))

cut off value 가 낮을 수록 민감도는 증가하고, 특이도는 감소한다
cut off value 가 높을 수록 민감도는 감소하고, 특이도는 증가한다.

정밀도 : '긍정' 으로 예측한것 중에서 실제로 '긍정' 인것의 비율( tp/(tp+fp) )
재현율 :  민감도와 같음 ( tp/(tp+fp) )            
                     
F-척도 :  정밀도와 재현율을 하나의 값을 결합한 성능척도 ( 2*(정밀도*재현율) /(정밀도+재현율))
            이 값이 작게나오면 FN 값과 FP 값을 확인해봐야합니다.

공격적 예측 :   정밀도  ↓  재현율 ↑
소극적 예측 :   정밀도  ↑  재현율 ↓

ROC 곡선 :  y축을 민감도(TPR)로 하고 x축을 1-특이도(FPR)하여 그린 2차원 그래프로
                FPR 을 낮추면서 TPR 을 높일 수 있는 가장 좋은 모형을 찾기위한 시각화 도구

AUC score :  ROC 곡선의 아래의 면적으로 이 면적이 크면 클수록 분류를 잘하는 모형이다.

cut off :  참긍정률(TPR) 과 거짓긍정률(FPR) 읠 적절한 기준점의 절사점 

모델의 성능평가
-------------------------------------------------------------------------
모델의 성능개선 

▩ 홀드아웃 방법(P466)

 훈련 데이터셋 (90%)  ---> 훈련 데이터셋의 일부를 최적의 하이퍼 파라미터를 찾기 위해
                                    검정 데이터로 사용한다. 

 테스트 데이터셋 (10%)

훈련 데이터셋( 50%), 검정 데이터셋(25%), 테스트 데이터셋(25%)
                                   ↓
            최적의 하이퍼 파라미터를 찾아내기 위해 사용되는 데이터 

100개의 데이터를 받았다면 

100개를 전부 이용해서  훈련(90개)과 검정(10개)데이터로 나눈다.

교차검정을 통해서 최적의 하이퍼 파라미터를 알아냅니다.
그림

홀드아웃이란 ?  훈련 데이터셋과 테스트 데이터셋을 분할 하는 것 

교차검정 ?  훈련 데이터의 일부를 검정 데이터로 사용하여 최적의 하이퍼 파라미터를
               발견할 수 있도록 검정 데이터를 교차해서 검정하는 방법 

위의 작업을 하는 이유 ?  모델의 성능을 높이기 위해서 입니다. 

훈련 데이터와 테스트 데이트를 분리할 때 정답컬럼의 데이터의 비율이
훈련 데이터와 테스트 데이터에 잘 분배가 되어야합니다.

독일은행 데이터(1000건) (채무이행자 : 채무불이행자 = 7 : 3 )

훈련 데이터  : 900  ( 채무이행자  : 채무불이행자  = 7 : 3 )
테스트 데이터 : 100 ( 채무이행자  : 채무불이행자 = 7 : 3 )

예제1. runif 함수를 이용해서 훈련데이터와 테스트 데이터를 나눴을때

credit <- read.csv("credit.csv")
nrow(credit)

random_ids <- order(runif(1000)) # 난수 1000 개를 생성 

credit_train <- credit[ random_ids[1:750],   ]  #훈련 데이터 75%
credit_test <-  credit[ random_ids[751:1000],  ] # 테스트 데이터 25%

nrow(credit_train)  # 750
nrow(credit_test)   # 250 

# credit_train 데이터프레임에서 default 의 데이터의 비율이 어떻게 되는지 확인하시오!

table(credit$default)

table(credit_train$default)

 no   yes 
532 218 

prop.table( table(credit_train$default) )

      no       yes 
0.7093333 0.2906667 

# credit_test 데이터프렘의 default 의 데이터 값들의 비율을 출력하시오 !

prop.table( table(credit_test$default) )

    no    yes 
0.672   0.328 

runif 함수를 이용해서 위와같이 훈련과 테스트 데이터를 분리하면 정답의 비율을 잘 나누지
못합니다. 

위의 현상을 해결하려면 ?    createDataPartition 함수를 이용합니다.

#예제2.  createDataPartition 함수를 이용해서 훈련 데이터와 테스트 데이터를 나누기 

credit <- read.csv("credit.csv")

library(caret)

in_train <- createDataPartition( credit$default, p=0.75, list=FALSE)

credit_train <- credit[ in_train,  ]
credit_test <- credit[-in_train,  ]

nrow(credit_train) #750
nrow(credit_test)  #250

prop.table( table(credit_train$default) )
prop.table( table(credit_test$default)  )

훈련 데이터와 테스트 데이터 둘다 모두 잘 정답라벨 데이터를 분리 했습니다. 

예제3. 파이썬의 사이킷런도 정확히 정답컬럼을 분배하는지 실험하시오

#1. 데이터를 불러옵니다.
import  pandas  as  pd
credit = pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape
 
#2. 명목형 데이터를 더미변수화 합니다.

credit2 = pd.get_dummies( credit, drop_first=True )
credit2.info()

#3. 훈련 데이터와 테스트 데이터로 분리합니다.

x = credit2.iloc[ : , 0:-1].to_numpy()
y = credit2['default_yes'].to_numpy()

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=1)

print( x_train.shape ) # (750, 35)
print( x_test.shape) # (250, 35)

#4. y_train 정답컬럼 데이터의 비율을 출력하시오 !

np.bincount( y_train) / 750

array([0.69866667, 0.30133333])

#5. y_test 정답컬럼 데이터의 비율을 출력하시오 !

np.bincount( y_test) / 250

채무 불이행자 데이터도 잘 학습해야하고 채무 이행자 데이터도 잘 학습해야하므로
정답 데이터의 비율을 원본 데이터의 비율과 똑같이 나눠줘야합니다. 

train_test_split 에서 stratify=y  옵션을 쓰면 정확하게 나눠줍니다. 


#1. 데이터를 불러옵니다.
import  pandas  as  pd
credit = pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape
 
#2. 명목형 데이터를 더미변수화 합니다.

credit2 = pd.get_dummies( credit, drop_first=True )
credit2.info()

#3. 훈련 데이터와 테스트 데이터로 분리합니다.

x = credit2.iloc[ : , 0:-1].to_numpy()
y = credit2['default_yes'].to_numpy()

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.25, random_state=1,stratify=y  )

print( x_train.shape ) # (750, 35)
print( x_test.shape) # (250, 35)

#4. y_train 정답컬럼 데이터의 비율을 출력하시오 !

np.bincount( y_train) / 750

array([0.69866667, 0.30133333])

#5. y_test 정답컬럼 데이터의 비율을 출력하시오 !

np.bincount( y_test) / 250


▩ 교차검증  (p470)

그림설명


예제1. 독일은행 데이터에 대해서 교차검정해서 카파통계량 출력하기

#1. 데이터를 로드합니다.
credit <-  read.csv("credit.csv", stringsAsFactors=TRUE)

#2. 훈련 데이터 전체에 대해서 10개의 교차검증을 위한 행번호를 각각 생성합니다.
folds <- createFolds( credit$default, k=10)
str(folds)

#3. 1번 폴드를 검정데이터로 하고 나머지를 훈련 데이터로 만드시오 

credit01_test <- credit[ folds$Fold01,   ]
credit01_train <- credit[ -folds$Fold01, ]
nrow( credit01_test )  # 100
nrow( credit01_train )  # 900

#4. 10개의 hold 를 교차검증해서 훈련 데이터에 대한 kappa 지수 10개를 출력하시오 !

install.packages("irr")
library(caret)
library(C50)
library(irr) 

credit <- read.csv("credit.csv", stringsAsFactors=TRUE)
folds <- createFolds( credit$default,  k=10)   # 10개의 교차검정할 행번호를 만든다. 

cv_results <- lapply( folds, function(x) { 
  credit_train <- credit[-x, ]
  credit_test   <- credit[x, ]  
  credit_model <- C5.0(default ~. , data= credit_train)
  credit_pred <-  predict( credit_model,  credit_test )
  credit_actual <-  credit_test$default
  kappa <- kappa2( data.frame( credit_actual, credit_pred) )$value
  return (kappa) 
} )

str(cv_results)

mean( unlist( cv_results )  )  # 10개의 카파통계량의 평균값 출력 # 0.3264546

#5. 위에는 trials =1 에 대한 카파지수이고 이번에는 trials=100 에 대한 카파지수의 
     평균값을 출력하시오 

예제2.  위의 교차검정의 결과인 카파통계량을 정확도로 출력되게하시오 

#install.packages("irr")
library(caret)
library(C50)
library(irr) 

credit <- read.csv("credit.csv", stringsAsFactors=TRUE)
folds <- createFolds( credit$default,  k=10)   # 10개의 교차검정할 행번호를 만든다. 

cv_results <- lapply( folds, function(x) { 
  credit_train <- credit[-x, ]
  credit_test   <- credit[x, ]  
  credit_model <- C5.0(default ~. , data= credit_train, trials=100)
  credit_pred <-  predict( credit_model,  credit_test )
  credit_actual <-  credit_test$default

  x <- data.frame(credit_actual, credit_pred)
  a <- sum( x$credit_actual==x$credit_pred) / length(credit_actual)

  return (a) 
} )

str(cv_results)

mean( unlist( cv_results )  ) 

설명: trials =1 의 정확도: 0.723
       trials = 100 의 정확도 : 0.748

예제3.  위의 실험결과로 trials=100 이 가장 좋은 하이퍼파라미터였다는것을 알아냈으면
          이 하이퍼 파라미터를 써서 다시 모델을 생성합니다. 

#1. 데이터를 로드합니다.
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

#2. 훈련 데이터와 테스트 데이터로 분리합니다.
library(caret)
in_train <- createDataPartition( credit$default, p=0.75, list=FALSE)

x_train <- credit[ -in_train, ]
x_test  <- credit[  in_train,  ]

#3. 모델을 생성합니다.
library(C50)

model <- C5.0( default ~ . ,data=x_train, trials=100)

#4. 테스트 데이터를 예측합니다.
result <- predict( model, x_test)

#5. 모델의 성능을 확인합니다. 
sum( x_test$default == result ) /length(x_test$default)

▩ 파이썬으로 k-holdout 을 구현하기 

* 기본예제

#1. 데이터를 로드합니다.
from  sklearn import  datasets
iris = datasets.load_iris()
iris['data']  # 훈련 데이터
iris['target'] # 정답 데이터 

#2. R 에서의 k-holdout 기능을 구현합니다.
from  sklearn.model_selection import  GridSearchCV

parameters = {'max_depth': [1,2,3,4,5], 'random_state':[1, 5, 10] }

#3. 모델을 생성합니다.
from  sklearn.tree  import  DecisionTreeClassifier 
model = DecisionTreeClassifier()

#4. k-holdout 의 k 값을 10으로 주고 모델을 훈련 시킵니다.

gmodel = GridSearchCV( model , parameters, cv=10, verbose=2, n_jobs=-1 )
gmodel.fit( iris.data ,  iris.target )

#5. 10 holdout 으로 훈련해서 알아낸 최적의 하이퍼 파라미터가 무엇인지 출력하시오 
print( gmodel.best_params_)

{'max_depth': 3, 'random_state': 1}

문제347.  독일 은행 데이터(credit.csv) 에 대한 의사결정트리 모델을 생성하기 위해
             최적의 하이퍼 파라미터가 무엇인지 알아내시오 !

max_depth :   ?
random_state :  ? 

답:

#1. 데이터를 불러옵니다.
import  pandas  as  pd
credit = pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape
 
#2. 명목형 데이터를 더미변수화 합니다.

credit2 = pd.get_dummies( credit, drop_first=True )
credit2.info()

#3. 훈련 데이터와 테스트 데이터로 분리합니다.

x = credit2.iloc[ : , 0:-1].to_numpy()
y = credit2['default_yes'].to_numpy()

#2. R 에서의 k-holdout 기능을 구현합니다.
from  sklearn.model_selection import  GridSearchCV

parameters = {'max_depth': [1,2,3,4,5], 'random_state':[1, 5, 10] }

#3. 모델을 생성합니다.
from  sklearn.tree  import  DecisionTreeClassifier 
model = DecisionTreeClassifier()

#4. k-holdout 의 k 값을 10으로 주고 모델을 훈련 시킵니다.

gmodel = GridSearchCV( model , parameters, cv=10, verbose=2, n_jobs=-1 )
gmodel.fit( x ,  y )

#5. 10 holdout 으로 훈련해서 알아낸 최적의 하이퍼 파라미터가 무엇인지 출력하시오 
print( gmodel.best_params_)

{'max_depth': 3, 'random_state': 1}

케글에 상위권에 도전하고 싶다면 테스트 데이터 전체를 사용해서 최적의 하이퍼 파라미터를 
알아내야합니다.

■ 11장.  모델 성능 개선 

 11장의 소목차:

  1. k-holdout
  2. caret 을 이용한 모델 자동튜닝
  3. 앙상블
  4. 배깅
  5. 부스팅

▩ caret 을 이용한 모델 자동 튜닝 

 이전에는 머신러닝 모델의 성능을 높이기 위해서 우리가 직접 모델의 성능을 높이는
 하이퍼파라미터(예: C50 의 trials) 를 직접 알아내야 했다. 
 그런데 caret 을 이용하면 이 패키지가 알아서 최적의 파라미터를 찾아줍니다. 

 알아내야할 최적의 하이퍼 파라미터 (책 483 의 표)

 의사결정트리의 튜닝은 model, trials, winnow 이 3가지 하이퍼파라미터의 조합으로
 튜닝을 하는데 이 조합의 갯수가 3의 3승인 27개의 조합으로 정확도를 각각 계산해서
 가장 정확도가 높은 조합을 찾아내는것을 caret 패키지가 자동으로 해줍니다. 

실습:
library(caret)
library(C50)
library(irr)

credit <- read.csv("credit.csv", stringsAsFactors=TRUE)
set.seed(300)

m <- train( default ~ . , data=credit, method="C5.0")

m # 튜닝된 결과를 확인할 수 있다. 

#The final values used for the model were trials = 20, model = tree and
# winnow = FALSE

result <-  predict( m, credit )
table( result, credit$default)

result  no  yes
   no  700   2
   yes   0   298

2개 틀리고 다 맞췄습니다. 

문제348.  유방암 데이터(wisc_bc_data.csv) 의 악성종양과 양성종양을 분류하는 의사결정트리
             모델을 만들고 정확도를 확인하는데 caret 의 자동튜닝 기능을 이용해서 수행하시오 !

library(caret)
library(C50)
library(irr)

wisc <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)
wisc2 <- wisc[ , -1] 

set.seed(300)

m <- train( diagnosis ~ . , data=wisc2, method="C5.0")

m # 튜닝된 결과를 확인할 수 있다. 

result <-  predict( m, wisc2[ , -1]  )
table( result, wisc2$diagnosis)


문제349.  위의 스크립트는 훈련 데이터를 통채로 다 써서 최적의 하이퍼 파라미터를 알아내고
             훈련 데이터를 통채로 다 넣어서 정확도를 확인했다면 이번에는 훈련 데이터 90%, 
             테스트 데이터 10% 로 나눠서 90% 의 훈련데이터로 자동 모델 튜닝하고 10% 테스트 데이터를
             예측하고 정확도를 확인하시오!

답글로 올려주세요 ~~~~~~

wisc <- read.csv("wisc_bc_data.csv", stringsAsFactors=T)

in_train <-  createDataPartition( wisc$diagnosis, p=0.9, list=F)

train_data <- wisc[ in_train, ]
test_data <- wisc[ -in_train, ]

set.seed(300)
m <- train( diagnosis ~ . , data=train_data[ , -1] , method="C5.0")
result <- predict( m,  test_data[  , c(-1,-2) ] )
table( result, test_data$diagnosis)

▩ 파이썬으로 하이퍼 파라미터 자동튜닝 기능을 구현하는 방법 

#1. 데이터를 불러옵니다.
import  pandas  as  pd
credit = pd.read_csv("d:\\data\\credit.csv")
credit.head()
credit.shape
 
#2. 명목형 데이터를 더미변수화 합니다.

credit2 = pd.get_dummies( credit, drop_first=True )
credit2.info()

#3. 훈련 데이터와 테스트 데이터로 분리합니다.

x = credit2.iloc[ : , 0:-1].to_numpy()
y = credit2['default_yes'].to_numpy()

# 4. train_test_split 를 이용해서 훈련 데이터와 테스트 데이터를 9 대 1로 나눕니다.
from  sklearn.model_selection import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1, stratify=y)

#5. R 에서의 k-holdout 기능을 구현합니다.
from  sklearn.model_selection import  GridSearchCV

parameters = {'max_depth': [1,2,3,4,5,6,7,8,9,10], 'random_state':[1,2,3,4,5] }

#6. 모델을 생성합니다.
from  sklearn.tree  import  DecisionTreeClassifier 
model = DecisionTreeClassifier()

#7. k-holdout 의 k 값을 10으로 주고 모델을 훈련 시킵니다.

gmodel = GridSearchCV( model , parameters, cv=10, verbose=2, n_jobs=-1 )
gmodel.fit( x_train ,  y_train )

#8. best 하이퍼 파라미터를 조회합니다.
print( gmodel.best_params_)

{'max_depth': 5, 'random_state': 1}

#9. 최적의 하이퍼 파라미터로 테스트 데이터를 예측합니다.
result = gmodel.predict(x_test)

#10. 정확도를 확인합니다.
sum( result ==y_test) /len(y_test)

문제350. 아래의 자동튜닝하는 코드에 독버섯 데이터를 적용해서 독버섯 데이터를 분류하는 
            나이브베이즈 모델의 정확도를 확인하시오 ! (데이터셋: mushrooms.csv)  
 
책 483 페이지 표를 참고해서 method 가 C5.0 이 아니라 nb 를 사용해서 자동튜닝하세요

library(caret)
library(e1071)  
library(irr)
mush <- read.csv("mushrooms.csv", stringsAsFactors = T)
str(mush)
set.seed(300)
train_num <- createDataPartition(mush$type, p=0.9, list=F)

train_data <- mush[train_num,-1]
test_data <- mush[-train_num,-1]

train_label <- mush[train_num, 'type']
test_label <- mush[-train_num, 'type']
set.seed(300)
m <- train(train_data, train_label, method='nb')
m
result <- predict(m, test_data)
table(result, test_label)

문제351. (오늘의 마지막 문제) 유방암 데이터를 분류하는 머신러닝 모델을 자동튜닝했을때
            가장 좋은 머신러닝 모델이 무엇인지 책 483페이지의 표에 나온데로 method를 변경해가면서
            확인하시오 !    5시 신호 보냈습니다. ~~  마지막 문제 답 올리시고 자유롭게 자습또는 스터디하세요
                                 6시 신보 보냈습니다. ~~

메서드 :  knn, nnet, C5.0, OneR, JRip,  rf

library(caret)
library(C50)
library(irr)

#1. 데이터를 로드
wisc<-read.csv("wisc_bc_data.csv", stringsAsFactors=T)
wisc2<-wisc[ ,-1]  # 필요없는 컬럼 제거

#2. 훈련데이터와 테스트데이터 분리 
set.seed(1)
in_train<-createDataPartition(wisc2$diagnosis, p=0.8, list=F)
train_data<-wisc2[in_train, ]
test_data<-wisc2[-in_train, ]


#3. 모델을 생성
set.seed(1)
m<-train(diagnosis~. , data=train_data, method="knn")  
m

#4. 모델예측
result<-predict(m, test_data[ ,-1])


#5. 정확도
table(result, test_data$diagnosis)
sum(result==test_data[ ,1])/length(test_data[ ,1])

 
■ 머신러닝의 성능을 더 높이기 위해 어제 마지막 문제의 코드를 커스터마이징 하는 방법
    (책 491 페이지)

여러개의 기계학습 모델중에 가장 좋은 하나를 선택하려하는 코드

                        +

     k-holdout 교차검정 기법 

* 의사결정트리 모델의 정확도 더 높이기 위해 좋은 하이퍼 파라미터 찾는 방법

library(caret)
library(C50)
library(irr)

#1. 데이터를 로드
wisc<-read.csv("wisc_bc_data.csv", stringsAsFactors=T)
wisc2<-wisc[ ,-1]  # 필요없는 컬럼 제거

#2. 훈련데이터와 테스트데이터 분리 
set.seed(1)
in_train<-createDataPartition(wisc2$diagnosis, p=0.8, list=F)
train_data<-wisc2[in_train, ]
test_data<-wisc2[-in_train, ]

#3. 모델을 생성
set.seed(1)

ctrl <- trainControl( method="cv", number=10, selectionFunction="oneSE")

# 교차검정 10 폴드아웃으로 진행하겠다
# oneSE 는 최고의 성능의 1표준오차내의 가장 단순한 후보를 선택한다.

m<-train(diagnosis~. , data=train_data,  trControl=ctrl, method="C5.0")  
m

#4. 모델예측
result<-predict(m, test_data[ ,-1])

#5. 정확도
table(result, test_data$diagnosis)
sum(result==test_data[ ,1])/length(test_data[ ,1])

설명 :  foldout 교차검정값을 10에서 20으로 변경했더니 정확도가 
          0.95 ----> 0.96으로 상승했습니다. 

문제352. 이번에는 C5.0 말고 knn 으로 해서  교차검정값을 10에서 20으로 늘리면
            정확도의 변화가 생기는지 확인하시오 !

정확도가 10 fold 교차검정일때는 0.96 이었는데
             20 fold 교차검정일때는 0.97로 상승되었습니다. 

▩ 앙상블  (p495)

  * 11장의 소목차
  1. k-holdout
  2. caret 을 이용한 모델 자동튜닝
  3. 앙상블 
  4. 배깅
  5. 부스팅 

* 앙상블이란 ?  다양한 전문가 팀을 만드는것과 유사한 원리를 활용하는 메타학습방법을
                     앙상블이라 합니다.
                     모든 앙상블 방법은 약한 학습자 여러개를 결합하면 강한 학습자가
                     만들어진다는 아이디어를 기반으로 합니다. 
                     앙상블 모형은 여러개의 분류모형을 같이 사용하여 한꺼번에 평가하는
                     모형을 말합니다. 

 실습1. 정확도가 60% 밖에 되지 않는 분류기 모형들이 즐비한데 이 모형들을
           최소한 몇개를 써야 정확도를 90% 를 능가하게 만들수 있을까 ?

ret_err <- function(n,err) {
  sum <- 0 

  for(i in floor(n/2):n) { 
    sum <- sum + choose(n,i) * err^i * (1-err)^(n-i)
  }
  sum
}
for(j in 1:60) {
  err <- ret_err(j , 0.4)
  cat(j,'--->',1-err,'\n') 
  if(1-err >= 0.9) break
}

약한 학습자들이 50개는 있어야 정확도가 90% 가 나옵니다. 


앙상블 그림 (R 수업 목차의 11장 )

 이론설명3. 앙상블 기법의 원리가 어떻게 되는가 ?


▩ 파이썬으로 앙상블 실험하기 기본예제1

import numpy as np
from sklearn.linear_model import LinearRegression  # 선형회귀 모델 모듈 임폴트
from sklearn.ensemble import RandomForestRegressor # 랜덤포레스트 모델 모듈 임폴트
from sklearn.ensemble import VotingRegressor  # 앙상블의 수치예측한 결과를 평균내는 모듈

r1 = LinearRegression()    #  선형회귀 모델 생성 
r2 = RandomForestRegressor(n_estimators=10, random_state=1) # 랜덤포레스트 모델 생성

X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])  # 훈련 데이터
y = np.array([2, 6, 12, 20, 30, 42])                               # 정답 데이터 

er = VotingRegressor([('lr', r1), ('rf', r2)])  # 선형회귀와 랜덤포레스트 모델을 결합한 강력한
                                                     # 앙상블 모델 er 을 생성함
print(er.fit(X, y).predict(X))  # 예측값 출력

import  numpy as  np
print ( np.corrcoef(y,result))

▩ 예제2. 보스톤 하우징 데이터의 집값을 예측하는 앙상블 모델 만들기 

# 예제2.1  보스톤 하우징의 선형회귀 모델의 상관계수값과 오버피팅 여부 확인 

# 1. 사이킷런의 보스톤 데이터를 로드합니다. 
from  sklearn import  datasets 
boston = datasets.load_boston()

x = boston.data
y = boston.target

#2. 훈련 데이터와 테스트 데이터로 분리합니다.(9대1)
from  sklearn.model_selection  import  train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)
print(x_train.shape) # (455, 13)
print(x_test.shape)  # (51, 13)

#3. 선형회귀 모델을 생성합니다.
from  sklearn.linear_model  import  LinearRegression

r1 = LinearRegression()
train_result1 = r1.fit(x_train, y_train).predict(x_train)  #훈련 데이터에 대한 예측
test_result1  = r1.fit(x_train, y_train).predict(x_test)   #테스트 데이터에 대한 예측

#4. 상관계수값을 확인합니다
print ( np.corrcoef( y_train, train_result1) )  #  0.85727551
print ( np.corrcoef( y_test,  test_result1) )   #  0.88901603

※ 오버피팅이 발생하지는 않았지만 성능이 좋지 않습니다. 

# 예제2.2  보스톤 하우징의 랜덤포레스트 모델의 상관계수값과 오버피팅 여부 확인 

 온라인 수업의 집중을 높이기 위해서 예제2.2 를 카페에 올려주세요 ~~

#1. sklearn의 보스톤 데이터 로드
from sklearn import datasets
boston=datasets.load_boston()
boston

x=boston.data
y=boston.target

#2. 훈련데이터와 테스트데이터로 분리(9:1)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1)
print(x_train.shape)   #(455,13)
print(x_test.shape)    #(51, 13)

#3. 랜덤포레스트 모델 생성
from sklearn.ensemble import RandomForestRegressor
r1=RandomForestRegressor()
train_result1=r1.fit(x_train, y_train).predict(x_train)   #훈련데이터에 대한 예측
test_result1=r1.fit(x_train, y_train).predict(x_test)     #테스트데이터에 대한 예측

#4. 상관계수값 확인
print(np.corrcoef(y_train, train_result1))   #0.992055
print(np.corrcoef(y_test, test_result1))    #0.970008

※ 설명: 회귀분석일때 보다는 정확도가 더 높아졌습니다. 그런데 회귀분석일 때와 마찬가지로
           훈련 데이터와 테스트 데이터의 성능차이가 발생하고 있습니다. 
           랜덤포레스트의 경우는 오버피팅이 발생하고 있습니다. 

#예제2.3. 회귀모형과 랜덤포레스트 모형을 결합해서 앙상블 모형을 만들고 수치예측을 하시오!

#1. sklearn의 보스톤 데이터 로드
from sklearn import datasets
boston=datasets.load_boston()
boston

x=boston.data
y=boston.target

#2. 훈련데이터와 테스트데이터로 분리(9:1)
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1)
print(x_train.shape)   #(455,13)
print(x_test.shape)    #(51, 13)

#3. 모델 2개를 결합한 앙상블 모형을 만들고 예측한다. 

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import VotingRegressor

r1 = LinearRegression()
r2 = RandomForestRegressor(n_estimators=10, random_state=1)

er = VotingRegressor([('lr', r1), ('rf', r2)])

train_result3=er.fit(x_train, y_train).predict(x_train)   #훈련데이터에 대한 예측
test_result3=er.fit(x_train, y_train).predict(x_test)     #테스트데이터에 대한 예측

#4. 상관계수값 확인
print(np.corrcoef(y_train, train_result3))  # 0.95853412
print(np.corrcoef(y_test, test_result3))    # 0.95623851

※ 설명:  앙상블 모형으로 만들었더니 훈련 데이터와 테스트 데이터간의 성능차이가
            비슷하게 나오고 있습니다. 

  여기서 상관계수값을 더 올리는것은 파생변수 추가, 10-hold 교차검정, 이상치 제거,
  결측치 대치를 통해서 더 올리면 됩니다.

▩ 앙상블 이용해서 유방암 데이터를 분류하는 모델 만들기 

 준비할 모형3가지 ?   1. 로지스틱 회귀 모형
                             2. 나이브베이즈 모형
                             3. 랜덤포레스트 모형 

예제1. 나이브 베이즈 모형을 이용해서 유방암 데이터 분류하기

#1. 데이터를 로드합니다.
from   sklearn  import  datasets
breast = datasets.load_breast_cancer()
x = breast.data
y = breast.target

#2. 훈련 데이터와 테스트 데이터를 분리합니다.
from  sklearn.model_selection  import  train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

#3. 나이브베이즈 모델을 생성합니다. 
from  sklearn.naive_bayes  import  GaussianNB

r1 = GaussianNB()

#4. 훈련 데이터와 테스트 데이터를 각각 예측합니다.

train_result = r1.fit( x_train, y_train).predict(x_train)
test_result =  r1.fit( x_train, y_train).predict(x_test )
test_result

#5. 훈련 데이터의 정확도와 테스트 데이터의 정확도를 확인합니다.
print( sum( train_result == y_train) / len(y_train) )  # 0.939453125
print( sum( test_result == y_test) / len(y_test) )  # 0.94736842105

※ 정확도는 0.93과 0.94로 정확도의 차이가 살짝 발생하고 있습니다. 

#예제2.  유방암 데이터를 로지스틱 회귀 모델로 생성하고 예측하기 

#1. 데이터를 로드합니다.
from   sklearn  import  datasets
breast = datasets.load_breast_cancer()
x = breast.data
y = breast.target

#2. 훈련 데이터와 테스트 데이터를 분리합니다.
from  sklearn.model_selection  import  train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

#3. 로지스틱 회귀 모델을 생성합니다. 
from  sklearn.linear_model  import  LogisticRegression

r2 = LogisticRegression()

#4. 훈련 데이터와 테스트 데이터를 각각 예측합니다.

train_result2 = r2.fit( x_train, y_train).predict(x_train)
test_result2 =  r2.fit( x_train, y_train).predict(x_test )
test_result2

#5. 훈련 데이터의 정확도와 테스트 데이터의 정확도를 확인합니다.
print( sum( train_result2 == y_train) / len(y_train) )  # 0.94921875
print( sum( test_result2 == y_test) / len(y_test) )  #  1.0

※ 로지스틱 회귀모델은 나이브 베이즈 보다 정확도 정확도는 더 높으나 정확도의 차이가 심합니다.

예제3.  유방암 데이터를 분류하는 머신러닝 모델을 랜덤포레스트 모델로 생성하고 분류하시오 !

#1. 데이터를 로드합니다.
from   sklearn  import  datasets
breast = datasets.load_breast_cancer()
x = breast.data
y = breast.target

#2. 훈련 데이터와 테스트 데이터를 분리합니다.
from  sklearn.model_selection  import  train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

#3. 랜덤포레스트  모델을 생성합니다. 
from  sklearn.ensemble  import  RandomForestClassifier 

r3 = RandomForestClassifier()

#4. 훈련 데이터와 테스트 데이터를 각각 예측합니다.

train_result3 = r3.fit( x_train, y_train).predict(x_train)
test_result3 =  r3.fit( x_train, y_train).predict(x_test )
test_result3

#5. 훈련 데이터의 정확도와 테스트 데이터의 정확도를 확인합니다.
print( sum( train_result3 == y_train) / len(y_train) )  # 1.0
print( sum( test_result3 == y_test) / len(y_test) )  #  0.98

설명:  랜덤포레스트 모델은 정확도 아주 높으나 오버피팅이 발생했습니다. 

예제4. (점심시간 문제) 위의 3개의 모델을 결합한 강력한 앙상블 모델을 만드시오 !

■변경해야할것1 :
from sklearn.ensemble import VotingRegressor  ( 수치예측 )
                     ↓ 
from sklearn.ensemble import VotingClassifier    ( 분류 )

■변경해야할것2: 
er = VotingRegressor([('lr', r1), ('rf', r2)])
                      ↓ 
eclf1 = VotingClassifier(estimators=[ ('gnb', r1), ('lr', r2), ('rf', r3)], voting='hard')


앙상블을 사용하게 되면 정확도를 더 높일수 있고 특히 오버피팅을 현저하게 줄일수 있습니다. 

▩ 앙상블 모형의 정확도를 더 올리기 위해 해야할 일들 

 1.  표준화 또는 정규화 작업 수행하기
 2.  하이퍼 파라미터 조정하기

▩ 점심시간 문제에 정확도를 더 높이기 위해서 표준화 또는 정규화 작업 수행하기 

#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

# 정규화를 진행합니다.
from   sklearn.preprocessing  import  MinMaxScaler 

scaler = MinMaxScaler()
scaler.fit(x)
x_scaled = scaler.transform(x)

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x_scaled, y, test_size=0.1, random_state=1)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류

r1=GaussianNB()
r2=LogisticRegression()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r1), ('lr', r2), ('rf', r3) ], voting='hard')

#4. 훈련데이터, 테스트데이터 예측
train_result=eclf1.fit(x_train, y_train).predict(x_train)
test_result=eclf1.fit(x_train, y_train).predict(x_test)


#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    #0.97
print(sum(test_result==y_test)/len(y_test))     #0.98


▩  사이킷런의 make_pipeline 함수를 이용해서 표준화 또는 정규화 하기 

#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류
from sklearn.pipeline  import   make_pipeline 
from sklearn.preprocessing  import  MinMaxScaler

r1=GaussianNB()
r2=LogisticRegression()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r1), ('lr', r2), ('rf', r3) ], voting='hard')

pipeline = make_pipeline( MinMaxScaler() , eclf1 )

#4. 훈련데이터, 테스트데이터 예측
train_result=pipeline.fit(x_train, y_train).predict(x_train)
test_result=pipeline.fit(x_train, y_train).predict(x_test)

#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    #0.97
print(sum(test_result==y_test)/len(y_test))     #0.98

예제1.  위의 예제는 pipeline 을 사용해서 정규화를 했는데 표준화를 하겠금 코드를 수정하고 
          수행하시오


#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1, stratify=y)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류
from sklearn.pipeline  import   make_pipeline 
from sklearn.preprocessing  import  MinMaxScaler, StandardScaler

r1=GaussianNB()
r2=LogisticRegression()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r1), ('lr', r2), ('rf', r3) ], voting='hard')

pipeline = make_pipeline( StandardScaler() , eclf1 )  # 모델과 표준화를 한묶음으로 묶어서
                                                                      # 새로운 모델을 생성 
#4. 훈련데이터, 테스트데이터 예측
train_result=pipeline.fit(x_train, y_train).predict(x_train)
test_result=pipeline.fit(x_train, y_train).predict(x_test)

#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    #0.97
print(sum(test_result==y_test)/len(y_test))     #0.98


▩ 앙상블 모형의 정확도를 더 올리기 위해 해야할 일들 

 1.  표준화 또는 정규화 작업 수행하기
 2.  하이퍼 파라미터 조정하기

▩ 랜덤포레스트의 최적의 하이퍼 파라미터 알아내는 방법

#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1, stratify=y)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류
from sklearn.pipeline  import   make_pipeline 
from sklearn.preprocessing  import  MinMaxScaler, StandardScaler
from sklearn.model_selection  import  StratifiedKFold

# k홀드 교차검정을 10으로 설정 
cv= StratifiedKFold(n_splits=10)  # 10개의 fold 를 만들어서 교차검정하겠다.
kfold = cv.split( x_train, y_train)  # 10개의 fold 의 훈련데이터와 검정데이터를 kfold 에 넣겠다.

# 모델생성 
r3=RandomForestClassifier(n_estimators=10, random_state=1)
pipeline = make_pipeline( StandardScaler() , r3 ) 
                                                                      
#4. 훈련데이터, 테스트데이터 예측
score2 =[]
for k, (train, test)  in  enumerate(kfold):
   # print(k, train, test) 
   pipeline.fit(x_train[train, : ] , y_train[train] )   #10 fold  데이터셋을 훈련 시켜서
   score = pipeline.score(x_train[test, : ], y_train[test] ) # 검정 데이터 10개에 대한 정확도가 
   score2.append(score)   # score2 에 append 된다. 

print(np.mean(score2))  # 0.9628959276018099

                                정확도 평균 
n_estimators=10, random_state=1---> 0.951131221719457
n_estimators=50 , random_state=1---> 0.9570135746606334
n_estimators=100 , random_state=1 --> 0.9628582202111614
n_estimators=150 , random_state=1 -->  0.9647812971342382

※  pipeline 에 standardScaler 와 kfold 을  같이 사용했는데 kfold 를 standardScaler 와
    효율적으로 사용하려고 pipeline 을 사용한것입니다.


#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류

r1=GaussianNB()
r2=LogisticRegression()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r1), ('lr', r2), ('rf', r3) ], voting='hard')

#4. 훈련데이터, 테스트데이터 예측
train_result=eclf1.fit(x_train, y_train).predict(x_train)
test_result=eclf1.fit(x_train, y_train).predict(x_test)


#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    #0.97
print(sum(test_result==y_test)/len(y_test))     #0.98

문제353.  점심시간 문제 코드에 모델 3가지중에 하나를 신경망으로 변경하면
             결과가 나오는지 확인하시오 !

나이브베이즈 -----------> 신경망 으로 변경 

점심시간 문제 코드:
#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x, y, test_size=0.1, random_state=1)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network  import  MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류

r0= GaussianNB()
r1=MLPClassifier()
r2=LogisticRegression()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r0), ('mlp', r1), ('lr', r2), ('rf', r3) ], voting='hard')

#4. 훈련데이터, 테스트데이터 예측
train_result=eclf1.fit(x_train, y_train).predict(x_train)
test_result=eclf1.fit(x_train, y_train).predict(x_test)


#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    #0.97
print(sum(test_result==y_test)/len(y_test))     #0.98

40분까지 쉬시고 40분에 마지막문제 내주겠습니다. 

문제354.(오늘의 마지막 문제)  사이킷런의 아이리스 데이터로 앙상블 모델을 만들어서
           분류하시오 ~ 

from sklearn import datasets
iris=datasets.load_iris()
x=iris.data
y=iris.target

마지막 문제 올리시고 자유롭게 자습 또는 스터디 하세요 ~

#5. 정확도 확인
#print(sum(train_result==y_train)/len(y_train))    #1.0
#print(sum(test_result==y_test)/len(y_test))      #0.92

■ 11장. 머신러닝 성능 개선 목차 

1. 앙상블 : 앙상블 이론설명 --> 코드 구현 ---> 앙상블 성능 개선 
2. 배깅
3. 부스팅 

■ 사이킷런의 앙상블 구현 코드에서 voting='hard' 와 voting='soft' 의 의미 ?

게시글 2006번.  voting='soft' 와 voting='hard' 설명 그림


■ 앙상블에서 최적의 하이퍼파라미터를 찾아서 앙상블의 성능을 높이는 방법

게시글 2007. 앙상블 최적의 하이퍼 파라미터 찾기 코드 

코드설명: 유방암 데이터의 악성과 양성의 분류를 앙상블 기법을 써서 분류하는 코드

   앙상블의 약한 학습자를 3개를 사용하고 있고 각각 knn,  random forest, 로지스틱 회귀를
   이용하고 있고 유방암에 대한 각각의 최적의 하이퍼 파라미터를 찾아서 그걸로 
   강력한 앙상블 코드로 voting='hard' 로 분류하는 코드 입니다.

예제:
#1. 사이킷런의 내장되어 있는 유방암 데이터를 로드 합니다. 
from sklearn import datasets
breast = datasets.load_breast_cancer()

x =breast.data    #  훈련 데이텃셋
y =breast.target  #  정답 데이터셋,  1이 악성이고 0 이 양성입니다. 

#2. 데이터를 훈련과 테스트로 9대1로 분리합니다. 
from sklearn.model_selection import train_test_split
#split data into train and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, stratify=y)
print(x_train.shape) # (512, 30)
print(x_test.shape)  # ( 57, 30) 

#3. knn 약한학습자의 최적의 하이퍼 파라미터 알아내기
import numpy as np
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

#모델을 생성 
knn = KNeighborsClassifier()

#knn 의 gridsearch 를 위한 k값의 범위 지정
params_knn = {'n_neighbors': np.arange(1, 25)}

#k-holdout 의 k 값을 5로 주고 최적의 knn 하이퍼파라미터를 찾는 모델 생성
knn_gs = GridSearchCV(knn, params_knn, cv=5)

# 그리드 서치 모델 훈련 
knn_gs.fit(x_train, y_train)

#4. knn 약학 학습자의 최적의 하이퍼 파라미터 보기 
#save best model
knn_best = knn_gs.best_estimator_

#check best n_neigbors value
print(knn_gs.best_params_)  #  k 값 12 입니다. 

# 5. 두번째 약한 학습자인 랜덤포레스트의 하이퍼 파라미터 찾는 코드 구현 
from sklearn.ensemble import RandomForestClassifier
#create a new random forest classifier
rf = RandomForestClassifier()
#create a dictionary of all values we want to test for n_estimators
params_rf = {'n_estimators': [50, 100, 200]} # 배깅의 샘플링 갯수
#use gridsearch to test all values for n_estimators
rf_gs = GridSearchCV(rf, params_rf, cv=5)
#fit model to training data
rf_gs.fit(x_train, y_train)

#6. 랜덤포레스트의 최적의 하이퍼 파라미터 보기 
#save best model
rf_best = rf_gs.best_estimator_  # 최적의 하이퍼 파라미터를 변수에 저장
#check best n_estimators value
print(rf_gs.best_params_) # n_estimators 가 100 

#7. 로지스틱 회귀 모델은 gridsearch 하지 않고 그냥 모델 생성함 
from sklearn.linear_model import LogisticRegression
#create a new logistic regression model
log_reg = LogisticRegression()
#fit the model to the training data
log_reg.fit(x_train, y_train)

#8. 위의 3개의 모델의 정확도(성능)을 확인합니다. 
#test the three models with the test data and print their accuracy scores
print('knn: {}'.format(knn_best.score(x_test, y_test)))
print('rf: {}'.format(rf_best.score(x_test, y_test)))
print('log_reg: {}'.format(log_reg.score(x_test, y_test)))

knn: 0.8947368421052632
rf: 0.9649122807017544
log_reg: 0.8947368421052632

#9. 위의 3개의 모형을 앙상블 모델로 구현 

from sklearn.ensemble import VotingClassifier
#create a dictionary of our models
estimators=[('knn', knn_best), ('rf', rf_best), ('log_reg', log_reg)]
#create our voting classifier, inputting our models
ensemble = VotingClassifier(estimators, voting='hard')

#10. 앙상블 모델을 훈련 시키고 정확도 확인 
#훈련데이터, 테스트데이터 예측
train_result=ensemble.fit(x_train, y_train).predict(x_train)
test_result=ensemble.fit(x_train, y_train).predict(x_test)

#정확도 확인
print(sum(train_result==y_train)/len(y_train))     # 0.97
print(sum(test_result==y_test)/len(y_test))        #0.89

설명: 각각의 약한 학습자들에 대해서 최적의 하이퍼 파라미터를 찾아서 앙상블 모형을
       구현했고 구현 결과는 훈련 데이터의 정확도는 0.97 이고 테스트 데이터의 정확도는
        0.89 입니다.

■ 11장 수업 목차

1. 앙상블
2. 배깅 
3. 부스팅 

■ 배깅과 부스팅 

* 배깅(bagging) ?  

 그림 


설명:  학습 데이터를 랜덤으로 샘플링하여 여러개의 bag 에 분할하고 각 bag 별로 모델을 
        학습한 후 ,각 결과를 합하여 최종결과를 추출한다.
        이때 데이터는 복원 추출을 하고 모델은 하나만 사용합니다. 일반적으로 의사결정트리
        모델을 사용합니다. 이렇게 약한 학습자들을 앙상블 기법으로 학습하고 그 결과를
        평균을 내게 되면 전체적으로 높은 분산(모델들이 예측한 정확도)을 낮은 분산으로 
        만들고 편차(정확도와 예측값과의 거리)도 줄일 수 있습니다. 

▩ 배깅 실습 (독일은행 데이터의 채무 불이행자를 예측하는 모델 만들기)

 아래의 2개의 모델을 비교하는 실습 입니다. 

1. 하나의 의사결정트리 모델로 구현
2. 의사결정트리 + 배깅 모델로 구현 

▩ 하나의 의사결정트리 모델로 구현 

#1. 데이터 로드
import pandas  as pd
credit = pd.read_csv("d:\\data\\credit.csv")
credit.head()

#2. 명목형 데이터를 더미변수화 한다.
credit2 = pd.get_dummies(credit.iloc[ : , : -1] )  # 정답전까지 더미화
credit2.head() 

#3. 훈련 데이터와 테스트 데이터로 분리합니다. 
x = credit2.to_numpy()
y = credit.iloc[ : , -1].to_numpy()

from sklearn.model_selection  import  train_test_split
x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

#4. 정규화 작업 수행
from  sklearn.preprocessing  import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(x_train)
x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

#5. 의사결정트리 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier
model1 = DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=1)

#6. 모델훈련
model1.fit( x_train2, y_train)

#7. 모델예측
result1 = model1.predict( x_test2 )

#8. 모델평가 
print( sum( result1 == y_test) / len(y_test)  )  # 0.72

▩ 의사결정트리 + 배깅 모델로 구현 

#앞에 데이터 불러오는 부분은 이미 했으므로 그대로 두고 모델 생성부터 시작합니다.
#모델생성 
from  sklearn.tree  import  DecisionTreeClassifier
model2 = DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=1)

from  sklearn.ensemble  import  BaggingClassifier
bagging2 = BaggingClassifier( model2, max_samples=0.9, max_features=0.5, random_state=1)

#설명: max_samples=0.9 는 bag 에 데이터 담을때 훈련 데이터의 90% 를 샘플링하겠다.
#       max_features= 0.5  하나의 예측기가 가져갈수 있는 최대 컬럼의 갯수를 50% 만 쓰겠다

# 모델훈련 
bagging2.fit( x_train2, y_train)

#모델예측
result2 = bagging2.predict(x_test2)

#모델평가 
print( sum( result2 == y_test) / len(y_test) ) # 0.74 

문제355. 의사결정트리 나무 모델과 배깅 모델의 random_state 를 1 에서 0으로 바꾸고 다시 
            실험하시오 ~

의사결정트리의 정확도 : 0.65
배깅의 정확도 : 0.75

문제356. 사이킷런의 배깅 메뉴얼을 참고해서 위의 배깅 모델의 정확도를 더 올리시오 !
            
게시글 2008번  사이킷런 메뉴얼

n_estimators : 앙상블 모델들의 갯수 : 기본값이 10 입니다.
max_samples :  데이터 샘플링을 얼마나 할지 : 기본값은 1.0 입니다.
max_features : 컬럼들을 얼마나 샘플링할지 : 기본값은 1.0 입니다. 

max_features 를 0.6 으로 올리세요 ~ 정확도가 0.75 에서 0.79 로 올라갔습니다.

▩ 랜덤포레스트(Random Forest) 란 ?

   의사결정트리와 배깅을 결합한 머신러닝 모델을 말합니다. 

예제:
#모델생성 
from  sklearn.ensemble  import  RandomForestClassifier 
model3 = RandomForestClassifier(n_estimators=10, max_depth=20, min_samples_split=2,
                                             random_state=1)

#설명:  n_estimators=10 은 의사결정트리 모델의 갯수를 10개로 하겠다.
          max_depth=20  은 의사결정트리의 가지치기했을때의 깊이
          max_samples_split  노드를 분할 하기 위한 최소한의 샘플 데이터 수 : 기본값2
           작게 설정하게 되면 분할 노드가 많아져서 오버피팅 가능성이 증가됨 

#모델훈련 
model3.fit( x_train2, y_train)

#모델예측
result3 = model3.predict(x_test2)

#모델평가
print( sum(result3==y_test ) / len(y_test) ) # 0.72

랜덤포레스트의 하이퍼 파라미터는 n_estimators 입니다. 
n_estimators 를 10에서 30으로 줬더니 0.76까지 올라갓습니다. 

문제357. 위의 랜덤포레스트 모델의 파라미터를 아래와 같이 설정하면 정확도 더 올라가는지
            확인하시오 !

n_estimators=30
max_depth= 30
min_samples_split=2

0.78  로 정확도가 개선되었습니다. 

▩  더욱 랜던함 랜덤포레스트 :  익스트림 랜덤 트리 

   의사결정트리 ---> 랜덤포레스트 ----> 익스트림 랜덤 트리 
                                 ↓                           ↓
                        의사결정트리 + 배깅     랜덤포레스트 모델의 변종으로 랜덤트리 또는
                                                        엑스트라 트리라고 합니다. 

  익스트림 트리모델은 후보 특성을 무작위로 분할하는 식으로 무작위성을 증가 시켰습니다. 

from  sklearn.ensemble  import  ExtraTreesClassifier 
model4 = ExtraTreesClassifier(n_estimators=10, max_depth=40, min_samples_split=5,
                                       random_state=1)

model4.fit(x_train2, y_train)
result4 = model4.predict(x_test2)
print( sum( result4==y_test) / len(y_test) ) 

문제358. (점심시간 문제) 
            방금 수행한 ExtraTreesClassifier 의 정확도를 출력하는 전체코드를 답글로 올리시오


 답글로 올리고 식사하세요

train_test_split 의 random_state =1 로 하고
ExtraTreesClassifier 의 random_state = 1로 하면 0.81 까지 올라갑니다. 

▩ 11장 목차

1. 앙상블
2. 배깅
3. 부스팅 

▩ 부스팅(boosting) (p500)

 그림 

boost 의 뜻은 격려하다, 향상시키다를 뜻으로 
부스팅은 배깅을 약간 개선시키니 알고리즘인데 샘플링하는 과정에서 복원 추출할때
동일한 확률로 하는게 아니라 추출할 때 마다 확률을 서로 다르게 개선시키는 방법을
쓴다.  처음에는 동일한 확률로 복원 추출을 하지만 다음번 추출과정에서는 오분류된
데이터를 추출확률이 더 높도록 조정하고 올바르게 분류된 데이터는 추출확률을 낮게
조정하여 복원추출합니다.  이런 작업을 정해진 단계의 수만큼 반복적으로 사용한다.

배깅과의 차이점은 배깅은 병렬적으로 모델을 만들비만 부스팅은 하나의 모델을 만들어
그 결과로 다른 모델을 만들어 나가는 순차적 모델을 완성시켜 나갑니다. 
가중치에 대해서도 배깅은 1/n 으로 가중치를 주지만 부스팅은 오차가 더 큰 객체에 대해
더 높은 가중치를 부여합니다. 

기존 부스팅:
                         모델1            모델2                 모델3
 암환자 : 정상환자 = 1 : 1   ---->     2  : 1   ------>    1 : 2  -----------> 최종분류기
                                       ↑                  ↑
             암환자를 분류하지 못했다면   정상환자를 못분류했다면 

Adaboosting :
                        모델1          모델2             모델3
암환자 : 정상환자 = 1 : 1 ---->  3:1 -----------> 1:4 -------------------> 최종분류기 

정확도를 높여가는식으로 학습해나간다.

* 부스팅 기법 
1. Adaboosting : 개별 분류기에 대해서 서로 다른 가중치를 주어서 최종 분류기를 만들어냅니다.
2. GradientBoosting : 손실함수의 오차를 최소화하는 방향으로 학습해 나갑니다. 
3. XGBoosting  : GradientBoosting를 개선한것으로 GradientBoosting와 마찬가지로
                      손실함수의 오차를 최소화하면서 모형의 복잡도까지 고려하는 기법입니다. 
                      그래서 하이퍼 파라미터가 아주 많습니다. 그래서 사람이 해야할 일들이 
                       많습니다. 

실습:  
배깅 실험할때 처음 사용했던 의사결정트리 모델 생성하는 코드를 일단 가져와서 실행합니다.

#▩ 판다스로 독일 은행의 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")

# 2. 데이터 탐색(명목형 데이터)
credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )

# 3. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

# 4. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 5. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

# 6. 모델 생성
from  sklearn.tree  import  DecisionTreeClassifier
model1 = DecisionTreeClassifier(criterion='entropy', max_depth=20, random_state=1)

# 7. 모델 훈련

model1.fit( x_train2,  y_train)

# 8. 모델 예측
result1 = model1.predict( x_test2 )

# 9. 모델 평가

sum( result1 == y_test ) / len(y_test) # 0.72

#10. Adaboosting 구현
from  sklearn.ensemble  import  AdaBoostClassifier
model5 = AdaBoostClassifier(n_estimators=100, random_state=1)

#모델훈련
model5.fit(x_train2, y_train)
#모델예측
result5 = model5.predict(x_test2)
#모델평가
print( sum(result5==y_test) / len(y_test) )  # 0.77

#11. 그레디언트 부스팅으로 모델 생성 
from  sklearn.ensemble  import  GradientBoostingClassifier 
model6 = GradientBoostingClassifier( n_estimators=300, random_state=1)

#모델훈련
model6.fit(x_train2, y_train)
#모델예측
result6 = model6.predict(x_test2)
#모델평가
print( sum(result6==y_test) / len(y_test) ) # 0.78

아나콘다 프롬프트창을 여시고 pip install xgboost 를 실행하세요 ~~

#12. xgboost 모델 생성 
from  xgboost  import  XGBClassifier

model7 = XGBClassifier(n_estimators=300, random_state=1)
evals = [ (x_test2, y_test) ] 

model7.fit( x_train2, y_train, early_stopping_rounds=100, eval_metric='error',
               eval_set = evals, verbose=1)

# 설명: early_stopping_rounds은 과적합을 방지 시키기 위해서 학습을 조기종료시키는 기능
# eval_metric 에 오차함수명을 기술하면 되는데 logloss는 여러개의 오차함수들중에 하나입니다.
# 오차함수명 mse, mae, logloss, error, merror, auc

result7 = model7.predict(x_test2)
print ( sum( result7 == y_test) / len(y_test)  ) #0.75 ---> 0.77

지금까지는 gridsearch 없이 배깅과 부스팅을 구현했고 앞으로는 gridsearch 를 써서 구현하겠습니다.


■ gridsearch 를 구현해서 최적의 파라미터로 셋팅된 모델의 정확도 확인하기 

1. 의사결정트리 단독으로만 했을때 

grid search 사용안했을 때: 0.72 
grid search 사용했을때 :  0.73     (정밀도 기준 정확도: 0.73, 재현율 기준 정확도: 0.73) 

게시글: 2036 [중요] 의사결정트리 gridsearch 적용 전체코드

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from  sklearn.tree  import  DecisionTreeClassifier

#▩ 판다스로 독일 은행의 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")

# 2. 데이터 탐색(명목형 데이터)
credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )

# 3. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

# 4. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 5. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

# Set the parameters by cross-validation

from sklearn.model_selection import GridSearchCV  # k-holdout 기능을 적용해서 최적의 하이퍼
                                                                     # 파라미터를 찾는 모듈

param_grid = [ { 'max_depth' :[2,3,4,5,6,7,8,9,10,20,50,100],  #의사결정트리 모델의 하이퍼 파라미터
                      'random_state' : [0,1,2,3,4,5,6,7,8,9,10] } ]    # 의 범위를 지정 

scores = ['precision', 'recall']  # 정밀도, 재현율로 최적의 하이퍼 파라미터를 찾는다. 

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    
    model =DecisionTreeClassifier(criterion='entropy') # 의사결정트리 모델 생성 
    
    clf = GridSearchCV(  DecisionTreeClassifier(criterion='entropy'), param_grid, 
                             scoring='%s_macro' % score )  # 최적의 하이퍼파라미터를 찾기위한 모델생성
    
    clf.fit(x_train2, y_train) # 모델훈련 

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)  # 최적의 하이퍼 파라미터를 출력
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score'] # 교차검정한 검정 데이터의 정밀도 평균
    stds = clf.cv_results_['std_test_score']  # 교차검정한 검정 데이터의 정밀도 표준편차 
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
              % (mean, std * 2, params))    # 교차검정한 정밀도 여러개 출력 
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full eval‎uation set.")
    print()
    y_true, y_pred = y_test, clf.predict(x_test2) #최적의 하이퍼파라미터로 테스트 데이터를 예측합니다. 
    print(classification_report(y_true, y_pred))  # 작은 이원교차표 출력한다.
    print(sum(y_true==y_pred)/len(y_true))     # 테스트 데이터에 대한 정확도를 출력합니다. 
    print()


2. 배깅 + 의사결정트리 모델의 gridsearch 를 적용한 코드 

그리드 서치 적용전 정확도:  0.75
그리드 서치 적용후 정확도:  0.76

#▩ 판다스로 독일 은행의 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")

# 2. 데이터 탐색(명목형 데이터)
credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )

# 3. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

# 4. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 5. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

# Set the parameters by cross-validation

from sklearn.ensemble  import  BaggingClassifier
from sklearn.model_selection import GridSearchCV  # k-holdout 기능을 적용해서 최적의 하이퍼
from sklearn.metrics import classification_report                                  

param_grid = [ {'n_estimators' : [10, 50, 100, 500, 1000] } ]

scores = ['precision', 'recall']  # 정밀도, 재현율로 최적의 하이퍼 파라미터를 찾는다. 

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    
    bagging2 = BaggingClassifier() 
    
    clf = GridSearchCV(  bagging2 , param_grid, 
                             scoring='%s_macro' % score )  # 최적의 하이퍼파라미터를 찾기위한 모델생성
    
    clf.fit(x_train2, y_train) # 모델훈련 

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)  # 최적의 하이퍼 파라미터를 출력
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score'] # 교차검정한 검정 데이터의 정밀도 평균
    stds = clf.cv_results_['std_test_score']  # 교차검정한 검정 데이터의 정밀도 표준편차 
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
              % (mean, std * 2, params))    # 교차검정한 정밀도 여러개 출력 
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full eval‎uation set.")
    print()
    y_true, y_pred = y_test, clf.predict(x_test2) #최적의 하이퍼파라미터로 테스트 데이터를 예측합니다. 
    print(classification_report(y_true, y_pred))  # 작은 이원교차표 출력한다.
    print(sum(y_true==y_pred)/len(y_true))     # 테스트 데이터에 대한 정확도를 출력합니다. 
    print()


3. 랜덤 포레스트 + gridsearch 적용 코드 

                               그리드 서치 적용전           그리드 서치 적용후 
의사결정트리 단독:              0.72                          0.73
배깅 + 의사결정트리:           0.75                          0.76
랜덤포래스트 :                    0.73                          0.79
엑스트라 트리 :                   0.81                           ?

45분까지 쉬세요 ~~

#▩ 판다스로 독일 은행의 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")

# 2. 데이터 탐색(명목형 데이터)
credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )

# 3. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

# 4. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 5. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

# Set the parameters by cross-validation

from sklearn.ensemble  import  RandomForestClassifier
from sklearn.model_selection import GridSearchCV  # k-holdout 기능을 적용해서 최적의 하이퍼
from sklearn.metrics import classification_report                                  

param_grid = [ {'n_estimators' : [10, 30, 50, 100],
                       'random_state' : [0,1,2,3,4,5,6,7,8,9,10] } ]

scores = ['precision', 'recall']  # 정밀도, 재현율로 최적의 하이퍼 파라미터를 찾는다. 

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    
    model9 =RandomForestClassifier()
    
    clf = GridSearchCV(  model9 , param_grid, 
                             scoring='%s_macro' % score )  # 최적의 하이퍼파라미터를 찾기위한 모델생성
    
    clf.fit(x_train2, y_train) # 모델훈련 

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)  # 최적의 하이퍼 파라미터를 출력
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score'] # 교차검정한 검정 데이터의 정밀도 평균
    stds = clf.cv_results_['std_test_score']  # 교차검정한 검정 데이터의 정밀도 표준편차 
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
              % (mean, std * 2, params))    # 교차검정한 정밀도 여러개 출력 
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full eval‎uation set.")
    print()
    y_true, y_pred = y_test, clf.predict(x_test2) #최적의 하이퍼파라미터로 테스트 데이터를 예측합니다. 
    print(classification_report(y_true, y_pred))  # 작은 이원교차표 출력한다.
    print(sum(y_true==y_pred)/len(y_true))     # 테스트 데이터에 대한 정확도를 출력합니다. 
    print()

4. 엑스트라 트리 + gridsearch 적용 코드 

                               그리드 서치 적용전           그리드 서치 적용후 
의사결정트리 단독:              0.72                          0.73
배깅 + 의사결정트리:           0.75                          0.76
랜덤포래스트 :                    0.73                          0.79
엑스트라 트리 :                   0.81                          0.77


# 1. 데이터를 로드
import  pandas  as  pd 
credit =  pd.read_csv("d:\\data\\credit.csv")

# 2. 데이터 탐색(명목형 데이터)
credit2 = pd.get_dummies(credit.iloc[ :  , :-1] )

# 3. 훈련 데이터와 테스트 데이터를 분리
x =  credit2.to_numpy()                #   학습 시킬 데이터 생성 
y =  credit.iloc[ :  , -1].to_numpy()   #   정답 데이터 생성

from  sklearn.model_selection  import  train_test_split

x_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.1, random_state=1)

# 4. 훈련 데이터로 정규화 계산
from  sklearn.preprocessing import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)

# 5. 계산된 내용으로 훈련데이터를 변형

x_train2 = scaler.transform(x_train)

#계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

# Set the parameters by cross-validation

from sklearn.ensemble  import  ExtraTreesClassifier
from sklearn.model_selection import GridSearchCV  # k-holdout 기능을 적용해서 최적의 하이퍼
from sklearn.metrics import classification_report                                  

param_grid = [ {'n_estimators' : [10, 30, 50, 100],
                      'max_depth' : [ 40, 50, 60 ],
                       'random_state' : [0,1,2,3,4,5,6,7,8,9,10] } ]

scores = ['precision', 'recall']  # 정밀도, 재현율로 최적의 하이퍼 파라미터를 찾는다. 

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()
    
    model10 = ExtraTreesClassifier()
    
    clf = GridSearchCV(  model10 , param_grid, 
                             scoring='%s_macro' % score )  # 최적의 하이퍼파라미터를 찾기위한 모델생성
    
    clf.fit(x_train2, y_train) # 모델훈련 

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)  # 최적의 하이퍼 파라미터를 출력
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score'] # 교차검정한 검정 데이터의 정밀도 평균
    stds = clf.cv_results_['std_test_score']  # 교차검정한 검정 데이터의 정밀도 표준편차 
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
              % (mean, std * 2, params))    # 교차검정한 정밀도 여러개 출력 
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full eval‎uation set.")
    print()
    y_true, y_pred = y_test, clf.predict(x_test2) #최적의 하이퍼파라미터로 테스트 데이터를 예측합니다. 
    print(classification_report(y_true, y_pred))  # 작은 이원교차표 출력한다.
    print(sum(y_true==y_pred)/len(y_true))     # 테스트 데이터에 대한 정확도를 출력합니다. 
    print()

결론:

                               그리드 서치 적용전           그리드 서치 적용후 
의사결정트리 단독:              0.72                          0.73
배깅 + 의사결정트리:           0.75                          0.76
랜덤포래스트 :                    0.73                          0.79
엑스트라 트리 :                   0.81                          0.77

파이썬으로 배깅과 부스팅 코드를 적용해보면 gridsearch 적용할때 하이퍼 파라미터의 범위를
사람이 지정해줘야하는 불편함이 있습니다. 그런데 R 은 역사가 더 깊어서인지 파라미터의 범위를
알아서 지정합니다. 

▩ R 코드를 이용해서 배깅 구현하기 

# 1.필요한 패키지 로드
install.packages("ipred")
library(ipred)
set.seed(300)

# 2.데이터 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)
head(credit)

# 3. 훈련 데이터와 테스트 데이터 분리
library(caret)
in_train <- createDataPartition( credit$default, p=0.75, list=FALSE)
credit_train <- credit[ in_train,  ] # 훈련 데이터 구성
credit_test  <- credit[ -in_train, ] # 테스트 데이터 구성 

# 4. 배깅 모델 생성 
mybag <- bagging( default ~ . , data=credit_train, nbagg=25)
# 설명 : nbagg=25 은 앙상블에 사용되는 bag 의 갯수를 25개

# 5. 모델 예측 
credit_pred <- predict( mybag, credit_test)

# 6. 모델 성능평가 

sum( credit_pred==credit_test$default) / length( credit_test$default)  # 0.752

예제1.  위의 배깅 모델의 bag 의 갯수를 400개로 늘리고 정확도를 확인하시오 !

sum( credit_pred==credit_test$default) / length( credit_test$default)  # 0.776

■ R 로 부스팅 모델 생성하기 

# 1. 필요한 패키지를 로드합니다.
install.packages("adabag")
library(adabag)
set.seed(300)

# 2. 데이터를 로드합니다. 
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 3. 훈련 데이터와 테스트 데이터를 분리합니다.
library(caret)
in_train <- createDataPartition( credit$default, p=0.75, list=FALSE)

credit_train <- credit[ in_train,  ] # 훈련 데이터 구성
credit_test  <- credit[ -in_train, ] # 테스트 데이터 구성

# 4. 부스팅 모델을 생성합니다.
m_adaboost <- boosting( default ~ . , data=credit_train) 

# 5. 모델 예측
p_adaboost <- predict( m_adaboost,  credit_test) 

# 6. 모델 평가 
sum( p_adaboost$class == credit_test$default) / length(credit_test$default)  # 0.756

예제1. 부스팅 모델의 bag 의 갯수를 100개로 늘려서 정확도 보시오 ! 

m_adaboost <- boosting( default ~ . , data=credit_train, nbagg=100 )  # 0.76

예제2. 부스팅 모델의 bag 의 갯수를 400개로 늘려서 정확도를 보시오 !

m_adaboost <- boosting( default ~ . , data=credit_train, nbagg=400) # 0.772 

▩ 위의 부스팅 모델의 자동 튜닝을 적용하기 

# 1. 필요한 패키지를 로드합니다.
install.packages("adabag")
library(adabag)
set.seed(300)

# 2. 데이터를 로드합니다. 
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 3. 훈련 데이터와 테스트 데이터를 분리합니다.
library(caret)
in_train <- createDataPartition( credit$default, p=0.75, list=FALSE)

credit_train <- credit[ in_train,  ] # 훈련 데이터 구성
credit_test  <- credit[ -in_train, ] # 테스트 데이터 구성

# 4. 부스팅 모델을 생성합니다.
#m_adaboost <- boosting( default ~ . , data=credit_train) 

#trCtl <- trainControl( method="cv", number=5) 
#m2 <- train( default ~ ., data=credit_train, trControl=trCtl, method="adaboost")

m2 <- train( default ~ ., data=credit_train, method="adaboost")
m2

# 5. 모델 예측
p_adaboost <- predict( m2,  credit_test) 

# 6. 모델 평가 
sum( p_adaboost$class == credit_test$default) / length(credit_test$default)  # 0.756


문제359.  (오늘의 마지막 문제) 독일 은행 데이터의 채무불이행자를 예측하는 머신러닝 모델을
            부스팅 모델과 배깅모델 2개의 정확도를 출력하는데 둘다 자동 튜닝 적용한 코드로 구현하시오 !

1. 부스팅 모델 (자동튜닝)
m2 <- train( default ~ ., data=credit_train, method="adaboost")
m2


2. 배깅 모델 (자동튜닝)
m1 <- train( default ~ ., data=credit_train, method="treebag")
m1

5시 신호 보냈습니다. ~~~~
6시 신호 보냈습니다. ~~~~~


요번주 금요일 4시에 단답형 문제 10문제(3문제는 서술형)

다음주 목요일 4시에 단답형 문제 10문제(3문제는 서술형)

게시글 2041번. 요번주 금요일 시험관련한 자료

R 포트폴리오 ppt 로 생성하고 제출 ( 8월 26일 )과 발표 (3개과목을 하나의 제출물로 평가)

이력서 제출할 때 R 또는 파이썬으로 데이터 분석 포트폴리오 같이 제출 

▩ 11장 목차

1. 앙상블 :  약한 학습자 여러개를 모아서 강력한 하나의 머신러닝 모델을 구현하는 방법
2. 배깅   :   훈련 데이터에서 데이터를 샘플링해서 각각의 같은 머신러닝 모델로 학습하여
                결과를 평균 또는 투표로 출력하는 앙상블 알고리즘
3. 부스팅 :  배깅을 개선한 앙상블 알고리즘으로 데이터를 샘플링할 때 잘못 분류한 데이터에 대해
                가중치를 주어 좀더 많이 샘플링하여 학습할 수 있도록 하는 앙상블 알고리즘 

▩ iris 데이터로 배깅과 부스팅 실습

1. 의사결정트리 단독으로 모델 생성
2. 배깅으로 모델 생성 
3. 부스팅으로 모델 생성 

▩ 1. 의사결정트리 단독으로 모델 생성

#1. 데이터 로드
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)

#2. 훈련 데이터와 테스트 데이터 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( iris$Species, p=0.8, list=FALSE)

iris_train <- iris[ in_train,   ]  #훈련 데이터 구성
iris_test  <- iris[-in_train,   ]  #테스트 데이터 구성 

#3. 모델생성
library(C50)
set.seed(1)
model <- C5.0( Species ~ .  ,  data=iris_train, trials=100)

#4. 모델예측
result <- predict( model,  iris_test )

#5. 모델평가 

sum( iris_test$Species==result ) / length(iris_test$Species) #  0.9333333

▩ 2. 배깅(bagging) 으로 실험하기 

#1. 데이터 로드
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)

#2. 훈련 데이터와 테스트 데이터 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( iris$Species, p=0.8, list=FALSE)

iris_train <- iris[ in_train,   ]  #훈련 데이터 구성
iris_test  <- iris[-in_train,   ]  #테스트 데이터 구성 

#3. 모델생성
library(ipred)
set.seed(1)
my_bag <- bagging( Species ~ .   ,  data = iris_train,  nbagg=25)

#4. 모델예측
p_bag  <- predict(my_bag,  iris_test )
p_bag

#5. 모델평가 

sum( iris_test$Species== p_bag) / length(iris_test$Species) # 0.9666667

문제360. 유방암 데이터를 의사결정트리 단독으로 했을때와 배깅으로 했을때 
            정확도의 차이가 있는지 확인하시오 !  (답글로 올려주세요)

wisc_bc_data <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

1. 의사결정트리 단독으로 했을때의 정확도 ( 위의 스크립트와 똑같이 하세요)
2. 배깅으로 했을때의 정확도(위의 스크립트와 똑같이 하세요)

▩ iris 데이터로 부스팅 모델 생성하기

 1. 단독 의사결정트리 ---> 2. 배깅일때 ----> 3. 부스팅일때 
        (0.933 )                     (0.966)                 ( 0.966)

#1. 데이터 로드
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)

#2. 훈련 데이터와 테스트 데이터 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( iris$Species, p=0.8, list=FALSE)

iris_train <- iris[ in_train,   ]  #훈련 데이터 구성
iris_test  <- iris[-in_train,   ]  #테스트 데이터 구성 

#3. 모델생성
library(adabag)
set.seed(1)
m_adaboost<- boosting( Species ~ .  ,  data=iris_train )

#4. 모델예측
p_adaboost <- predict( m_adaboost , iris_test )

#5. 모델평가 

sum( iris_test$Species== p_adaboost$class) / length(iris_test$Species) #  0.9333333

▩ 유방암 데이터로 부스팅 모델 생성하기

 1. 의사결정트리 단독일때 ----> 2. 배깅일때 ------> 부스팅일때 
      ( 0.9646018 )                   ( 0.9823009 )           ( 0.9557522 )

배깅을 개선한 알고리즘이 부스팅이지만 꼭 배깅보다 더 성능이 좋은것은 아닙니다.

문제361. 유방암 데이터로 부스팅 모델을 생성하고 정확도가 얼마나 나오는지 확인하시오 !

#1. 데이터 로드
wisc <- read.csv("wisc_bc_data.csv",stringsAsFactors = T)

#2. 훈련 데이터와 테스트 데이터 분리
library(caret)
set.seed(1)
in_train <- createDataPartition(wisc$diagnosis,p=0.8,list=FALSE)

wisc_train<- wisc[in_train,-1]
wisc_test <- wisc[-in_train,-1]

#3. 모델 생성
library(adabag)
set.seed(1)
m_adaboost<- boosting( diagnosis~., data=wisc_train )

#4.모델 예측
p_adaboost <-predict(m_adaboost ,wisc_test)
p_adaboost

#5. 모델평가
sum(wisc_test$diagnosis==p_adaboost$class)/length(wisc_test$diagnosis)  # 0.9557522

의사결정트리 단독 ---> 배깅으로 구현 ----> 부스팅으로 구현 ---> train 함수로 자동으로 최적의
하이퍼 파라미터를 찾게 배깅구현 

▩ iris 데이터로  배깅 + 하이퍼 파라미터 찾기 자동화 

 1. 단독 의사결정트리 ---> 2. 배깅일때 ----> 3. 부스팅일때  ---> 4. train 함수를 이용했을때 
        (0.933 )                     (0.966)                 ( 0.966)                   ( 0.9666667) 

# 1. 데이터 로드
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)

# 2. 훈련 데이터와 테스트 데이터 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( iris$Species, p=0.8, list=FALSE) 

iris_train <- iris[ in_train,   ]
iris_test  <- iris[ - in_train,  ]

# 3. 배깅 + 자동화 모델 생성 
library(ipred)
library(caret)
set.seed(1)
trCtl <- trainControl( method='cv', number=5)
m1 <- train( Species ~ .  , data=iris_train, trControl= trCtl,  method="treebag" )
m1
# 4. 모델 예측
p_bag2 <- predict( m1,  iris_test)
p_bag2

# 5. 모델 평가 
sum( p_bag2 == iris_test$Species)  / length(iris_test$Species)  # 0.9666667

설명: 책 500페이지 


 1. 단독 의사결정트리 ---> 2. 배깅일때 ----> 3. 부스팅일때  ---> 4. train 함수를 이용했을때 
        (0.933 )                     (0.966)                 ( 0.966)                   ( 0.9666667) 

지금까지는 150건의 iris 데이터로 앙상블 실험을 했고 이번에는 독일은행 데이터(1000건)로
앙상블 실험을 해보겠습니다.

▩ 독일 은행 데이터의 머신러닝 모델을 만드는데 앙상블 안썼을 때와 앙상블 사용했을때의 차이 실험

1. 앙상블 안썼을 때(의사결정트리 단독)

2. 앙상블 사용했을때(배깅)

▩ 1. 앙상블 안썼을 때(의사결정트리 단독)

# 1. 데이터를 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련과 테스트 분리
library(caret)
set.seed(1)
in_trian <- createDataPartition(credit$default, p=0.8, list=FALSE)
credit_train <- credit[ in_train,  ]
credit_test  <- credit[ -in_train, ]

# 3. 모델 생성
library(C50)
set.seed(1)
model <- C5.0( default ~ .,  data=credit_train, trials=100)

# 4. 모델 예측
result <- predict( model,  credit_test )
result

# 5. 모델 평가 
sum( credit_test$default == result ) / length(credit_test$default)  # 0.719

45분까지 쉬시고 그리고 앙상블로 모델을 만들어보겠습니다. 

▩  2. 앙상블 사용했을때(배깅)

앙상블 사용하지 않았을때의 정확도 : 0.719
앙상블 사용 했을 때의 정확도 :  ? 

# 1. 데이터 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련과 테스트 분리(8:2)
library( caret )
set.seed(1)
in_train <- createDataPartition( credit$default, p=0.8, list=FALSE)
credit_train <- credit[ in_train,   ]
credit_test  <- credit[ -in_train,  ]

# 3. 앙상블 모델 생성(배깅)
library(ipred)
set.seed(1)
mybag <- bagging( default ~ . , data=credit_train, nbagg=100)

# 4. 모델 예측
credit_pred <- predict( mybag , credit_test) 
credit_pred

# 5. 모델 평가 
sum( credit_test$default == credit_pred$class) / length( credit_test$default)  # 0.745

설명: 앙상블 사용안했을때는 0.719 였는데 앙상블 사용하니까 0.745로 정확도가 상승되었습니다.

▩  3. 앙상블 + 자동튜닝으로 실험 (p500)

앙상블 사용하지 않았을때의 정확도 : 0.719
앙상블 사용 했을 때의 정확도 :  0.745
앙상블 + 자동 튜닝의 정확도 :    ?

# 1. 데이터 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련 데이터와 테스트 데이트를 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( credit$default, p=0.8, list=FALSE)
credit_train <- credit[ in_train,  ]
credit_test  <- credit[ -in_train,  ] 

# 3. 앙상블 + 자동튜닝 모델을 생성
library(adabag)
library(caret)

trCtrl <-  trainControl( method='cv', number=10) 
set.seed(1)
m2 <- train( default ~ .  , data=credit_train, trControl=trCtrl, method="treebag")
m2

# 4. 모델 예측
credit_pred <- predict( m2, credit_test) 
credit_pred

# 5. 모델 평가 
sum( credit_test$default == credit_pred)  / length( credit_test$default) # 0.775 

앙상블 사용하지 않았을때의 정확도 : 0.719
앙상블 사용 했을 때의 정확도 :  0.745
앙상블 + 자동 튜닝의 정확도 :   0.775 

문제362. (점심시간 문제)  위의 자동튜닝 모델을 다시 생성하는데 method 를 treebag 말고
            다른 모델로 넣고 수행하시오 !  ( 아래의 사이트를 참고하세요!)

http://topepo.github.io/caret/train-models-by-tag.html

또는   method = 'bagFDA' 로 변경해서 수행하고 정확도를 올려놓고 검사받으세요 ~~ 

   method = 'bagFDA'   정확도는 0.72
   method="xgbTree"   정확도는 0.73 

  코드만 답글로 올려놓고 정확도는 식사하고서 수정해놓으세요 ~~~

▩ 11장. 목차

1. 앙상블
2. 배깅
3. 부스팅 
4. 랜덤포레스트 (배깅 + 의사결정트리)

▩ 랜덤포레스트 (배깅 + 의사결정트리)  p504

랜덤 포레스트란 ? 앙상블 기법중에 하나인데 의사결정트리와 배깅의 결합한 모델입니다. 
                       트리 앙상블이 생성된 후 트리의 예측을 결합하고자 투표를 합니다. 

실습:
1. 자동튜닝을 하지않은 랜덤포레스트 모델로 독일은행 데이터를 분류
2. 자동튜닝을 적용한 랜덤포레스트 모델로 독일은행 데이터를 분류

▩ 1. 자동튜닝을 하지않은 랜덤포레스트 모델로 독일은행 데이터를 분류
 
# 1. 데이터 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련과 테스트로 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( credit$default, p=0.8, list=F)
credit_train <- credit[ in_train,  ]
credit_test  <- credit[ -in_train, ]

# 3. 랜덤포레스트 모델 생성
install.packages("randomForest")
library(randomForest)
set.seed(1)
rf <- randomForest( default ~ . , data=credit_train)
rf 

# 4. 모델 예측
result <- predict( rf,  credit_test )
result 

# 5. 모델 평가 
sum( result == credit_test$default ) / length(credit_test$default)  # 0.735

▩ 2. 자동튜닝을 적용한 랜덤포레스트 모델로 독일은행 데이터를 분류
 
# 1. 데이터 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련과 테스트로 분리
library(caret)
set.seed(1)
in_train <- createDataPartition( credit$default, p=0.8, list=F)
credit_train <- credit[ in_train,  ]
credit_test  <- credit[ -in_train, ]

# 3. 자동튜닝 모델 생성
library(caret)
trCtl <- trainControl(method='cv', number=10) 
set.seed(1)
m3 <- train( default ~  ., data=credit_train, method="rf", trControl=trCtl )
m3
# 설명:  mtry 가 랜덤포레스트의 유일한 하이퍼파라라미터인데 분할시점에 임의로 선택되는
# feature(컬럼)의 갯수 
mtry   정확도     카파통계량
   2    0.72750   0.1481595
  18    0.73875   0.3162702
  35    0.75000   0.3529312

# 4. 모델 예측
result2 <- predict( m3,  credit_test )
result2 

# 5. 모델 평가 
sum( result2 == credit_test$default ) / length(credit_test$default)  #  0.75

※ 자동 튜닝을 적용했더니 0.73 ---> 0.75로 정확도가 향상되었습니다. 

문제363. 책 512페이지를 보고 위의 랜덤포레스트 모델 평가지표인 ROC 곡선 그래프를 그리시오 !

trCtl <- trainControl(method='cv', number=10, classProbs=TRUE, savePredictions =TRUE,
                      summaryFunction = twoClassSummary) 

m3 <- train( default ~  ., data=credit_train, method="rf", trControl=trCtl, metric="ROC" )

m3

library(pROC)
roc_rf <- roc( m3$pred$obs , m3$pred$yes)
plot(roc_rf, col='blue', legacy.axes=TRUE)

문제364.  iris 의 품종을 분류하는 랜덤포레스트 모델을 자동튜닝을 적용한 머신러닝 모델로
             생성하시오 ~

 카페에 답 코드를 올려주세요 ~~~~~~

iris <- read.csv("iris2.csv", stringsAsFactors=TRUE) 

sum( result2 == iris_test$Species ) / length(iris_test$Species)  
[1] 0.9666667

■ 12장. 특화된 머신러닝 주제 

 * 주요목차:  1.  병렬로 작업을 수행하는 방법
                 2.  사회적 연결망 
                 3.  오라클과 R 연동 --> 이미 수업했음
                 4.  R 로 웹스크롤링 ---> 파이썬으로 웹스크롤링 

▩ 1.  병렬로 작업을 수행하는 방법 (p557) 

▣ 병렬 컴퓨팅 이란?

   동시에 많은 계산을 하는 연산의 한 방법인데 크고 복잡한 문제를 작게 나눠서 
   동시에 병렬적으로 해결하는 방법입니다. 

▣ 병렬처리란 ?

   멀티 코어 CPU 환경에서 하나의 작업을 분할해서 각각의 코어가 병렬적으로 처리하는것

  그림 

 동시성:  멀티작업을 위해 멀티 스레드가 번갈아가며 실행하는 성질
 병렬성:   멀티작업을 위해 멀티 코어를 이용해서 동시에 실행하는 성질

▣ 여러분들 노트북에 CORE 수가 몇개인지 확인하기 

 프로그램 다운로드:  CPU_Z 

코어란?  CPU(컴퓨터의 두뇌) 에서 실질적으로 연산을 담당하는 장치 

과거에는 CPU 1개당 코어가 1개가 들어갔으나 현재 출시되는 CPU 에는 많게는 32개까지
들어가는 멀티 코어로 구성되어 있습니다. 

쓰래드란 ?  코어에 할당된 작업공간 입니다. 

코어가 일을하는 노동자라고 하면 쓰래드는 노동자가 동시에 할 수 있는 업무의 갯수라고 
이해하면 됩니다. 

실습:  

# 1. R에서 코어의 갯수 확인하기 !

install.packages("future")  # 병렬처리를 쉽게 구현해주는 패키지 
library(future)

availableCores()

system 
    12   <-- 12개의 작업을 동시에 수행할 수 있다는 뜻입니다. 

#2. 병렬작업을 위해 추가로 패키지 설치 

install.packages("future.apply")
install.packages("furrr")

library(future.apply)

#3. 실험을 위해 테스트 데이터 만들기 
library(dplyr)
data.frame(  
                 t1 = rnorm(10000),
                 t2 = rnorm(10000),
                 t3 = rnorm(10000)
               ) %>% write.csv( . , "testFile.csv" )

#4. 병렬처리를 하지 않았을때의 수행속도 확인 

a <- system.time( {   for ( i in 1:500 ) {  
                                                   read.csv("testFile.csv")
                                                 }
                        } )

a

사용자  시스템 elapsed 
  28.13    0.67   28.86 

# 5. 병렬처리를 했을때

작업관리자를 먼저 여세요 (ctl+alt +delete) 

plan(multisession, workers=8)
library(future.apply)

a2 <- system.time( {  future_lapply(1:500, function(x) { read.csv("testFile.csv") } )
                        } )

a2
 사용자  시스템 elapsed 
   0.35    0.25    6.39 

plan(multisession, workers=1)

※ 주의할 사항 !!  쓰래드의 갯수보다 작게 동시에 작업할 작업자들의 갯수를 지정해줘야합니다
                       만약에 시스템에 지정된 쓰래드의 갯수보다 더 많이 지정을 하게 되면
                       오히려 성능이 더 느려집니다. 

▩ 머신러닝 모델 생성시 병렬로 작업해서 모델 생성하는 방법 

1. 독일은행 데이터의 채무 불이행자 예측하는 배깅 모델을 병렬 작업 없이 수행 

2.  독일은행 데이터의 채무 불이행자 예측하는 배깅 모델을 병렬 작업으로 수행 

▩ 1. 독일은행 데이터의 채무 불이행자 예측하는 배깅 모델을 병렬 작업 없이 수행 

# 1. 데이터를 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련 데이터와 테스트 데이터를 나눕니다. 
library(caret)
set.seed(1)
in_train <- createDataPartition( credit$default, p=0.8, list=FALSE)
credit_train <- credit[ in_train,  ]
credit_test  <- credit[ -in_train, ]

# 3. 앙상블 + 자동튜닝 모델 생성 (병렬작업 없이 수행)
library(adabag)
library(caret)
trCtrl <- trainControl( method='cv', number=10)
set.seed(1)
a <- system.time( {  
    m2 <- train( default ~ . , data=credit_train, trControl=trCtrl, method="treebag")
} )
a

사용자  시스템 elapsed 
   4.42    0.03    4.49 

# 4. 모델 예측
result2 <- predict( m2,  credit_test )
result2 

# 5. 모델 평가 
sum( result2 == credit_test$default ) / length(credit_test$default)   # 0.775

▩ 2. 독일은행 데이터의 채무 불이행자 예측하는 배깅 모델을 병렬으로 수행 

# 0. 병렬처리를 위한 패키지 library 
library(parallel)
library(doParallel)

# 1. 데이터를 로드
credit <- read.csv("credit.csv", stringsAsFactors=TRUE)

# 2. 훈련 데이터와 테스트 데이터를 나눕니다. 
library(caret)
set.seed(1)
in_train <- createDataPartition( credit$default, p=0.8, list=FALSE)
credit_train <- credit[ in_train,  ]
credit_test  <- credit[ -in_train, ]

# 3. 앙상블 + 자동튜닝 모델 생성 (병렬작업 없이 수행)
library(adabag)
library(caret)
trCtrl <- trainControl( method='cv', number=10)

cl <- makeCluster(detectCores() -1 )  # 현재 시스템의 코어의 갯수의 마이너스 1개를 cl 넣는다.
cl

registerDoParallel(cl)

set.seed(1)
a <- system.time( {  
    m2 <- train( default ~ . , data=credit_train, trControl=trCtrl, method="treebag", allowParallel=TRUE)
} )
a

# 4. 모델 예측
result2 <- predict( m2,  credit_test )
result2 

# 5. 모델 평가 
sum( result2 == credit_test$default ) / length(credit_test$default)   # 0.775

#6. 병렬 프로세서 정리하는 방법

stopCluster(cl)
on.exit(stopCluster(cl) )

문제365.  iris 의 품종을 분류하는 랜덤포레스트+자동튜닝 모델을 병렬로 작업해서 생성하시오 !

답글로 올렸던 문제364번 코드를 병렬로 처리하겠금 코드만 수정하세요 ~~

■ 12장. 특화된 머신러닝 

 1. 병렬 처리 방법
 2. 사회적 연결망 분석 

▩ 사회적 연결망 분석(social network analysis)  p546

 사회적 연결망 분석이란 사회 연결망 데이터를 활용하여 사회 연결망과
 사회구조등을 사회과학적으로 분석하는 하나의 방식입니다.
 
 인간과 인간의 관계, 인간과 사회의 관계등 다양한 관계들을 과학적으로 밝히고
 분석하는것입니다. 20세기 후반부터 인터넷과 소셜미디어의 발전으로 분석이
 활발하게 이루지고 있음

 책 549 페이지의 그림 12.4 를 참조 

▩ igraph 함수를 이용해서 emp 테이블의 사원간의 관계도를 시각화 하기 

# 1. emp.csv 를 불러옵니다. 

emp <- read.csv("emp.csv", header=T)
emp

# 2. emp 테이블을 셀프조인해서 사원이름을 출력하고 관리자의 이름을 출력합니다.

SQL> select  사원.ename,  관리자.ename
              from  emp  사원,  emp  관리자
              where  사원.mgr = 관리자.empno;

x <- merge( emp, emp, by.x="mgr", by.y="empno")
x
a <- x [    ,  c("ename.x",  "ename.y") ]
a

# 3. 그래프를 그리기 위해서 사원이름과 관리자 이름을 서로 연결시키는 데이터프레임 생성

install.packages("igraph")
library(igraph)
b <- graph.data.frame( a , directed=T)
b

#4. 그래프를 그립니다.

plot(b)

▩ 글로벌 회사들의 소송 관계도를 사회적 연결망으로 시각화 하기 

데이터 게시판 316번. 사회적 연결망 데이터 : node2.csv,  link2.csv 

데이터 소개: node2.csv 는 노드명(node) 과 노드번호(idx)으로 회사이름과 회사번호로 구성
                 link2.csv 는 source 와 target 은 회사명이고 source_idx, target_idx 는 회사번호
                 로 소송관계 데이터 입니다. 

#1. 데이터를 로드
node_df  <- read.csv("node2.csv")
link_df  <- read.csv("link2.csv")
node_df
link_df

#2. 시각화 
install.packages("networkD3")
install.packages("dplyr")
library(networkD3)
library(dplyr)

network1 <- forceNetwork( Links = link_df,       # 관계도 데이터 프레임명
                                     Nodes = node_df,   # 회사이름, 회사번호가 있는 데이터 프레임명
                                     Source = 'source_idx' , # 소송하는 회사번호
                                     Target = 'target_idx' ,   # 소송당하는 회사번호
                                     NodeID ='node',         # 회사명
                                     Group ='idx',              # 회사번호 
                                     opacityNoHover= TRUE, # 정적일 때 불투명 정도 
                                     zoom=TRUE, # FALSE 로하면 해당 노드의 동그라미만 보이고 TRUE 로 하면
                                                       # 연관된 회사들이 선명하게 보입니다. 
                                     bounded =TRUE,  # 그래프가 화면 밖으로 빠져나가지 않게 한다.
                                     fontSize= 15,   # 글씨 크기 
                                     linkDistance=75,  # 연결선 길이
                                     opacity=0.9 )      #   불투명 정도

network1
                                      
                                      
문제366. 영화 기생충에 나오는 배우들의 관계도를 사회적 연결망 그래프로 시각화 하시오 !

데이터 게시판의 게시글 317번 


문제367. 사원 테이블의 사회적 연결망을 3D network 그래프로 시각화 하시오 !

 3D network 그래프를 그릴려면 node_df 와 link_df  데이터 프레임을 생성하면 됩니다. 

답:
# 1. emp.csv 를 로드합니다.
emp <- read.csv("emp.csv", header=T)
emp

# 2. 0번부터 시작하는 번호를 emp 데이터프레임에 rownum 라는 컬럼으로 생성합니다.
emp$rownum <- 0:13

# 3. 노드에 대한 정보(노드명, 노드번호)를 담는 emp_node 라는 데이터 프레임을 생성한다.
                               ↓           ↓
                        사원이름      rownum 

emp_node <- emp[   , c("ename", "rownum") ]
emp_node

# 4. 링크에 대한 정보(사원이름, 관리자이름, 사원의 rownum, 관리자의 rownum) 을 담는
      emp_link 라는 이름의 데이터 프레임을 생성한다. 

x <-  merge( emp, emp,  by.x="mgr", by.y="empno")
x

emp_link <- x[    , c("ename.x", "ename.y", "rownum.x", "rownum.y") ]
emp_link

# 5. emp_node 의 컬럼명을 node 와 idx 로 변경한다. 

colnames(emp_node) <-  c("node", "idx")
emp_node

# 6. emp_link 의 컬럼명을  source, target, source_idx, target_idx 로 변경한다. 

colnames(emp_link) <- c("source", "target", "source_idx", "target_idx") 
emp_link

# 7. emp_link 와 emp_node 를 이용해서 3D network 으로 사회적 연결망 그래프를 그리시오

library(dplyr)
library(networkD3)

network1 <- forceNetwork( Links = emp_link ,       
                                        Nodes = emp_node,
                                        Source = 'source_idx' ,
                                        Target = 'target_idx' ,
                                        NodeID ='node',
                                        Group ='idx',
                                        opacityNoHover= TRUE,  
                                        zoom=TRUE,
                                        bounded =TRUE,
                                        fontSize= 15,
                                        linkDistance=175,
                                        opacity=100 )

network1


문제368. 소설 장발장의 인물 관계도를 사회적 연결망으로 시각화 하시오 !

 데이터셋 :  게시글 315 소설 장발장 사회적 연결망 

 온라인 수업의 집중을 높이기 위해서 답글로 시각화된 이미지를 올려주세요 ~~~


문제369.  코로나 확산에 대한 사회적 연결망 그래프를 그리기 위해 코로나 환자 데이터를
             cov_df 라는 이름으로 불러오시오 ~

데이터셋:  PatientInfo.csv   , 데이터 게시판 318번 게시글

cov_df <- read.csv("d:\\data\\PatientInfo.csv")
head(cov_df)

문제370.  infected_by 컬럼을 출력하고 그 옆에 infected_by 의 건수를 출력하시오

x <- aggregate( patient_id ~ infected_by, cov_df, length)
x

문제371.  위의 결과를 다시 출력하는데 건수가 높은것부터 출력하시오 !

library( doBy)
head( orderBy( ~-patient_id, x)  )

문제372.  환자번호 2000000205  인 환자가 사는 지역(city) 이 어딘지 출력하시오 !

cov_df[ cov_df$patient_id=='2000000205',  'city']

"Seongnam-si"

문제373. cov_df 에서 사는 지역이 성남시인 환자들의 모든 데이터를 불러서 cov2 라는 
            데이터 프레임을 생성하시오 

cov2 <-  cov_df[ cov_df$city=='Seongnam-si',      ]
nrow(cov2)  # 173건

문제374.  cov2 에  번호를 0번부터 172번까지를 담는 rownum 컬럼을 생성하시오 

cov2$rownum <- 0:172
head(cov2)

문제375. (점심시간 문제)  환자번호와 rownum 컬럼을 담는 cov2_node 라는 데이터 프레임을
            생성하시오~


문제376. 아래의 cov_df  데이터프레임에서 강남구에 해당하는 데이터만 불러와서 
            cov2 라는 데이터 프레임으로 생성하시오 

cov_df <- read.csv("d:\\data\\PatientInfo.csv")
head(cov_df)

cov2 <- cov_df[cov_df$city=='Gangnam-gu',   ]
nrow(cov2) # 83

문제377.  cov2 에 0번부터 82번까지의 숫자를 담는 rownum 컬럼을 추가하시오 !

cov2$rownum <- 0:82
head(cov2)

문제378.  환자번호(patient_id) 와 rownum 컬럼을 담는 cov2_node 라는 데이터프레임을
             생성하시오 ~

cov2_node <- cov2[    , c("patient_id", "rownum") ]
head(cov2_node)

문제379.  환자번호, infected_by(감염한 환자번호), rownum.x 와 rownum.y 컬럼을 담는
             cov2_link 라는 데이터 프레임을 생성하시오 !

# 참고코드:   x <- merge(emp, emp, by.x="mgr", by.y="empno")
#                emp_link <- x [   , c("ename.x", "ename.y","rownum.x", "rownum.y") ]

x <- merge( cov2, cov2, by.x="infected_by", by.y="patient_id")
head(x)

cov2_link <- x[    , c("patient_id", "infected_by", "rownum.x", "rownum.y") ]
head(cov2_link)

문제380.  cov2_node 의 컬럼명을 node, idx 로 변경하시오 !

colnames(cov2_node) <- c("node", "idx")

문제381. cov2_link 의 컬럼명을  source, target, source_idx, target_idx 로 변경하시오 

colnames(cov2_link) <- c("source", "target", "source_idx" , "target_idx")  
head(cov2_link)

     source           target      source_idx target_idx
1 1000000064 1000000047          2          0
2 1000000065 1000000068          3          5
3 1000000344 1000000294         18         16
4 1000000373 1000000357         24         19
5 1000000513 1000000368         43         23
6 1000000422 1000000401         36         29

문제382. cov2_link 와 cov2_node 데이터프레임을 이용해서 사회연결망 그래프를 시각화 하시오

D3_network_LM<-forceNetwork(Links = cov2_link,
                            Nodes = cov2_node,
                            Source = 'source_idx', Target = 'target_idx',
                            NodeID = 'node', Group = 'idx',
                            opacityNoHover = TRUE, zoom = TRUE,
                            bounded = TRUE,
                            fontSize = 15,
                            linkDistance = 75,
                            opacity = 40)

D3_network_LM

다른 데이터로 포트폴리오를 만들고자 하면 데이터를 검색해서 찾으려 하지 말고
직접 만드는게 더 빠릅니다. 

링크정보, 노드정보를 담는 데이터 프레임 2개만 생성하면 됩니다. 

■ 7장. 서포트 벡터 머신  p343

■ 이론설명1. 서포트 벡터 머신이란 무엇인가요 ?

서포트벡터 머신(support vector machine) 은 기계학습의 분야중 하나로
패턴인식, 자료분석을 위한 지도학습 모델이며 주로 분류와 회귀를 위해 사용한다. 
두 클래스가 주어졌을때 SVM 알고리즘은 주어진 데이터 집합을 바탕으로 하여 
새로운 데이터가 어느 클래스에 속할지 판단하는 비확률적 이진 선형분류 모델을
생성합니다. 선형분류와 더불어 비선형 분류에서도 사용됩니다. 

■ 이론설명2. 서포트 벡터 머신에서 결정경계가 무엇인가요?

 그림

결정경계란 분류를 위한 기준선 입니다. 2차원일 때는 선이고 3차원일때는 평면이 됩니다.
우리가 생각할 수 있는 부분은 3차원까지이고 차원이 더 많아지게 되면 평면이 아니라
초평면이 됩니다. 

■ 이론설명3. 아래에서 데이터를 가장 잘 분류한 결정경계는 ?

 그림

답: F 인데 두 클래스 사이의 거리가 가장 먼 선이 좋은 결정경계 입니다. 

■ 이론설명4. 서포트 벡터머신에서 서포트 벡터는 무엇입니까?

그림

답:  결정경계와 가까이 있는 데이터 포인트들을 의미합니다. 
     이 데이터들이 경계를 정의하는 결정적인 역활을 합니다. 

■ 이론설명5. 서포트 벡터머신에서 마진(Margin)이 무엇입니까?

그림

답: 점선으로 부터 결정경계까지의 거리가 마진입니다. 최적의 결정경계는 마진을
     최대화 합니다. 

■ 이론설명6.  소프트 마진은 무엇이고 하드 마진은 무엇입니까 ? 

그림

답:  위의 그림중에서 위쪽 그림은 아웃라이어를 허용하지 않고 기준을 까다롭게
      세운 모델입니다. 이걸 하드 마진이라합니다.
      그리고 서포트 벡터와 결정경계사이의 거리가 매우 좁습니다. 
      즉 마진이 작아입니다. 그러면 오버피팅 문제가 발생 할 수 있습니다. 

      아래쪽 그림은 아웃라이어들이 마진 안에 어느정도 포함되도록 너그럽게
      기준을 잡았습니다. 이걸 소프트 마진이라 합니다. 
       이렇게 너그럽게 잡아놓으면 서포트 벡터와 결정경계사이의 거리가 멀어집니다.
       그러면 언더피팅 문제가 발생할 수 있습니다. 

■ 이론설명7.  그럼 이런 오류를 어느정도 허용하는지 결정하는 하이퍼 파라미터는 무엇입니까 ?

 
 답:  C  와  gamma 입니다. 

      C 는 클수록 하드마진(오류허용안함)이 일어나고
             작을수록 소프트마진(오류를 허용함) 이 일어납니다. 

      gamma 는 결정경계를 얼마나 유연하게 그을것인지를 정해주는 것입니다.
      gamma 를 높이면 학습 데이터에 많이 의존해서 결정경계를 구불구불하게 긋게 됩니다.
      그래서 오버피팅을 초래할 수 있습니다.  
     반대로 gamma 를 낮추면 학습 데이터에 별로 의존하지 않고 결정경계를 긋게 되므로
     언더 피팅이 발생할 수 있습니다. 

■ 서포트 벡터 머신 실습1 (iris 데이터)

# 1. 데이터 로드
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)

# 2. 훈련 데이터와 테스트 데이터로 분리
library(caret)
set.seed(1)
in_train <- createDataPartition(iris$Species, p=0.8, list=F)
train_data <- iris[ in_train ,  ]
test_data  <- iris[-in_train,   ]
nrow(train_data) # 120
nrow(test_data)  # 30

# 3. 모델 생성
install.packages("e1071")
library(e1071)

svm_model <- svm(Species ~ . , data=train_data,  kernel="linear")

# 4. 모델 예측
result <- predict( svm_model, test_data)
result 

# 5. 모델 평가
sum( result == test_data$Species) / length(test_data$Species)

# 6. 모델 성능 개선 

#커널변경: kernel="polynomial"

svm_model2 <- svm(Species ~ . , data=train_data,  kernel="polynomial")

# 4. 모델 예측
result2 <- predict( svm_model2, test_data)
result2 

# 5. 모델 평가
sum( result2 == test_data$Species) / length(test_data$Species)

커널을 linear 로 하나 polynomial 로 하나 똑같이 0.9666667 정확도가 출력되고 있습니다.

문제383. 커널을 sigmoid 로 변경하고 모델을 생성해서 정확도를 출력하세요 

0.8666667

문제384.  위의 아이리스 품종을 분류하는 머신러닝 모델을 생성하는데 11장에서 배웠던
             자동튜닝 기능을 이용해서 모델을 생성하고 정확도를 확인하시오 ! (p483 참고)

책 485페이지 : m <- train(Species ~ .,  data=train_data, method='svmLinear') 
                    m2 <- train(Species ~ .,  data=train_data, method='svmRadial') 

# 1. 데이터 로드
iris <- read.csv("iris2.csv", stringsAsFactors=TRUE)

# 2. 훈련 데이터와 테스트 데이터로 분리
library(caret)
set.seed(1)
in_train <- createDataPartition(iris$Species, p=0.8, list=F)
train_data <- iris[ in_train ,  ]
test_data  <- iris[-in_train,   ]
nrow(train_data)
nrow(test_data)


# 3. 모델 생성
#install.packages("e1071")
library(e1071)
library(caret)

m2 <- train(Species ~ .,  data=train_data, method='svmRadial') 

# 4. 모델 예측
result2 <- predict( m2, test_data)
result2 

sum( result2 == test_data$Species) / length(test_data$Species)

문제385.  유방암 데이터의 악성과 양성을 분류하는 머신러닝 모델을 서포트 벡터머신으로
              생성하고 정확도를 확인하시오 ! ( 훈련 : 테스트 = 8 : 2 ) 자동튜닝 바로 적용하세요~


온라인 수업의 집중을 높이기 위해서 답글로 코드를 올려주세요 ~~

wisc_bc_data <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

# 1. 데이터 로드
# 2. 훈련과 테스트 나눔
# 3. 자동 튜닝 모델 생성
# 4. 모델예측
# 5. 모델평가

■ 파이썬으로 iris 데이터의 서포트 벡터 머신 모델 생성하기 

#1. 데이터 로드
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")
iris.head()

#2. min/max 정규화 하기
x = iris.iloc[ : ,0:4]
y = iris['Species']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x_scaled = scaler.transform(x)

#3. 훈련 데이터와 테스트 데이터 분리하기
from  sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( x_scaled, y, test_size=0.2, random_state=1)

#4. 모델 생성
from  sklearn  import  svm 

svm_model = svm.SVC(kernel='rbf')
# 커널의 종류:  rbf,  poly, sigmoid, linear 

#5. 모델 훈련
svm_model.fit( x_train, y_train) 

#6. 모델 예측
result = svm_model.predict(x_test)

#7. 모델 평가 
sum( result == y_test ) / len(y_test)  # 0.966

문제386.  gridsearch 의 자동튜닝 기능을 이용해서 위의 아이리스 모델의 성능을 더 
             올리시오 !

힌트:
from  sklearn  import  svm
from  sklearn.model_selection  import GridSearchCV

param_grid ={'C' : [0.1, 1, 10, 100, 1000],
                   'gamma' : [1, 0.1, 0.01, 0.001, 0.0001],
                   'kernel' : ['rbf', 'poly', 'sigmoid', 'linear'] }

grid = GridSearchCV( svm.SVC(), param_grid, refit=True, cv=3, n_jobs=-1, verbose=2) 

※설명:refit=True 는 최적의 하아퍼 파라미터를 찾은뒤 찾아낸 최적의 하이퍼 파라미터로
  재학습시킨다. 

grid.fit( x_train, y_train ) 
print(grid.best_params_)  # {'C': 10, 'gamma': 1, 'kernel': 'linear'}

문제387.  아래의 3개의 머신러닝 모델의 조합으로 앙상블 모델을 만들어서 아이리스 품종을 
              분류하는 머신러닝 모델을 만들고 정확도를 출력하시오 !

모델1:  서포트 벡터 머신
모델2:  나이브 베이즈
모델3:  랜덤포레스트 

참조코드:

#1. 데이터 로드
from sklearn import datasets
breast=datasets.load_breast_cancer()
x=breast.data
y=breast.target

# 정규화를 진행합니다.
from   sklearn.preprocessing  import  MinMaxScaler 

scaler = MinMaxScaler()
scaler.fit(x)
x_scaled = scaler.transform(x)

#2. 훈련데이터, 테스트데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test=train_test_split(x_scaled, y, test_size=0.1, random_state=1)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류

r1=GaussianNB()
r2=LogisticRegression()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r1), ('lr', r2), ('rf', r3) ], voting='hard')

#4. 훈련데이터, 테스트데이터 예측
train_result=eclf1.fit(x_train, y_train).predict(x_train)
test_result=eclf1.fit(x_train, y_train).predict(x_test)


#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    #0.97
print(sum(test_result==y_test)/len(y_test))     #0.98


답:

#1. 데이터 로드
import  pandas  as  pd
iris = pd.read_csv("d:\\data\\iris2.csv")
iris.head()

# 2.정규화를 진행합니다.
x = iris.iloc[ : ,0:4]
y = iris['Species']

from  sklearn.preprocessing  import  MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x)
x_scaled = scaler.transform(x)

#3. 훈련 데이터와 테스트 데이터 분리하기
from  sklearn.model_selection  import  train_test_split 

x_train, x_test, y_train, y_test = train_test_split( x_scaled, y, test_size=0.2, random_state=1)

#3. 앙상블 모델생성
from sklearn.naive_bayes import GaussianNB
from sklearn.svm  import  SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier    #분류

r1=GaussianNB()
r2=SVC()
r3=RandomForestClassifier()
eclf1=VotingClassifier(estimators=[ ('gnb', r1), ('svc', r2), ('rf', r3) ], voting='hard')

#4. 훈련데이터, 테스트데이터 예측
train_result=eclf1.fit(x_train, y_train).predict(x_train)
test_result=eclf1.fit(x_train, y_train).predict(x_test)


#5. 정확도 확인
print(sum(train_result==y_train)/len(y_train))    
print(sum(test_result==y_test)/len(y_test))     


문제388. (오늘의 마지막 문제) 아이리스 품종을 예측하는 앙상블 모델의 조합을 아래와 같이
            구성하고 앙상블 모델을 만들어서 정확도를 출력하시오 !

모델1:  서포트 벡터 머신
모델2:  신경망
모델3:  랜덤포레스트 

▩ 서포트 벡터 머신의 원리 

■ 원리설명1. 서포트 벡터 머신에서 벡터에서 벡터가 무엇인가요 ?

 방향과 크기의 의미를 모두 포함하는 표현 도구로서 주로 힘이나 자기장, 전기장,
 변위등의 물리적 개념을 설명할 때 이용됩니다.
 크기만을 의미하는 스칼라와 비교되는 양입니다. 


   " 방향과 크기를 같이 나타낼 수 있는 표현도구 "

■ 원리설명2. 벡터가 실생활에서 어떻게 활용되고 있나요?

  그림 

■ 원리설명3. normal vector 가 무엇인가요 ?


 한 평면이나 직선에 대하여 수직인 벡터를 말합니다. 


■ 원리설명4. 벡터의 내적 공식이 어떻게 되나요 ? 

그림

예제1.  코사인 120도는 얼마인가 ?

cos(120*pi/180)  # -0.5  음수값이 출력됩니다. 둔각이면 음수값이 출력됩니다.

예제2.  코사인 30도는 얼마인가 ?

cos(30*pi/180)  # 0.86  양수값이 출력됩니다. 예각이면 양수값이 출력이 됩니다.


예제3.  서포트 벡터 머신으로 분류하는 과정 설명


      로지스틱 회귀 오차함수 ---> 서포트 벡터 머신 



■ 로지스틱 회귀와 서포트 벡터 머신 수학식의 차이 

 로직스틱 회귀 수학식

 서포트 벡터 머신 수학식


설명:   이 둘의 결정적인 차이는 이상치에 민감하지 않고 가까이 있는 데이터들에 대해서만
          집중하겠금 개선한것이 바로 서포트 벡터 머신 수학식 입니다.

                  로지스틱 회귀                                                       서포트 벡터 머신

 1958년 영국의 통계학자인 cox 박사가 제안한 모델 ------->  1963년의 러시아의 수학자 만든 모델

 
■  로지스틱 회귀 오차함수의 유래 

  오즈함수 ----->  로짓함수 --------> 시그모이드 함수 

▩ 오즈(Odds) 함수

 오즈란 상공확률/실패확률 입니다. 

 예를 들어서 동전을 10번 던져서 성공을 2번, 실패를 8번했다고 하면

 성공확률 2/10,  실패확률  8/10 

  오즈는 (2/10) / (8/10)  = 1/4

  보통확률은  (성공시행수/전체시행수) 로 나타내는 데 비해 
   오즈는 (성공시행수/실패시행수) 입니다. 

예제1.  오즈 함수를 파이썬으로 생성하기 

import  numpy  as  np
import   matplotlib.pyplot  as  plt 

def odds_ratio(x):
    return x/(1-x)

x = np.arange( 0 , 1, 0.1)
y = odds_ratio(x)

plt.plot(x, y, label='odds ratio 0 ~ 1')
plt.grid()
plt.legend()
print(np.round(y, 2))

예제2. 위의 오즈함수에 로그를 씌워서 로짓함수 만들기 

import  numpy  as  np
import   matplotlib.pyplot  as  plt 

def odds_ratio(x):
    return x/(1-x)

def  log_odds_ratio(x):
    return  np.log( odds_ratio(x)) 

x = np.arange( 0 , 1, 0.01)
y = log_odds_ratio(x)

plt.plot(x, y, label='log odds ratio 0 ~ 1')
plt.grid()
plt.legend()
print(np.round(y, 2))


예제3. 로짓함수로 시그모이드 함수 만들기 

def  sigmoid(x):
    return  1/(1+np.exp(-x) ) 

우리가 예측하려는 값이 1 또는 0 이므로 확률 0.5 를 기준으로 0.5 보다 크면 1이고
0.5 보다 작으면 0 이 출력되는 함수로 사용하면서 로지스틱 회귀를 구현한다.

예제4.  시그모이드 함수에 로그를 씌우고 마이너스를 붙인 로지스틱회귀 함수 구현

def  log_sigmoid(x):
    return  -np.log(sigmoid(x) )

x = np.arange( -7, 7, 0.01)
y = log_sigmoid(x)

plt.plot( x, y, label='logstic function')
plt.grid()
plt.legend()

예제5. (점심시간 문제)  위의 반대 상황의 그래프를 출력하시오 ! 

반드시 코드와 그래프 올리고 식사하고 오세요 ~
즐거운 점심 시간 되세요 ~~~~~

* 파이썬으로 구현한 로지스틱회귀의 오차함수 

loss = -y_train*np.log(y_head) - (1-y_train)*np.log(1-y_head) 

■ R 로 로지스틱 회귀와 서포트 벡터머신의 성능 비교 

▦ 로직스틱회귀로 유방암 데이터 분류 모델 만들기 
         ↓
 정답이 0과 1로 되어있는 데이터를 분류하는 모델 

#1. 데이터 로드
wisc <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

wisc$diagnosis2 <- ifelse( wisc$diagnosis=='M', 1, 0)
head(wisc)

#2. 훈련 데이터와 테스트 데이터를 분리
library(caret)
set.seed(1)
in_train <- createDataPartition(wisc$diagnosis2, p=0.8, list=F)
train_data <- wisc[ in_train, 3:33 ]  # id 와 diagnosis 를 제외시킵니다. 
test_data <- wisc[-in_train, 3:33 ]  # id 와 diagnosis 를 제외시킵니다. 
nrow(train_data) #456
nrow(test_data)  #113

#3. 모델 생성
glm_model <- glm( diagnosis2 ~ . ,  data=train_data, family='binomial')
# 설명:  예측이 0과 1이 아니라 0과 1사이의 확률로 나올것이라는 경고메세지를 출력하고 있다.

#4. 모델 예측
result <- predict( glm_model,  test_data, type="response")
result

#5. 모델 평가 
pred1 <- ifelse( result >0.5, 1, 0 )
pred1

sum(pred1==test_data$diagnosis2) / length(test_data$diagnosis2) 

0.929  <--- 로지스틱 회귀 모델의 정확도 

문제389.  위의 유방암 데이터의 분류기를 서포트 벡터 머신으로 생성하여 로지스틱회귀의 정확도
             보다 더 높게 나오는지 실험하시오 !

         library(e1071)
힌트:  svm_model <- svm(diagnosis2 ~ . , data=train_data, kernel="linear")

온라인 수업의 집중을 높이기 위해서 카페에 코드를 올려주세요 ~~

답:
#데이터로드
wisc<-read.csv('wisc_bc_data.csv',stringsAsFactors = TRUE)
wisc$diagnosis2<-ifelse(wisc$diagnosis=='M',1,0)

#훈련/테스트데이터 분리
library(caret)
set.seed(1)
in_train<-createDataPartition(wisc$diagnosis2,p=0.8,list=F)
train_data<-wisc[in_train,c(-1,-2)]
test_data<-wisc[-in_train,c(-1,-2)]

#모델생성
library(e1071)
svm_model<-svm(diagnosis2~.,data=train_data,kernel='linear')

#모델예측
pred<-predict(svm_model,test_data)
pred1<-ifelse(pred>0.5,1,0)

#모델평가
sum(pred1==test_data$diagnosis2)/length(test_data$diagnosis2)

0.9557522
 
▩ 로지스틱 회귀 모델의 최적의 하이퍼 파라미터 찾기 실험 

자동튜닝 안했을때의 정확도:  0.929 

#1. 데이터 로드
wisc <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

wisc$diagnosis2 <- ifelse( wisc$diagnosis=='M', 1, 0)
head(wisc)

#2. 훈련 데이터와 테스트 데이터를 분리
library(caret)
set.seed(1)
in_train <- createDataPartition(wisc$diagnosis2, p=0.8, list=F)
train_data <- wisc[ in_train, 3:33 ]  # id 와 diagnosis 를 제외시킵니다. 
test_data <- wisc[-in_train, 3:33 ]  # id 와 diagnosis 를 제외시킵니다. 
nrow(train_data) #456
nrow(test_data)  #113

#3. 모델 생성
#glm_model <- glm( diagnosis2 ~ . ,  data=train_data, family='binomial')
# 설명:  예측이 0과 1이 아니라 0과 1사이의 확률로 나올것이라는 경고메세지를 출력하고 있다.
library(caret)

glm_model2 <- train( diagnosis2 ~ ., data=train_data, method='glm' )

#4. 모델 예측
result2 <- predict( glm_model2,  test_data, response="prob")
result2

#5. 모델 평가 
pred2 <- ifelse( result2 >0.5, 1, 0 )
pred2

sum(pred2==test_data$diagnosis2) / length(test_data$diagnosis2)  #  0.9734513

문제390. 이번에는 서포트 벡터 머신+자동튜닝으로 유방암 분류 모델의 정확도를 출력하시오

힌트:
library(e1071)
library(caret)
m2 <- train(Species ~ .,  data=train_data, method='svmRadial') 

답:

#1. 데이터 로드
wisc <- read.csv("wisc_bc_data.csv", stringsAsFactors=TRUE)

wisc$diagnosis2 <- ifelse( wisc$diagnosis=='M', 1, 0)
head(wisc)

#2. 훈련 데이터와 테스트 데이터를 분리
library(caret)
set.seed(1)
in_train <- createDataPartition(wisc$diagnosis2, p=0.8, list=F)
train_data <- wisc[ in_train, 3:33 ]  # id 와 diagnosis 를 제외시킵니다. 
test_data <- wisc[-in_train, 3:33 ]  # id 와 diagnosis 를 제외시킵니다. 
nrow(train_data) #456
nrow(test_data)  #113

#3. 모델 생성
#glm_model <- glm( diagnosis2 ~ . ,  data=train_data, family='binomial')
# 설명:  예측이 0과 1이 아니라 0과 1사이의 확률로 나올것이라는 경고메세지를 출력하고 있다.
library(caret)

glm_model4 <- train( diagnosis2 ~ ., data=train_data, method='svmRadial' )

#4. 모델 예측
result4 <- predict( glm_model4,  test_data, response="prob")
result4

#5. 모델 평가 
pred4 <- ifelse( result4 >0.5, 1, 0 )
pred4  

sum(pred4==test_data$diagnosis2) / length(test_data$diagnosis2)   #0.9911504

▩ 유방암 데이터를 파이썬으로 로지스틱 회귀 모델 구현하기 

# 1. 데이터 로드
wisc = pd.read_csv("d:\\data\\wisc_bc_data.csv")
wisc.head()
# 2. 정규화 진행
x = wisc.iloc[ : , 2: ] # 환자번호와 diagnosis 는 제외합니다.
y = wisc['diagnosis']

from  sklearn.preprocessing  import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x)
x_scaled = scaler.transform(x)

# 3. 훈련 데이터와 테스트 데이터로  분리
from  sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test = train_test_split( x_scaled, y, test_size=0.2, random_state=1, stratify=y)

# 4. 모델 생성
from  sklearn.linear_model  import  LogisticRegression
model = LogisticRegression()

# 5. 모델  훈련
model.fit(x_train, y_train)

# 6. 모델 예측
result = model.predict(x_test)

# 7. 모델 평가
sum( result == y_test) / len(y_test)  # 0.9649122807017544

문제391. 위의 머신러닝 모델을 grid search 를 이용해서 자동 튜닝하고 정확도를 확인하시오 

힌트: from  sklearn.model_selection  import GridSearchCV

       param_grid = { 'C' : [0.001, 0.01, 0.1, 1, 10, 100] }

       model2 = GridSearchCV( LogisticRegression(), param_grid, cv=5) 
    
 오차의 크기를 조정하는 하이퍼 파라미터인데 그냥 출력되는 오차보다 오차를 더 부풀려서 학습시킴으로
 오차를 더 최소화 시키기겠금 유도하는 하이퍼 파라미터 입니다. 
   
답:

# 1. 데이터 로드
wisc = pd.read_csv("d:\\data\\wisc_bc_data.csv")
wisc.head()
# 2. 정규화 진행
x = wisc.iloc[ : , 2: ] # 환자번호와 diagnosis 는 제외합니다.
y = wisc['diagnosis']

from  sklearn.preprocessing  import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x)
x_scaled = scaler.transform(x)

# 3. 훈련 데이터와 테스트 데이터로  분리
from  sklearn.model_selection  import  train_test_split 
x_train, x_test, y_train, y_test = train_test_split( x_scaled, y, test_size=0.2, random_state=1, stratify=y)

# 4. 모델 생성
from  sklearn.linear_model  import  LogisticRegression
from  sklearn.model_selection  import GridSearchCV
param_grid = { 'C' : [0.001, 0.01, 0.1, 1, 10, 100] }
model2 = GridSearchCV( LogisticRegression(), param_grid, cv=5) 

# 5. 모델  훈련
model2.fit(x_train, y_train)

# 6. 모델 예측
result2 = model2.predict(x_test)

# 7. 모델 평가
sum( result2 == y_test) / len(y_test) #0.9824561403508771

print(model2.best_params_) # C 값 10

▩ 파이썬으로 서포트 벡터 머신 분류와 시각화 

예제1.  사이킷런의 데이터셋 생성하는 make_blobs 함수를 이용해서 2개의 군집으로 데이터 생성하기

from  sklearn.datasets  import  make_blobs

x, y = make_blobs( centers=2,  random_state=8)

예제2. 위의  데이터를 시각화 하시오 

아나 콘다 프롬프트 창을 열고 pip  install  mglearn 

import  mglearn
import  matplotlib.pyplot  as  plt 

mglearn.discrete_scatter( x[ : , 0 ], x[ : , 1], y )
plt.xlabel("feature 0")
plt.ylabel("feature 1")

예제3.  서포트 벡터 머신 모델을 이용해서 데이터를 학습 시키고 분류 시각화 하기 
from  sklearn.svm  import  LinearSVC
model = LinearSVC()

model.fit(x, y)

mglearn.plots.plot_2d_separator( model, x )  # 분류 경계선을 시각화 
mglearn.discrete_scatter( x[ : , 0 ], x[ : , 1], y )
plt.xlabel("feature 0")
plt.ylabel("feature 1")

예제4. 위의 모델의 훈련 데이터에 대한 정확도를 확인하시오 !

model.score( x, y) 


▩ make_blobs 를 이용해서 2개의 군집인데 분포를 다양하게 변경하고 모델을 생성하기

예제1. 2개의 군집으로 분포를 다양하게 데이터 생성하기

from  sklearn.datasets  import  make_blobs

x, y = make_blobs( centers=4, random_state=8)
x

예제2. 위에서 나온 정답 4개를 정답 2개로 데이터 변경하기 

from  sklearn.datasets  import  make_blobs

x, y = make_blobs( centers=4, random_state=8)
y = y % 2
y

설명: 정답 3이 1로 변경되고 정답 2가 0으로 변경되어서 정답이 0과 1만 있게 했습니다.

예제3. 위의 데이터셋을 시각화 하시오 !

import  mglearn
import  matplotlib.pyplot  as  plt 

mglearn.discrete_scatter( x[ : , 0 ], x[ : , 1], y )
plt.xlabel("feature 0")
plt.ylabel("feature 1")

설명: 아까와는 다르게 직선으로는 분류하기 어려운 상태의 데이터셋임이 확인되고 있습니다. 

예제4.  선형 서포트 백터 머신 모델(LinearSVC) 를 이용해서 분류를 하고 시각화 하시오 
from  sklearn.svm  import  LinearSVC
model2 = LinearSVC()

model2.fit(x, y)

mglearn.plots.plot_2d_separator( model2, x )  # 결정 구분선 긋기 시각화
mglearn.discrete_scatter( x[ : , 0], x[ : , 1], y)
plt.xlabel("feature 0")
plt.ylabel("feature 1")

시각화를 딱봐도 분류를 잘 못했습니다. 

예제5. 정확도를 확인하세요 !

model2.score(x,y) # 66%의 정확도가 출력되고 있습니다. 

문제392.   서포트 벡터 머신의 모델을 변경해서 위의 정확도를 더 올리시오 ~

답코드로 달아주세요 ~~

참고 메뉴얼 사이트 : https://scikit-learn.org/stable/modules/svm.html

답:

from sklearn.datasets import make_blobs
x,y = make_blobs(centers=4, random_state=8)
y = y%2

from sklearn.svm import SVC  #또는 from sklearn import svm 
model3 = SVC()                   #model = svm.SVC()
model3.fit(x,y)


#아나콘다 프롬프트 > pip install mglearn
import mglearn
import matplotlib.pyplot as plt
mglearn.plots.plot_2d_separator(model3, x)  #결정 구분선 긋기 시각화
mglearn.discrete_scatter(x[ :,0], x[ :,1], y)
plt.xlabel('feature 0')
plt.ylabel('feature 1')

model3.score(x,y)  #정확도 확인

 
문제393.  6개의 군집으로 데이터를 만드는데 정답은 0과 1 두가지로만 해서 데이터셋을 생성하시오

from  sklearn.datasets  import  make_blobs

x, y = make_blobs( centers=6, random_state=8)
y = (y>=3).astype(int)
y

문제394. 위의 데이터를 시각화 하시오 !

import  mglearn
import  matplotlib.pyplot  as  plt 

mglearn.discrete_scatter( x[ : , 0 ], x[ : , 1], y )
plt.xlabel("feature 0")
plt.ylabel("feature 1")

문제395.  위의 데이터를 서포트 백터 머신으로 분류하는데 분류된 결과도 시각화 하시오 !

from sklearn.datasets import make_blobs
x,y = make_blobs(centers=6, random_state=8)
y = (y>=3).astype(int)


from sklearn.svm import SVC  #또는 from sklearn import svm 
model3 = SVC()                   #model = svm.SVC()
model3.fit(x,y)


#아나콘다 프롬프트 > pip install mglearn
import mglearn
import matplotlib.pyplot as plt
mglearn.plots.plot_2d_separator(model3, x)  #결정 구분선 긋기 시각화
mglearn.discrete_scatter(x[ :,0], x[ :,1], y)
plt.xlabel('feature 0')
plt.ylabel('feature 1')

model3.score(x,y)  #정확도 확인

문제396.  서포트 벡터 머신의 하이퍼 파라미터인 C 와 gamma 를 이용하면 정확도를 더 높일수 
              있다고 했습니다.  지금 위의 코드에 모델 생성부분에 하이퍼 파라미터 C 를 수정해서
               정확도 100% 로 만들고 시각화도 하세요 !


model3 = SVC( C=? ) 

답: model3 = SVC( C=10)                    45분까지 쉬세요 ~~~


from sklearn.datasets import make_blobs
x,y = make_blobs(centers=6, random_state=8)
y = (y>=3).astype(int)

from sklearn.svm import SVC  #또는 from sklearn import svm 
model3 = SVC(C=10)                   #model = svm.SVC()
model3.fit(x,y)

#아나콘다 프롬프트 > pip install mglearn
import mglearn
import matplotlib.pyplot as plt
mglearn.plots.plot_2d_separator(model3, x)  #결정 구분선 긋기 시각화
mglearn.discrete_scatter(x[ :,0], x[ :,1], y)
plt.xlabel('feature 0')
plt.ylabel('feature 1')

model3.score(x,y)  #정확도 확인



문제397. (머신러닝 마지막 문제)  아래와 같이 다시 데이터셋을 구성하고 분류하는데
            서포트 벡터머신의 하이퍼 파라미터인 C 와 gamma 를 이용해서 분류하고 시각화
            하시오 !

from sklearn.datasets import make_blobs                 5시 신호 보냈습니다. 
x,y = make_blobs(centers=6, random_state=8)          마지막 문제 올리시고 자유롭게 스터디
y = y%2                                                            또는 자습하세요 ~~



6시 신호 보냈습니다.  오늘 반에 10명이 출석했는데 내일 8강의장에서 다른반 이수자평가가
있어서 내일은 다시 5명만 출석할 수 있고 다음주 부터 다시 지금 신청했던 10명 출석 가능합니다














































































































































































































































































































































































































































































 

























































































































































































































































 
















































































































































































































































































































































































 










































































 





























































































































































































 












    








  












































 















































































































 





















































































 










































































































































































































































































































































































































































































































































































































































            




















































































  








 
















































                                                        






































































































































































































































































































































































































































































































명규코드:
from matplotlib import font_manager, rc
font = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name()
rc('font', family=font)

c_cnt = pd.read_csv("d:\\data\\창업건수.csv",encoding='euckr')
x = c_cnt[['치킨집','년도']]
x.plot(kind='bar',figsize=(10,10),x='년도')













 






























































































































































 











































































































































































































































































































































































                                  






















